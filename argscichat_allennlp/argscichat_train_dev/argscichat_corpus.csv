Article Title,Article Content,P_Message,DE_Message,Chat History,Supporting Facts,Split
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Hi, it is an interesting paper title. What kind of knowledge does domain knowledge refer to here.",Hi! We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.,,['Constrained problem are classically modeled by domain experts in a fully declarative fashion'],val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Interesting, how are you injecting the domain knowledge?",We have developed a technique inspired by Constraint Programming.,"Hi, it is an interesting paper title. What kind of knowledge does domain knowledge refer to here.","['we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",Have been previous Machine Learning approaches been drastically failing on this kind of task?,"Actually, they struggle to provide feasible assignements but they have some advantages. Do you want to know which ones?","Hi, it is an interesting paper title. What kind of knowledge does domain knowledge refer to here. Hi! We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.
 Facts: 
1. Constrained problem are classically modeled by domain experts in a fully declarative fashion
 Interesting, how are you injecting the domain knowledge?","['Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Yes, what are their advantages.",They can take into account some constraints that are not easy to be modelled by the domain expert but that are satisfied in past solutions.,"Hi, it is an interesting paper title. What kind of knowledge does domain knowledge refer to here. Hi! We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.
 Facts: 
1. Constrained problem are classically modeled by domain experts in a fully declarative fashion
 Interesting, how are you injecting the domain knowledge? We have developed a technique inspired by Constraint Programming.
 Facts: 
1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.
 Have been previous Machine Learning approaches been drastically failing on this kind of task?","['Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences)']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",But for some cases you still require domain knowledge? How did you test that?,"Unfortunately, I have not enough information to answer your question.","Hi, it is an interesting paper title. What kind of knowledge does domain knowledge refer to here. Hi! We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.
 Facts: 
1. Constrained problem are classically modeled by domain experts in a fully declarative fashion
 Interesting, how are you injecting the domain knowledge? We have developed a technique inspired by Constraint Programming.
 Facts: 
1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.
 Have been previous Machine Learning approaches been drastically failing on this kind of task? Actually, they struggle to provide feasible assignements but they have some advantages. Do you want to know which ones?
 Facts: 
1. Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility
 Yes, what are their advantages.",[],val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",Hi! What is the constrained problem mentioned in the title?,Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?,,['We use as a benchmark the Partial Latin Square (PLS) completion problem'],train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Yes, please. What is Partial Latin Square completion problem?","It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.",Hi! What is the constrained problem mentioned in the title?,"['We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.']",train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Thank you for the explanation. Then, what is the domain knowledge in this case?",It is the declarative formulation of the constrained problem.,"Hi! What is the constrained problem mentioned in the title? Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem
 Yes, please. What is Partial Latin Square completion problem?",['Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)'],train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","I see. Then, how do they inject the domain knowledge in the neural network?",Our method is inspired by Constraint Programming.,"Hi! What is the constrained problem mentioned in the title? Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem
 Yes, please. What is Partial Latin Square completion problem? It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.
 Thank you for the explanation. Then, what is the domain knowledge in this case?","['we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.']",train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","so, is it done by adding the constraints in the loss function?",I am sorry but I have not enough information to answer your question. Do you think that the Partial Latin Square is a suitable benchmark?,"Hi! What is the constrained problem mentioned in the title? Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem
 Yes, please. What is Partial Latin Square completion problem? It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.
 Thank you for the explanation. Then, what is the domain knowledge in this case? It is the declarative formulation of the constrained problem.
 Facts: 
1. Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)
 I see. Then, how do they inject the domain knowledge in the neural network?",[],train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",it seems fine since it is possible to obtain domain knowledge. But I'm not sure whether it is possible to adapt this approach to other domains such as NLP or computer vision. What do you think?,I think that the main application are Constraints Satisfaction Problems.,"Hi! What is the constrained problem mentioned in the title? Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem
 Yes, please. What is Partial Latin Square completion problem? It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.
 Thank you for the explanation. Then, what is the domain knowledge in this case? It is the declarative formulation of the constrained problem.
 Facts: 
1. Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)
 I see. Then, how do they inject the domain knowledge in the neural network? Our method is inspired by Constraint Programming.
 Facts: 
1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.
 so, is it done by adding the constraints in the loss function?","['Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems.']",train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",I see. Thank you for your explanations. Good bye,Bye!,"Hi! What is the constrained problem mentioned in the title? Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem
 Yes, please. What is Partial Latin Square completion problem? It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.
 Thank you for the explanation. Then, what is the domain knowledge in this case? It is the declarative formulation of the constrained problem.
 Facts: 
1. Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)
 I see. Then, how do they inject the domain knowledge in the neural network? Our method is inspired by Constraint Programming.
 Facts: 
1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.
 so, is it done by adding the constraints in the loss function? I am sorry but I have not enough information to answer your question. Do you think that the Partial Latin Square is a suitable benchmark? it seems fine since it is possible to obtain domain knowledge. But I'm not sure whether it is possible to adapt this approach to other domains such as NLP or computer vision. What do you think?",[],train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",Hello! What is the type of knowledge that is injected in the neural networks?,Hi! We inject domain knowledge from Constrained Satisfaction Problems.,,"['Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).']",train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",Interesting! What is the problem used to perform the experiment?,We employ the Partial Latin Square completion problem. Do you want to know how does it work?,Hello! What is the type of knowledge that is injected in the neural networks?,"['We use as a benchmark the Partial Latin Square (PLS) completion problem,']",train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Yes, thank you",You have a partially filled NxN square and you have to complete it with all the values in the range from 1 to N preventing the same value to appear more than once in the same row or column.,"Hello! What is the type of knowledge that is injected in the neural networks? Hi! We inject domain knowledge from Constrained Satisfaction Problems.
 Facts: 
1. Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).
 Interesting! What is the problem used to perform the experiment?","['We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.']",train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",I understand. And how is knowledge injected in these networks?,I'm sorry but I have not enough information to answer your question. Do you want to know why we choose the to inject knowledge in a data-driven approach?,"Hello! What is the type of knowledge that is injected in the neural networks? Hi! We inject domain knowledge from Constrained Satisfaction Problems.
 Facts: 
1. Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).
 Interesting! What is the problem used to perform the experiment? We employ the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem,
 Yes, thank you",[],train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","yes, thank you!",Data-driven methods can take into account preferences that are hard to design in Constrained Satisfaction Problem. At the same time data-driven method struggle to preserve feasibility.,"Hello! What is the type of knowledge that is injected in the neural networks? Hi! We inject domain knowledge from Constrained Satisfaction Problems.
 Facts: 
1. Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).
 Interesting! What is the problem used to perform the experiment? We employ the Partial Latin Square completion problem. Do you want to know how does it work?
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem,
 Yes, thank you You have a partially filled NxN square and you have to complete it with all the values in the range from 1 to N preventing the same value to appear more than once in the same row or column.
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.
 I understand. And how is knowledge injected in these networks?","['Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences)', 'Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility']",train
End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents,"Answering complex questions from long documents requires aggregating multiple pieces of evidence and then predicting the answers. In this paper, we propose a multihop retrieval method, DOCHOPPER, to answer compositional questions over long documents. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step. In contrast to many other retrieval-based methods (e.g., RAG or REALM) the query is not augmented with a token sequence: instead, it is augmented by ""numerically"" combining it with another neural representation.

This means that model is end-to-end differentiable. We demonstrate that utilizing document structure in this was can largely improve question-answering and retrieval performance on long documents. We experimented with DOCHOPPER on three different QA tasks that require reading long documents to answer compositional questions: discourse entailment reasoning, factual QA with table and text, and information seeking QA from academic papers. DOC-HOPPER outperforms all baseline models and achieves state-of-the-art results on all datasets.

Additionally, DOCHOPPER is efficient at inference time, being 3∼10 times faster than the baselines. Recent advances in Transformer-based pretrained Language Models (e.g., BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) ) have dramatically improved performance on question answering (QA) tasks (Rajpurkar et al., 2016) . Although effective, Transformer-based models are usually limited to documents of fixed length, typically 512 tokens, due to their O(n 2 ) memory complexity. This is an important restrictions for reading long documents such as Wikipedia pages, academic papers, and technical reports, which can contain tens of thousands of tokens. To address this issue, Transformer models that use sparse self-attention have been introduced (e.g., Beltagy et al., 2020) ).

For example, ETC  introduced a global-local attention mechanism, where local tokens only attend to the global tokens they are assigned to, while global tokens attend to each other to communicate information across a document. This reduces the complexity of the model to the square of the number of global tokens. A similar idea is also adopted by Longformer (Beltagy et al., 2020) . However, while these models can include larger documents than the original Transformer, their input size is still limited to 4,096 on finetuning tasks.

An alternative way to perform question answering over long documents (Clark and Gardner, 2017) is to adopt the ""retrieve and read"" pipeline that is commonly used in open-domain QA. In retrieve-and-read systems, a retriever module retrieves the top passages from the documents, and then a Transformer-based reader extracts or generates answers from the retrieved passages. Even though dense retrievers (e.g. DPR (Karpukhin et al., 2020) ) show competitive retrieval performance in open-domain QA tasks, and recent work (Zhao et al., 2021; Qi et al., 2021; to extend the query with tokens from previously retrieved text, and re-encode the query to perform the next round of retrieval, such multi-hop retrieval process is not end-to-end differentiable, and requires careful training to avoid cascaded errors.

These models also treat each passage independently, ignoring the rich information in document structures. In this work we propose a multi-hop retrieval method, DOCHOPPER, to solve compositional questions over long documents. DOCHOPPER first builds a retrieval index for each document from a pretrained language model. At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.

The update is computed in the dense space without having to re-encode the query or the context, making it differentiable end-to-end. Different from previous multi-hop retrieval methods (Zhao et al., 2021; Qi et al., 2021; Sun et al., 2019) that treat each retrieval candidate independently, DOCHOPPER also utilizes the document structure to improve the retrieval accuracy: in particular, it performs retrieval at both paragraph-level and sentence-level. DOCHOPPER performs well on complex compositional reasoning tasks over long documents. We evaluate DOCHOPPER on three different tasks that require reading long documents to answer complex questions: conversational QA for discourse entailment reasoning (ShARC (Saeidi et al., 2018 )) 1 , factual QA with table and text (HybridQA ), and information seeking QA on academic papers (QASPER (Dasigi et al., 2021) ).

Since the outputs of the three tasks are different, additional layers or downstream models are appended to DOCHOPPER to make final predictions. DOCHOPPER achieves state-of-the-art results on all three datasets, outperforming the baseline models by 3%-5%. Additionally, DOCHOPPER runs 3∼10 faster than the baseline models, because index of documents is pre-computed, which significantly reduces computation cost at inference time.",what is the main idea of the paper?,Hello! The main idea is to approach multi-hop and sequential question answering on long documents using semantic indexing and dense-space query updates. This enables end-to-end training and higher performance.,,"['At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step.', 'This means that model is end-to-end differentiable.']",train
End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents,"Answering complex questions from long documents requires aggregating multiple pieces of evidence and then predicting the answers. In this paper, we propose a multihop retrieval method, DOCHOPPER, to answer compositional questions over long documents. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step. In contrast to many other retrieval-based methods (e.g., RAG or REALM) the query is not augmented with a token sequence: instead, it is augmented by ""numerically"" combining it with another neural representation.

This means that model is end-to-end differentiable. We demonstrate that utilizing document structure in this was can largely improve question-answering and retrieval performance on long documents. We experimented with DOCHOPPER on three different QA tasks that require reading long documents to answer compositional questions: discourse entailment reasoning, factual QA with table and text, and information seeking QA from academic papers. DOC-HOPPER outperforms all baseline models and achieves state-of-the-art results on all datasets.

Additionally, DOCHOPPER is efficient at inference time, being 3∼10 times faster than the baselines. Recent advances in Transformer-based pretrained Language Models (e.g., BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) ) have dramatically improved performance on question answering (QA) tasks (Rajpurkar et al., 2016) . Although effective, Transformer-based models are usually limited to documents of fixed length, typically 512 tokens, due to their O(n 2 ) memory complexity. This is an important restrictions for reading long documents such as Wikipedia pages, academic papers, and technical reports, which can contain tens of thousands of tokens. To address this issue, Transformer models that use sparse self-attention have been introduced (e.g., Beltagy et al., 2020) ).

For example, ETC  introduced a global-local attention mechanism, where local tokens only attend to the global tokens they are assigned to, while global tokens attend to each other to communicate information across a document. This reduces the complexity of the model to the square of the number of global tokens. A similar idea is also adopted by Longformer (Beltagy et al., 2020) . However, while these models can include larger documents than the original Transformer, their input size is still limited to 4,096 on finetuning tasks.

An alternative way to perform question answering over long documents (Clark and Gardner, 2017) is to adopt the ""retrieve and read"" pipeline that is commonly used in open-domain QA. In retrieve-and-read systems, a retriever module retrieves the top passages from the documents, and then a Transformer-based reader extracts or generates answers from the retrieved passages. Even though dense retrievers (e.g. DPR (Karpukhin et al., 2020) ) show competitive retrieval performance in open-domain QA tasks, and recent work (Zhao et al., 2021; Qi et al., 2021; to extend the query with tokens from previously retrieved text, and re-encode the query to perform the next round of retrieval, such multi-hop retrieval process is not end-to-end differentiable, and requires careful training to avoid cascaded errors.

These models also treat each passage independently, ignoring the rich information in document structures. In this work we propose a multi-hop retrieval method, DOCHOPPER, to solve compositional questions over long documents. DOCHOPPER first builds a retrieval index for each document from a pretrained language model. At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.

The update is computed in the dense space without having to re-encode the query or the context, making it differentiable end-to-end. Different from previous multi-hop retrieval methods (Zhao et al., 2021; Qi et al., 2021; Sun et al., 2019) that treat each retrieval candidate independently, DOCHOPPER also utilizes the document structure to improve the retrieval accuracy: in particular, it performs retrieval at both paragraph-level and sentence-level. DOCHOPPER performs well on complex compositional reasoning tasks over long documents. We evaluate DOCHOPPER on three different tasks that require reading long documents to answer complex questions: conversational QA for discourse entailment reasoning (ShARC (Saeidi et al., 2018 )) 1 , factual QA with table and text (HybridQA ), and information seeking QA on academic papers (QASPER (Dasigi et al., 2021) ).

Since the outputs of the three tasks are different, additional layers or downstream models are appended to DOCHOPPER to make final predictions. DOCHOPPER achieves state-of-the-art results on all three datasets, outperforming the baseline models by 3%-5%. Additionally, DOCHOPPER runs 3∼10 faster than the baseline models, because index of documents is pre-computed, which significantly reduces computation cost at inference time.",can you describe how does the query be updated?,"The query is updated, intuitively speaking, by computing the residual query information that is not covered by the answer of the previous hop. This happens in the embedding space using simple vector operations. Would you like to know more about how the system deals with long documents?",what is the main idea of the paper?,"[' At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.']",train
End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents,"Answering complex questions from long documents requires aggregating multiple pieces of evidence and then predicting the answers. In this paper, we propose a multihop retrieval method, DOCHOPPER, to answer compositional questions over long documents. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step. In contrast to many other retrieval-based methods (e.g., RAG or REALM) the query is not augmented with a token sequence: instead, it is augmented by ""numerically"" combining it with another neural representation.

This means that model is end-to-end differentiable. We demonstrate that utilizing document structure in this was can largely improve question-answering and retrieval performance on long documents. We experimented with DOCHOPPER on three different QA tasks that require reading long documents to answer compositional questions: discourse entailment reasoning, factual QA with table and text, and information seeking QA from academic papers. DOC-HOPPER outperforms all baseline models and achieves state-of-the-art results on all datasets.

Additionally, DOCHOPPER is efficient at inference time, being 3∼10 times faster than the baselines. Recent advances in Transformer-based pretrained Language Models (e.g., BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) ) have dramatically improved performance on question answering (QA) tasks (Rajpurkar et al., 2016) . Although effective, Transformer-based models are usually limited to documents of fixed length, typically 512 tokens, due to their O(n 2 ) memory complexity. This is an important restrictions for reading long documents such as Wikipedia pages, academic papers, and technical reports, which can contain tens of thousands of tokens. To address this issue, Transformer models that use sparse self-attention have been introduced (e.g., Beltagy et al., 2020) ).

For example, ETC  introduced a global-local attention mechanism, where local tokens only attend to the global tokens they are assigned to, while global tokens attend to each other to communicate information across a document. This reduces the complexity of the model to the square of the number of global tokens. A similar idea is also adopted by Longformer (Beltagy et al., 2020) . However, while these models can include larger documents than the original Transformer, their input size is still limited to 4,096 on finetuning tasks.

An alternative way to perform question answering over long documents (Clark and Gardner, 2017) is to adopt the ""retrieve and read"" pipeline that is commonly used in open-domain QA. In retrieve-and-read systems, a retriever module retrieves the top passages from the documents, and then a Transformer-based reader extracts or generates answers from the retrieved passages. Even though dense retrievers (e.g. DPR (Karpukhin et al., 2020) ) show competitive retrieval performance in open-domain QA tasks, and recent work (Zhao et al., 2021; Qi et al., 2021; to extend the query with tokens from previously retrieved text, and re-encode the query to perform the next round of retrieval, such multi-hop retrieval process is not end-to-end differentiable, and requires careful training to avoid cascaded errors.

These models also treat each passage independently, ignoring the rich information in document structures. In this work we propose a multi-hop retrieval method, DOCHOPPER, to solve compositional questions over long documents. DOCHOPPER first builds a retrieval index for each document from a pretrained language model. At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.

The update is computed in the dense space without having to re-encode the query or the context, making it differentiable end-to-end. Different from previous multi-hop retrieval methods (Zhao et al., 2021; Qi et al., 2021; Sun et al., 2019) that treat each retrieval candidate independently, DOCHOPPER also utilizes the document structure to improve the retrieval accuracy: in particular, it performs retrieval at both paragraph-level and sentence-level. DOCHOPPER performs well on complex compositional reasoning tasks over long documents. We evaluate DOCHOPPER on three different tasks that require reading long documents to answer complex questions: conversational QA for discourse entailment reasoning (ShARC (Saeidi et al., 2018 )) 1 , factual QA with table and text (HybridQA ), and information seeking QA on academic papers (QASPER (Dasigi et al., 2021) ).

Since the outputs of the three tasks are different, additional layers or downstream models are appended to DOCHOPPER to make final predictions. DOCHOPPER achieves state-of-the-art results on all three datasets, outperforming the baseline models by 3%-5%. Additionally, DOCHOPPER runs 3∼10 faster than the baseline models, because index of documents is pre-computed, which significantly reduces computation cost at inference time.","Yes, please tell me more how to deal long documents","The transformer-based model ETC is used that is based on sparse attention. This does not only allow longer input token length, but also enables modeling paragraphs (and hereby document structure) very naturally. ","what is the main idea of the paper? Hello! The main idea is to approach multi-hop and sequential question answering on long documents using semantic indexing and dense-space query updates. This enables end-to-end training and higher performance.
 Facts: 
1. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step.
2. This means that model is end-to-end differentiable.
 can you describe how does the query be updated?","['Transformer models that use sparse self-attention have been introduced', 'ETC  introduced a global-local attention mechanism, where local tokens only attend to the global tokens they are assigned to, while global tokens attend to each other to communicate information across a document.']",train
End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents,"Answering complex questions from long documents requires aggregating multiple pieces of evidence and then predicting the answers. In this paper, we propose a multihop retrieval method, DOCHOPPER, to answer compositional questions over long documents. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step. In contrast to many other retrieval-based methods (e.g., RAG or REALM) the query is not augmented with a token sequence: instead, it is augmented by ""numerically"" combining it with another neural representation.

This means that model is end-to-end differentiable. We demonstrate that utilizing document structure in this was can largely improve question-answering and retrieval performance on long documents. We experimented with DOCHOPPER on three different QA tasks that require reading long documents to answer compositional questions: discourse entailment reasoning, factual QA with table and text, and information seeking QA from academic papers. DOC-HOPPER outperforms all baseline models and achieves state-of-the-art results on all datasets.

Additionally, DOCHOPPER is efficient at inference time, being 3∼10 times faster than the baselines. Recent advances in Transformer-based pretrained Language Models (e.g., BERT (Devlin et al., 2019) , RoBERTa (Liu et al., 2019) ) have dramatically improved performance on question answering (QA) tasks (Rajpurkar et al., 2016) . Although effective, Transformer-based models are usually limited to documents of fixed length, typically 512 tokens, due to their O(n 2 ) memory complexity. This is an important restrictions for reading long documents such as Wikipedia pages, academic papers, and technical reports, which can contain tens of thousands of tokens. To address this issue, Transformer models that use sparse self-attention have been introduced (e.g., Beltagy et al., 2020) ).

For example, ETC  introduced a global-local attention mechanism, where local tokens only attend to the global tokens they are assigned to, while global tokens attend to each other to communicate information across a document. This reduces the complexity of the model to the square of the number of global tokens. A similar idea is also adopted by Longformer (Beltagy et al., 2020) . However, while these models can include larger documents than the original Transformer, their input size is still limited to 4,096 on finetuning tasks.

An alternative way to perform question answering over long documents (Clark and Gardner, 2017) is to adopt the ""retrieve and read"" pipeline that is commonly used in open-domain QA. In retrieve-and-read systems, a retriever module retrieves the top passages from the documents, and then a Transformer-based reader extracts or generates answers from the retrieved passages. Even though dense retrievers (e.g. DPR (Karpukhin et al., 2020) ) show competitive retrieval performance in open-domain QA tasks, and recent work (Zhao et al., 2021; Qi et al., 2021; to extend the query with tokens from previously retrieved text, and re-encode the query to perform the next round of retrieval, such multi-hop retrieval process is not end-to-end differentiable, and requires careful training to avoid cascaded errors.

These models also treat each passage independently, ignoring the rich information in document structures. In this work we propose a multi-hop retrieval method, DOCHOPPER, to solve compositional questions over long documents. DOCHOPPER first builds a retrieval index for each document from a pretrained language model. At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.

The update is computed in the dense space without having to re-encode the query or the context, making it differentiable end-to-end. Different from previous multi-hop retrieval methods (Zhao et al., 2021; Qi et al., 2021; Sun et al., 2019) that treat each retrieval candidate independently, DOCHOPPER also utilizes the document structure to improve the retrieval accuracy: in particular, it performs retrieval at both paragraph-level and sentence-level. DOCHOPPER performs well on complex compositional reasoning tasks over long documents. We evaluate DOCHOPPER on three different tasks that require reading long documents to answer complex questions: conversational QA for discourse entailment reasoning (ShARC (Saeidi et al., 2018 )) 1 , factual QA with table and text (HybridQA ), and information seeking QA on academic papers (QASPER (Dasigi et al., 2021) ).

Since the outputs of the three tasks are different, additional layers or downstream models are appended to DOCHOPPER to make final predictions. DOCHOPPER achieves state-of-the-art results on all three datasets, outperforming the baseline models by 3%-5%. Additionally, DOCHOPPER runs 3∼10 faster than the baseline models, because index of documents is pre-computed, which significantly reduces computation cost at inference time.",how does the model decide when to retrieve a sentence or a paragraph?,Good question! Both are represented in the vector space and the one most fitting one for the question is selected.,"what is the main idea of the paper? Hello! The main idea is to approach multi-hop and sequential question answering on long documents using semantic indexing and dense-space query updates. This enables end-to-end training and higher performance.
 Facts: 
1. At each step, DOCHOPPER retrieves a paragraph or sentence embedding from the document, mixes the retrieved result with the query, and updates the query for the next step.
2. This means that model is end-to-end differentiable.
 can you describe how does the query be updated? The query is updated, intuitively speaking, by computing the residual query information that is not covered by the answer of the previous hop. This happens in the embedding space using simple vector operations. Would you like to know more about how the system deals with long documents?
 Facts: 
1.  At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.
 Yes, please tell me more how to deal long documents","['At each retrieval step, DOCHOPPER retrieves a candidate (paragraph or sentence) from the index and updates the query for the next step of retrieval.']",train
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.","Hi, interesting paper title. How do you add the syntactic information into the word representations?",we build a syntactic graph neural model that takes pre-trained word representations as feature input. The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.,,['SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.'],val
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",Which kind of graph neural network do you experimented with?,We use a graph convolutional neural network (GCN). ,"Hi, interesting paper title. How do you add the syntactic information into the word representations?",['the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.'],val
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",Do GCNs come with special properties which helps for the task?,The word graph is built on syntactic dependency and other types of connections such as word order connections. Do you want to know what objectives the model trained on?,"Hi, interesting paper title. How do you add the syntactic information into the word representations? we build a syntactic graph neural model that takes pre-trained word representations as feature input. The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.
 Facts: 
1. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.
 Which kind of graph neural network do you experimented with?","['Since in English word order is important, we preserve it by adding these connections into the graph layer.']",val
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.","Yes, please. Which objectives did the model try to solve?",The model is trained for part-of-speech tagging and syntactic dependency parsing.,"Hi, interesting paper title. How do you add the syntactic information into the word representations? we build a syntactic graph neural model that takes pre-trained word representations as feature input. The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.
 Facts: 
1. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.
 Which kind of graph neural network do you experimented with? We use a graph convolutional neural network (GCN). 
 Facts: 
1. the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.
 Do GCNs come with special properties which helps for the task?",['The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.'],val
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",And which pre-trained word representations did you use?,"Our model can take both static and contextualized word representations including GloVe, ELMo and BERT.","Hi, interesting paper title. How do you add the syntactic information into the word representations? we build a syntactic graph neural model that takes pre-trained word representations as feature input. The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.
 Facts: 
1. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.
 Which kind of graph neural network do you experimented with? We use a graph convolutional neural network (GCN). 
 Facts: 
1. the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.
 Do GCNs come with special properties which helps for the task? The word graph is built on syntactic dependency and other types of connections such as word order connections. Do you want to know what objectives the model trained on?
 Facts: 
1. Since in English word order is important, we preserve it by adding these connections into the graph layer.
 Yes, please. Which objectives did the model try to solve?","['a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.', 'We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations.']",val
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",How did you evaluate the improvement of the new word representations?,We compare performance of using the word representations in the downstream tasks compared to using the base representations. The downstream tasks include a nested named entity recognition and two relation extraction tasks.,"Hi, interesting paper title. How do you add the syntactic information into the word representations? we build a syntactic graph neural model that takes pre-trained word representations as feature input. The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.
 Facts: 
1. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.
 Which kind of graph neural network do you experimented with? We use a graph convolutional neural network (GCN). 
 Facts: 
1. the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.
 Do GCNs come with special properties which helps for the task? The word graph is built on syntactic dependency and other types of connections such as word order connections. Do you want to know what objectives the model trained on?
 Facts: 
1. Since in English word order is important, we preserve it by adding these connections into the graph layer.
 Yes, please. Which objectives did the model try to solve? The model is trained for part-of-speech tagging and syntactic dependency parsing.
 Facts: 
1. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.
 And which pre-trained word representations did you use?","['We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).', 'In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE.']",val
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",Hi! Thanks for joining. Can you explain what is the main idea of the paper?,This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.,,"['we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.', 'To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.']",train
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",What type of syntactic information do you use? Is it based on dependency or constituency? ,We use part-of-speech and syntactic dependency. ,Hi! Thanks for joining. Can you explain what is the main idea of the paper?,"['The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term \'\'syntactic information"" interchangeably with \'\'POS tags"" and \'\'dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing ']",train
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",Sounds great! What tasks do you test?,We evaluate the extracted word representations on a nested named entity recognition task and two relation extraction tasks.,"Hi! Thanks for joining. Can you explain what is the main idea of the paper? This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.
 Facts: 
1. we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.
2. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.
 What type of syntactic information do you use? Is it based on dependency or constituency? ","['We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).', 'In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE.']",train
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",Can you tell me why you only select part-of-speech and dependency as the syntactic information you want to use to help pretraining?,"Previous work shows that part-of-speech and dependency are useful for downstream tasks. However, this doesn't limit the use of other information such as coreference in our model.","Hi! Thanks for joining. Can you explain what is the main idea of the paper? This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.
 Facts: 
1. we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.
2. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.
 What type of syntactic information do you use? Is it based on dependency or constituency?  We use part-of-speech and syntactic dependency. 
 Facts: 
1. The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing 
 Sounds great! What tasks do you test?","['Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .']",train
Syntactically-informed word representations from graph neural network,"Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text. These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information. Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.

The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools. SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations. We finally replace word representations in downstream models with SIWRs for applications. We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).

The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction. Our SIWRs also perform better than finetuning BERT in binary RE. We also conduct extensive experiments to analyse the proposed method. Word representations have been widely used in natural language processing (NLP) tasks. Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector. Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] . These contextual word representations have demonstrated improvements in downstream tasks over the static ones.

Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] . These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] . However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] . On the contrary, contextual representations can be easily adopted as plug-in plugout features.

Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top. In other words, the computational costs for fine-tuning methods are much higher than contextual approaches. All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures. Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .

The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] . Many task-oriented neural models do not take into account such syntactic information despite potential performance gains. To include such information to existing models, it is necessary to change the model architecture.

This leads to the following research question:

 Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information? In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications. We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture. Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.

We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations. Since in English word order is important, we preserve it by adding these connections into the graph layer. We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme. Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.

We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks. Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models. Fig. 2 shows the architecture of our SIWR model.

We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations. We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information. The GCN is used to include syntactic information into the base word representations. The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.

We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools. Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model. We simply replace word representations in downstream task models with SIWRs to cater for different applications. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).

SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE. We also employ BERT as the base representations in our SIWRs BERT for comparison. Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.

Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively. Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement. The contributions of our work are as follows:

 We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations. SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.

We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE. The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models. Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] . Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference.",Thanks for explaining. I love to see the results of injecting other types of information. Do you think if it would be a good idea if you try to inject syntactic information beginning from pretraining from scratch?,"We cannot answer given the current content. We can inject syntactic information during pre-training from scratch, however, this will cost more time and computational resources. Intermediate pretraining allows us to directly incorporate additional information into pre-trained models.","Hi! Thanks for joining. Can you explain what is the main idea of the paper? This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.
 Facts: 
1. we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.
2. To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.
 What type of syntactic information do you use? Is it based on dependency or constituency?  We use part-of-speech and syntactic dependency. 
 Facts: 
1. The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig. 1 ; in this paper, we use the term ''syntactic information"" interchangeably with ''POS tags"" and ''dependencies""), has been well studied and can be obtained efficiently with high accuracy using existing 
 Sounds great! What tasks do you test? We evaluate the extracted word representations on a nested named entity recognition task and two relation extraction tasks.
 Facts: 
1. We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).
2. In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE.
 Can you tell me why you only select part-of-speech and dependency as the syntactic information you want to use to help pretraining?","['We propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.']",train
When Do You Need Billions of Words of Pretraining Data?,"NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.

A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models. Pretrained language models (LMs) like BERT and
RoBERTa have become ubiquitous in NLP. These
models use massive datasets on the order of tens or
even hundreds of billions of words (Brown et al.,
2020) to learn linguistic features and world knowl-
edge, and they can be fine-tuned to achieve good
performance on many downstream tasks. Much recent work has used probing methods
to evaluate what these models have and have not
learned (Belinkov and Glass, 2019; Tenney et al.,
2019b; Rogers et al., 2020; Ettinger, 2020). Since
most of these works only focus on models pre-
trained on a fixed data volume (usually billions
of words), many interesting questions regarding
the effect of the amount of pretraining data remain
unanswered: What do data-rich models know that
models with less pretraining data do not? How
much pretraining data is required for LMs to learn
different grammatical features and linguistic phe-
nomena? Which of these skills do we expect to
improve if we increase the pretraining data to over
30 billion words? Which aspects of grammar can
be learned from data volumes on par with the input
to human learners, around 10M to 100M words
(Hart and Risley, 1992)?
With these questions in mind, we probe the
MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M,
and 1B words, and RoBERTaBASE(Liu et al., 2019)
pretrained on about 30B words, using four methods:
First we use standard classifier probing on the edge
probing suite of NLP tasks (Tenney et al., 2019b)
to measure the quality of the syntactic and semantic
features that can be extracted by a downstream clas-
sifier with each level of pretraining. Second, we ap-
ply minimum description length probing (Voita and
Titov, 2020) to the edge probing suite, with the goal
of quantifying the accessibility of these features.
Third, we probe the models’ knowledge of various
syntactic phenomena using unsupervised accept-
ability judgments on the BLiMP suite (Warstadt
et al., 2020a). Fourth, we fine-tune the models on
five tasks from SuperGLUE (Wang et al., 2019), to
measure their ability to solve conventional NLU
tasks.
Figure 1 shows the interpolated learning curves
for these four methods as a function of the amount
of pretraining data. We have two main findings:
First, the results of three probing methods we adopt
show that the linguistic knowledge of RoBERTa
pretrained on 100M words is already very close
to that of RoBERTaBASE, which is pretrained on
around 30B words. Second, RoBERTa requires bil-
lions of words of pretraining data to make substan-
tial improvements in performance on dowstream
NLU tasks. From these results, we conclude that
there are skills critical to solving downstream NLU
tasks that LMs can only acquire with billions of
words of pretraining data and that we need to look
beyond probing for linguistic features to explain
why LMs improve at these large data scales.",Hello! Interesting question as the title. Do you have any conclusion in this paper?,"Yes! We show that most linguistic features can be learned with training data between 10 million and 100 million, while the rest requires more.",,"['We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.', 'A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.']",train
When Do You Need Billions of Words of Pretraining Data?,"NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.

A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models. Pretrained language models (LMs) like BERT and
RoBERTa have become ubiquitous in NLP. These
models use massive datasets on the order of tens or
even hundreds of billions of words (Brown et al.,
2020) to learn linguistic features and world knowl-
edge, and they can be fine-tuned to achieve good
performance on many downstream tasks. Much recent work has used probing methods
to evaluate what these models have and have not
learned (Belinkov and Glass, 2019; Tenney et al.,
2019b; Rogers et al., 2020; Ettinger, 2020). Since
most of these works only focus on models pre-
trained on a fixed data volume (usually billions
of words), many interesting questions regarding
the effect of the amount of pretraining data remain
unanswered: What do data-rich models know that
models with less pretraining data do not? How
much pretraining data is required for LMs to learn
different grammatical features and linguistic phe-
nomena? Which of these skills do we expect to
improve if we increase the pretraining data to over
30 billion words? Which aspects of grammar can
be learned from data volumes on par with the input
to human learners, around 10M to 100M words
(Hart and Risley, 1992)?
With these questions in mind, we probe the
MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M,
and 1B words, and RoBERTaBASE(Liu et al., 2019)
pretrained on about 30B words, using four methods:
First we use standard classifier probing on the edge
probing suite of NLP tasks (Tenney et al., 2019b)
to measure the quality of the syntactic and semantic
features that can be extracted by a downstream clas-
sifier with each level of pretraining. Second, we ap-
ply minimum description length probing (Voita and
Titov, 2020) to the edge probing suite, with the goal
of quantifying the accessibility of these features.
Third, we probe the models’ knowledge of various
syntactic phenomena using unsupervised accept-
ability judgments on the BLiMP suite (Warstadt
et al., 2020a). Fourth, we fine-tune the models on
five tasks from SuperGLUE (Wang et al., 2019), to
measure their ability to solve conventional NLU
tasks.
Figure 1 shows the interpolated learning curves
for these four methods as a function of the amount
of pretraining data. We have two main findings:
First, the results of three probing methods we adopt
show that the linguistic knowledge of RoBERTa
pretrained on 100M words is already very close
to that of RoBERTaBASE, which is pretrained on
around 30B words. Second, RoBERTa requires bil-
lions of words of pretraining data to make substan-
tial improvements in performance on dowstream
NLU tasks. From these results, we conclude that
there are skills critical to solving downstream NLU
tasks that LMs can only acquire with billions of
words of pretraining data and that we need to look
beyond probing for linguistic features to explain
why LMs improve at these large data scales.",10 to 100 million is a rather large range of data. What is the difference between learning from 10 or 100 million words?,The performances of our four testing cases related to syntactic information leverage the most compared to other scales.,Hello! Interesting question as the title. Do you have any conclusion in this paper?,"['We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume']",train
When Do You Need Billions of Words of Pretraining Data?,"NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.

A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models. Pretrained language models (LMs) like BERT and
RoBERTa have become ubiquitous in NLP. These
models use massive datasets on the order of tens or
even hundreds of billions of words (Brown et al.,
2020) to learn linguistic features and world knowl-
edge, and they can be fine-tuned to achieve good
performance on many downstream tasks. Much recent work has used probing methods
to evaluate what these models have and have not
learned (Belinkov and Glass, 2019; Tenney et al.,
2019b; Rogers et al., 2020; Ettinger, 2020). Since
most of these works only focus on models pre-
trained on a fixed data volume (usually billions
of words), many interesting questions regarding
the effect of the amount of pretraining data remain
unanswered: What do data-rich models know that
models with less pretraining data do not? How
much pretraining data is required for LMs to learn
different grammatical features and linguistic phe-
nomena? Which of these skills do we expect to
improve if we increase the pretraining data to over
30 billion words? Which aspects of grammar can
be learned from data volumes on par with the input
to human learners, around 10M to 100M words
(Hart and Risley, 1992)?
With these questions in mind, we probe the
MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M,
and 1B words, and RoBERTaBASE(Liu et al., 2019)
pretrained on about 30B words, using four methods:
First we use standard classifier probing on the edge
probing suite of NLP tasks (Tenney et al., 2019b)
to measure the quality of the syntactic and semantic
features that can be extracted by a downstream clas-
sifier with each level of pretraining. Second, we ap-
ply minimum description length probing (Voita and
Titov, 2020) to the edge probing suite, with the goal
of quantifying the accessibility of these features.
Third, we probe the models’ knowledge of various
syntactic phenomena using unsupervised accept-
ability judgments on the BLiMP suite (Warstadt
et al., 2020a). Fourth, we fine-tune the models on
five tasks from SuperGLUE (Wang et al., 2019), to
measure their ability to solve conventional NLU
tasks.
Figure 1 shows the interpolated learning curves
for these four methods as a function of the amount
of pretraining data. We have two main findings:
First, the results of three probing methods we adopt
show that the linguistic knowledge of RoBERTa
pretrained on 100M words is already very close
to that of RoBERTaBASE, which is pretrained on
around 30B words. Second, RoBERTa requires bil-
lions of words of pretraining data to make substan-
tial improvements in performance on dowstream
NLU tasks. From these results, we conclude that
there are skills critical to solving downstream NLU
tasks that LMs can only acquire with billions of
words of pretraining data and that we need to look
beyond probing for linguistic features to explain
why LMs improve at these large data scales.",Can you tell me more about the information-theoretic probing?,Sorry but I can only answer the questions where I can find in the abstract or introduction.,"Hello! Interesting question as the title. Do you have any conclusion in this paper? Yes! We show that most linguistic features can be learned with training data between 10 million and 100 million, while the rest requires more.
 Facts: 
1. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.
2. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.
 10 to 100 million is a rather large range of data. What is the difference between learning from 10 or 100 million words?",[],train
When Do You Need Billions of Words of Pretraining Data?,"NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.

A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models. Pretrained language models (LMs) like BERT and
RoBERTa have become ubiquitous in NLP. These
models use massive datasets on the order of tens or
even hundreds of billions of words (Brown et al.,
2020) to learn linguistic features and world knowl-
edge, and they can be fine-tuned to achieve good
performance on many downstream tasks. Much recent work has used probing methods
to evaluate what these models have and have not
learned (Belinkov and Glass, 2019; Tenney et al.,
2019b; Rogers et al., 2020; Ettinger, 2020). Since
most of these works only focus on models pre-
trained on a fixed data volume (usually billions
of words), many interesting questions regarding
the effect of the amount of pretraining data remain
unanswered: What do data-rich models know that
models with less pretraining data do not? How
much pretraining data is required for LMs to learn
different grammatical features and linguistic phe-
nomena? Which of these skills do we expect to
improve if we increase the pretraining data to over
30 billion words? Which aspects of grammar can
be learned from data volumes on par with the input
to human learners, around 10M to 100M words
(Hart and Risley, 1992)?
With these questions in mind, we probe the
MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M,
and 1B words, and RoBERTaBASE(Liu et al., 2019)
pretrained on about 30B words, using four methods:
First we use standard classifier probing on the edge
probing suite of NLP tasks (Tenney et al., 2019b)
to measure the quality of the syntactic and semantic
features that can be extracted by a downstream clas-
sifier with each level of pretraining. Second, we ap-
ply minimum description length probing (Voita and
Titov, 2020) to the edge probing suite, with the goal
of quantifying the accessibility of these features.
Third, we probe the models’ knowledge of various
syntactic phenomena using unsupervised accept-
ability judgments on the BLiMP suite (Warstadt
et al., 2020a). Fourth, we fine-tune the models on
five tasks from SuperGLUE (Wang et al., 2019), to
measure their ability to solve conventional NLU
tasks.
Figure 1 shows the interpolated learning curves
for these four methods as a function of the amount
of pretraining data. We have two main findings:
First, the results of three probing methods we adopt
show that the linguistic knowledge of RoBERTa
pretrained on 100M words is already very close
to that of RoBERTaBASE, which is pretrained on
around 30B words. Second, RoBERTa requires bil-
lions of words of pretraining data to make substan-
tial improvements in performance on dowstream
NLU tasks. From these results, we conclude that
there are skills critical to solving downstream NLU
tasks that LMs can only acquire with billions of
words of pretraining data and that we need to look
beyond probing for linguistic features to explain
why LMs improve at these large data scales.",Which model did you use in your experiments?,We use RoBERTa-base and MiniBERTas which are RoBERTa models pretrained on scaled down data.,"Hello! Interesting question as the title. Do you have any conclusion in this paper? Yes! We show that most linguistic features can be learned with training data between 10 million and 100 million, while the rest requires more.
 Facts: 
1. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.
2. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.
 10 to 100 million is a rather large range of data. What is the difference between learning from 10 or 100 million words? The performances of our four testing cases related to syntactic information leverage the most compared to other scales.
 Facts: 
1. We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume
 Can you tell me more about the information-theoretic probing?","['with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words.']",train
When Do You Need Billions of Words of Pretraining Data?,"NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.

A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models. Pretrained language models (LMs) like BERT and
RoBERTa have become ubiquitous in NLP. These
models use massive datasets on the order of tens or
even hundreds of billions of words (Brown et al.,
2020) to learn linguistic features and world knowl-
edge, and they can be fine-tuned to achieve good
performance on many downstream tasks. Much recent work has used probing methods
to evaluate what these models have and have not
learned (Belinkov and Glass, 2019; Tenney et al.,
2019b; Rogers et al., 2020; Ettinger, 2020). Since
most of these works only focus on models pre-
trained on a fixed data volume (usually billions
of words), many interesting questions regarding
the effect of the amount of pretraining data remain
unanswered: What do data-rich models know that
models with less pretraining data do not? How
much pretraining data is required for LMs to learn
different grammatical features and linguistic phe-
nomena? Which of these skills do we expect to
improve if we increase the pretraining data to over
30 billion words? Which aspects of grammar can
be learned from data volumes on par with the input
to human learners, around 10M to 100M words
(Hart and Risley, 1992)?
With these questions in mind, we probe the
MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M,
and 1B words, and RoBERTaBASE(Liu et al., 2019)
pretrained on about 30B words, using four methods:
First we use standard classifier probing on the edge
probing suite of NLP tasks (Tenney et al., 2019b)
to measure the quality of the syntactic and semantic
features that can be extracted by a downstream clas-
sifier with each level of pretraining. Second, we ap-
ply minimum description length probing (Voita and
Titov, 2020) to the edge probing suite, with the goal
of quantifying the accessibility of these features.
Third, we probe the models’ knowledge of various
syntactic phenomena using unsupervised accept-
ability judgments on the BLiMP suite (Warstadt
et al., 2020a). Fourth, we fine-tune the models on
five tasks from SuperGLUE (Wang et al., 2019), to
measure their ability to solve conventional NLU
tasks.
Figure 1 shows the interpolated learning curves
for these four methods as a function of the amount
of pretraining data. We have two main findings:
First, the results of three probing methods we adopt
show that the linguistic knowledge of RoBERTa
pretrained on 100M words is already very close
to that of RoBERTaBASE, which is pretrained on
around 30B words. Second, RoBERTa requires bil-
lions of words of pretraining data to make substan-
tial improvements in performance on dowstream
NLU tasks. From these results, we conclude that
there are skills critical to solving downstream NLU
tasks that LMs can only acquire with billions of
words of pretraining data and that we need to look
beyond probing for linguistic features to explain
why LMs improve at these large data scales.","Besides the finding regarding number of training data, do you also have other findings?",We also find that commonsense knowledge could be a direction guiding pretraining of language models.,"Hello! Interesting question as the title. Do you have any conclusion in this paper? Yes! We show that most linguistic features can be learned with training data between 10 million and 100 million, while the rest requires more.
 Facts: 
1. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.
2. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.
 10 to 100 million is a rather large range of data. What is the difference between learning from 10 or 100 million words? The performances of our four testing cases related to syntactic information leverage the most compared to other scales.
 Facts: 
1. We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume
 Can you tell me more about the information-theoretic probing? Sorry but I can only answer the questions where I can find in the abstract or introduction. Which model did you use in your experiments?","['A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.', 'it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.']",train
When Do You Need Billions of Words of Pretraining Data?,"NLP is currently dominated by generalpurpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from largescale pretraining that they cannot learn from less data? We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.

A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models. Pretrained language models (LMs) like BERT and
RoBERTa have become ubiquitous in NLP. These
models use massive datasets on the order of tens or
even hundreds of billions of words (Brown et al.,
2020) to learn linguistic features and world knowl-
edge, and they can be fine-tuned to achieve good
performance on many downstream tasks. Much recent work has used probing methods
to evaluate what these models have and have not
learned (Belinkov and Glass, 2019; Tenney et al.,
2019b; Rogers et al., 2020; Ettinger, 2020). Since
most of these works only focus on models pre-
trained on a fixed data volume (usually billions
of words), many interesting questions regarding
the effect of the amount of pretraining data remain
unanswered: What do data-rich models know that
models with less pretraining data do not? How
much pretraining data is required for LMs to learn
different grammatical features and linguistic phe-
nomena? Which of these skills do we expect to
improve if we increase the pretraining data to over
30 billion words? Which aspects of grammar can
be learned from data volumes on par with the input
to human learners, around 10M to 100M words
(Hart and Risley, 1992)?
With these questions in mind, we probe the
MiniBERTas (Warstadt et al., 2020b), a group of RoBERTa models pretrained on 1M, 10M, 100M,
and 1B words, and RoBERTaBASE(Liu et al., 2019)
pretrained on about 30B words, using four methods:
First we use standard classifier probing on the edge
probing suite of NLP tasks (Tenney et al., 2019b)
to measure the quality of the syntactic and semantic
features that can be extracted by a downstream clas-
sifier with each level of pretraining. Second, we ap-
ply minimum description length probing (Voita and
Titov, 2020) to the edge probing suite, with the goal
of quantifying the accessibility of these features.
Third, we probe the models’ knowledge of various
syntactic phenomena using unsupervised accept-
ability judgments on the BLiMP suite (Warstadt
et al., 2020a). Fourth, we fine-tune the models on
five tasks from SuperGLUE (Wang et al., 2019), to
measure their ability to solve conventional NLU
tasks.
Figure 1 shows the interpolated learning curves
for these four methods as a function of the amount
of pretraining data. We have two main findings:
First, the results of three probing methods we adopt
show that the linguistic knowledge of RoBERTa
pretrained on 100M words is already very close
to that of RoBERTaBASE, which is pretrained on
around 30B words. Second, RoBERTa requires bil-
lions of words of pretraining data to make substan-
tial improvements in performance on dowstream
NLU tasks. From these results, we conclude that
there are skills critical to solving downstream NLU
tasks that LMs can only acquire with billions of
words of pretraining data and that we need to look
beyond probing for linguistic features to explain
why LMs improve at these large data scales.","To integrate other forms of knowledge in a less data scenario, does the model require labeled data? Can we include such knowledge by defining knowledge-specific objectives and learning on the unlabeled data?","I believe we can define specific objectives regarding this, though we didn't mention what exactly to do in the paper.","Hello! Interesting question as the title. Do you have any conclusion in this paper? Yes! We show that most linguistic features can be learned with training data between 10 million and 100 million, while the rest requires more.
 Facts: 
1. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test.
2. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.
 10 to 100 million is a rather large range of data. What is the difference between learning from 10 or 100 million words? The performances of our four testing cases related to syntactic information leverage the most compared to other scales.
 Facts: 
1. We adopt four probing methods classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks-and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume
 Can you tell me more about the information-theoretic probing? Sorry but I can only answer the questions where I can find in the abstract or introduction. Which model did you use in your experiments? We use RoBERTa-base and MiniBERTas which are RoBERTa models pretrained on scaled down data.
 Facts: 
1. with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words.
 Besides the finding regarding number of training data, do you also have other findings?","['The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.']",train
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",Hi,Hi!,,[],val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",Is this work about natural language processing or social sciences?,This is work about natural language processing.,Hi,"['In this paper, we present the first computational study of parody.']",val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",How is the study conducted?,"First, we are collecting data from Twitter. Then, we perform on analysis of the data. Finally, we train models on our data to predict whether a tweet is parody or not.",Hi Hi! Is this work about natural language processing or social sciences?,"['We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets', 'Finally, we identify the markers of parody through a linguistic analysis.']",val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",How many annotators were involved in corpus creation?,"The corpus was created by automatically retrieving tweets from parody accounts. Parody accounts were identified by the authors of this work. In total, 4 researchers took a look at the data.","Hi Hi! Is this work about natural language processing or social sciences? This is work about natural language processing.
 Facts: 
1. In this paper, we present the first computational study of parody.
 How is the study conducted?","['For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries']",val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",Interesting work. What features are more indicative of parody?,"In our work we examine political parody. In political parody, exaggerated views and caricatures are the main features. Posts are also made to mimic real facts, but with a humorous twist.","Hi Hi! Is this work about natural language processing or social sciences? This is work about natural language processing.
 Facts: 
1. In this paper, we present the first computational study of parody.
 How is the study conducted? First, we are collecting data from Twitter. Then, we perform on analysis of the data. Finally, we train models on our data to predict whether a tweet is parody or not.
 Facts: 
1. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets
2. Finally, we identify the markers of parody through a linguistic analysis.
 How many annotators were involved in corpus creation?","['Parody tweets can often be misinterpreted as facts', 'these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas']",val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .","There may be some links to other NLP tasks, like irony detection. What do you think?","I agree with this statement. Irony can be used to make a parody post, so detecting irony could facilitate better parody detection as well.","Hi Hi! Is this work about natural language processing or social sciences? This is work about natural language processing.
 Facts: 
1. In this paper, we present the first computational study of parody.
 How is the study conducted? First, we are collecting data from Twitter. Then, we perform on analysis of the data. Finally, we train models on our data to predict whether a tweet is parody or not.
 Facts: 
1. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets
2. Finally, we identify the markers of parody through a linguistic analysis.
 How many annotators were involved in corpus creation? The corpus was created by automatically retrieving tweets from parody accounts. Parody accounts were identified by the authors of this work. In total, 4 researchers took a look at the data.
 Facts: 
1. For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries
 Interesting work. What features are more indicative of parody?","['While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods.', 'We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond.']",val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",How about fact checking?,"While fact checking could definitely help, I believe we would not get enough use out of fact checking. Verifying facts can be very challenging and gains would be minimal. Furthermore, while parody can indeed be misinterpreted as facts, it mostly relies on the style of writing (ie. mimicking the real politician) and not so much the content.","Hi Hi! Is this work about natural language processing or social sciences? This is work about natural language processing.
 Facts: 
1. In this paper, we present the first computational study of parody.
 How is the study conducted? First, we are collecting data from Twitter. Then, we perform on analysis of the data. Finally, we train models on our data to predict whether a tweet is parody or not.
 Facts: 
1. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets
2. Finally, we identify the markers of parody through a linguistic analysis.
 How many annotators were involved in corpus creation? The corpus was created by automatically retrieving tweets from parody accounts. Parody accounts were identified by the authors of this work. In total, 4 researchers took a look at the data.
 Facts: 
1. For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries
 Interesting work. What features are more indicative of parody? In our work we examine political parody. In political parody, exaggerated views and caricatures are the main features. Posts are also made to mimic real facts, but with a humorous twist.
 Facts: 
1. Parody tweets can often be misinterpreted as facts
2. these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas
 There may be some links to other NLP tasks, like irony detection. What do you think?",['Parody tweets can often be misinterpreted as facts'],val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",Great work. Thanks for the chat,"Thank you, have a nice day!","Hi Hi! Is this work about natural language processing or social sciences? This is work about natural language processing.
 Facts: 
1. In this paper, we present the first computational study of parody.
 How is the study conducted? First, we are collecting data from Twitter. Then, we perform on analysis of the data. Finally, we train models on our data to predict whether a tweet is parody or not.
 Facts: 
1. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets
2. Finally, we identify the markers of parody through a linguistic analysis.
 How many annotators were involved in corpus creation? The corpus was created by automatically retrieving tweets from parody accounts. Parody accounts were identified by the authors of this work. In total, 4 researchers took a look at the data.
 Facts: 
1. For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries
 Interesting work. What features are more indicative of parody? In our work we examine political parody. In political parody, exaggerated views and caricatures are the main features. Posts are also made to mimic real facts, but with a humorous twist.
 Facts: 
1. Parody tweets can often be misinterpreted as facts
2. these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas
 There may be some links to other NLP tasks, like irony detection. What do you think? I agree with this statement. Irony can be used to make a parody post, so detecting irony could facilitate better parody detection as well.
 Facts: 
1. While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods.
2. We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond.
 How about fact checking?",[],val
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .","Hi, how is political parody defined the paper?",Parody is defined as a means to mimic behavior of a target for comedic purposes.,,"['Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.', 'Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons']",train
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",Can it be detected automatically in social media posts?,It can! In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.,"Hi, how is political parody defined the paper?","['Our results show that political parody tweets can be predicted with an accuracy up to 90%', 'Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.']",train
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .","Can the model distinguish political parody from other forms of parody? The definition seemed to allow for non-political parody, too.","Here we only collected political parody data, so the models only learn to identify data in that domain. We have not investigated whether the same model could detect general parody. I believe that while it is possible to detect non-political parody, more general data would be needed for that task.","Hi, how is political parody defined the paper? Parody is defined as a means to mimic behavior of a target for comedic purposes.
 Facts: 
1. Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.
2. Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons
 Can it be detected automatically in social media posts?","['Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Rober']",train
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .","I see. So the model distinguishes political parody from other posts without any parody, yes? Is it possible that the model only detects the political topic without learning about parody at all?","The data is all in the political domain. Data comes from politician accounts as well as political parody accounts. So, the model operates entirely within the political domain, and learns to separate between parody and non-parody data.","Hi, how is political parody defined the paper? Parody is defined as a means to mimic behavior of a target for comedic purposes.
 Facts: 
1. Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.
2. Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons
 Can it be detected automatically in social media posts? It can! In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.
 Facts: 
1. Our results show that political parody tweets can be predicted with an accuracy up to 90%
2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.
 Can the model distinguish political parody from other forms of parody? The definition seemed to allow for non-political parody, too.","['For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries']",train
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",That is very interesting. The resource sounds very useful. Are there annotations on the tweet-level or do you assume that a parody account only tweets parody and the others don't? Could the model be fooled into an author identification task?,"Annotations are on the account level. That is, a parody account only tweets parody, while a real account only tweets normal speech. I do not believe this task will be reduced to author identification though. In our experiments, we also predict parody from previously unseen accounts (both parody and real) with the model performing well on those. This goes to show that the model is able to identify parody to an adequate level.)","Hi, how is political parody defined the paper? Parody is defined as a means to mimic behavior of a target for comedic purposes.
 Facts: 
1. Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.
2. Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons
 Can it be detected automatically in social media posts? It can! In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.
 Facts: 
1. Our results show that political parody tweets can be predicted with an accuracy up to 90%
2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.
 Can the model distinguish political parody from other forms of parody? The definition seemed to allow for non-political parody, too. Here we only collected political parody data, so the models only learn to identify data in that domain. We have not investigated whether the same model could detect general parody. I believe that while it is possible to detect non-political parody, more general data would be needed for that task.
 Facts: 
1. Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Rober
 I see. So the model distinguishes political parody from other posts without any parody, yes? Is it possible that the model only detects the political topic without learning about parody at all?","['test data from: a) users; b) genders; c) locations; unseen in training', 'Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures.']",train
Analyzing Political Parody in Social Media,"Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts. In this paper, we present the first computational study of parody. We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts. We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.

Our results show that political parody tweets can be predicted with an accuracy up to 90%. Finally, we identify the markers of parody through a linguistic analysis. Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances. 1 * Equal contribution.

† Work was done while at the University of Sheffield. 1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person. 2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures. Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.

There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona. A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) . Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter. Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.

While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods. In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media. To this end, we make the following contributions: 1. A novel classification task where we seek to automatically classify real and parody tweets.

For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1. These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3. Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).

We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'. 5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .

Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) .",That is very convincing. Thank you for the explanations. Could you tell me a bit about the analyzing part?,"We performed a linguistic analysis, showing that a major difference between politician and parody accounts is the use of first-person pronouns in parody accounts. We also analyzed errors on the model's side, with humorous tweets from real politician's sometimes being confused for parody.","Hi, how is political parody defined the paper? Parody is defined as a means to mimic behavior of a target for comedic purposes.
 Facts: 
1. Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.
2. Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) . Traditional forms of parody include editorial cartoons
 Can it be detected automatically in social media posts? It can! In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.
 Facts: 
1. Our results show that political parody tweets can be predicted with an accuracy up to 90%
2. Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.
 Can the model distinguish political parody from other forms of parody? The definition seemed to allow for non-political parody, too. Here we only collected political parody data, so the models only learn to identify data in that domain. We have not investigated whether the same model could detect general parody. I believe that while it is possible to detect non-political parody, more general data would be needed for that task.
 Facts: 
1. Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Rober
 I see. So the model distinguishes political parody from other posts without any parody, yes? Is it possible that the model only detects the political topic without learning about parody at all? The data is all in the political domain. Data comes from politician accounts as well as political parody accounts. So, the model operates entirely within the political domain, and learns to separate between parody and non-parody data.
 Facts: 
1. For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries
 That is very interesting. The resource sounds very useful. Are there annotations on the tweet-level or do you assume that a parody account only tweets parody and the others don't? Could the model be fooled into an author identification task?",['Linguistic analysis of the markers of parody tweets and of the model errors'],train
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Hi, what is this paper about?",Hi! This paper assess the impact of knowledge injection in a data-driven approach.,,"['Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",This sounds very interesting. How exactly do you inject knowledge in the network?,Unfourtunately I have not enough information to answer your question.,"Hi, what is this paper about?",[],val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Ok, thanks anyhow. Which kind of problems has the method you propose been applied to?",We applied to a particular class of Constrained Satisfaction problem. Do you want to know which one?,"Hi, what is this paper about? Hi! This paper assess the impact of knowledge injection in a data-driven approach.
 Facts: 
1. Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.
 This sounds very interesting. How exactly do you inject knowledge in the network?","['One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.', ' As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Yes, please. Which kind of CSP problems?",We employ the Partial Latin Square completion problem which require to complete a partially filled NxN square with all the values in the range from 1 to N such that no value appear twice in any row and column.,"Hi, what is this paper about? Hi! This paper assess the impact of knowledge injection in a data-driven approach.
 Facts: 
1. Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.
 This sounds very interesting. How exactly do you inject knowledge in the network? Unfourtunately I have not enough information to answer your question. Ok, thanks anyhow. Which kind of problems has the method you propose been applied to?","['We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.",Thank you. I believe that plugging knowledge into neural architectures is a crucial issue for AI in the coming years. Do you need to encode the knowledge in some language? Does knowledge injection affect training time?,Our method takes inspiration from Constraint Programming  and it can be applied also at test time.,"Hi, what is this paper about? Hi! This paper assess the impact of knowledge injection in a data-driven approach.
 Facts: 
1. Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.
 This sounds very interesting. How exactly do you inject knowledge in the network? Unfourtunately I have not enough information to answer your question. Ok, thanks anyhow. Which kind of problems has the method you propose been applied to? We applied to a particular class of Constrained Satisfaction problem. Do you want to know which one?
 Facts: 
1. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.
2.  As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.
 Yes, please. Which kind of CSP problems?","['As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.', 'We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time']",val
Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem,"Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available. It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.

Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results. Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy. In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process. Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce. In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.

On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems. Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric. Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs). Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.

Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column. Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g. in optical fiber routing), and serves as the basis for more complex problems (e.g.

timetabling). Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions). This combination enables controlled experiments that are difficult to perform on more traditional datasets. We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.

We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions. Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results. As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains. While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.

Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems. The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks.","Thank you, this is all I wanted to know!",You are welcome!,"Hi, what is this paper about? Hi! This paper assess the impact of knowledge injection in a data-driven approach.
 Facts: 
1. Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.
 This sounds very interesting. How exactly do you inject knowledge in the network? Unfourtunately I have not enough information to answer your question. Ok, thanks anyhow. Which kind of problems has the method you propose been applied to? We applied to a particular class of Constrained Satisfaction problem. Do you want to know which one?
 Facts: 
1. One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.
2.  As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.
 Yes, please. Which kind of CSP problems? We employ the Partial Latin Square completion problem which require to complete a partially filled NxN square with all the values in the range from 1 to N such that no value appear twice in any row and column.
 Facts: 
1. We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n × n square with values in {1..n}, such that no value appears twice on any row or column.
 Thank you. I believe that plugging knowledge into neural architectures is a crucial issue for AI in the coming years. Do you need to encode the knowledge in some language? Does knowledge injection affect training time?",[],val
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Hi! What is the article about?,The article is about a new neural architecture to perform Argument Mining. This architecture features an attention module and is validated on multiple corpora. Which aspect would you like to know more about?,,"['A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information', 'A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Can you tell me how does the attention module work?,"I'm sorry, but I don't have information on this aspect. Would you like to know more about the argument mining task instead?",Hi! What is the article about?,"['The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.","Yes, tell me more about it.",Argument Mining is a research field related to NLP that concerns the extraction and the classification of arguments and their relationships.,"Hi! What is the article about? The article is about a new neural architecture to perform Argument Mining. This architecture features an attention module and is validated on multiple corpora. Which aspect would you like to know more about?
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information
2. A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model
 Can you tell me how does the attention module work?","['A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] .', 'Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",What kind of corpora was the neural architecture tested on?,It was tested on corpora composed by user-generated comments and by scientific publications,"Hi! What is the article about? The article is about a new neural architecture to perform Argument Mining. This architecture features an attention module and is validated on multiple corpora. Which aspect would you like to know more about?
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information
2. A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model
 Can you tell me how does the attention module work? I'm sorry, but I don't have information on this aspect. Would you like to know more about the argument mining task instead?
 Facts: 
1. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them.
 Yes, tell me more about it.","['We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",On which of the two did the architecture perform best?,The best performance was achieved on the first one,"Hi! What is the article about? The article is about a new neural architecture to perform Argument Mining. This architecture features an attention module and is validated on multiple corpora. Which aspect would you like to know more about?
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information
2. A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model
 Can you tell me how does the attention module work? I'm sorry, but I don't have information on this aspect. Would you like to know more about the argument mining task instead?
 Facts: 
1. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them.
 Yes, tell me more about it. Argument Mining is a research field related to NLP that concerns the extraction and the classification of arguments and their relationships.
 Facts: 
1. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] .
2. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.
 What kind of corpora was the neural architecture tested on?","['On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Do you know why this happened?,There is no information available about this. Do you want to know more or are you satisfied?,"Hi! What is the article about? The article is about a new neural architecture to perform Argument Mining. This architecture features an attention module and is validated on multiple corpora. Which aspect would you like to know more about?
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information
2. A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model
 Can you tell me how does the attention module work? I'm sorry, but I don't have information on this aspect. Would you like to know more about the argument mining task instead?
 Facts: 
1. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them.
 Yes, tell me more about it. Argument Mining is a research field related to NLP that concerns the extraction and the classification of arguments and their relationships.
 Facts: 
1. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] .
2. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.
 What kind of corpora was the neural architecture tested on? It was tested on corpora composed by user-generated comments and by scientific publications
 Facts: 
1. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.
 On which of the two did the architecture perform best?",[],train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Hi! What is this paper about?,Hello! This paper is about a novel neural architecture proposed for the task of Argument Mining.,,"['A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning', 'We explore the use of residual networks and neural attention for argument mining and in particular link prediction.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",What does it mean 'attentive' network?,"it is not explicitly stated, but it means that the network include a neural attention module",Hi! What is this paper about?,"['The model exploits neural attention', 'this paper extends the neural architecture with attention and ensemble learning']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",What is the attention module used for?,"I'm sorry, such information is not available. Would you like to know more about the argument mining task?","Hi! What is this paper about? Hello! This paper is about a novel neural architecture proposed for the task of Argument Mining.
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning
2. We explore the use of residual networks and neural attention for argument mining and in particular link prediction.
 What does it mean 'attentive' network?",[],train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.","Yes, thank you","Argument mining is a research field related to NLP, and concerns the extraction and classification of argumentative components and their relationships in documents. Are you satisfied or would you like to know more?","Hi! What is this paper about? Hello! This paper is about a novel neural architecture proposed for the task of Argument Mining.
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning
2. We explore the use of residual networks and neural attention for argument mining and in particular link prediction.
 What does it mean 'attentive' network? it is not explicitly stated, but it means that the network include a neural attention module
 Facts: 
1. The model exploits neural attention
2. this paper extends the neural architecture with attention and ensemble learning
 What is the attention module used for?","['A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] .', 'In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.","I would like to know more. For example, what type of argument mining did you do in this research?",We focus on classifying the category of the component and predicting the relationships,"Hi! What is this paper about? Hello! This paper is about a novel neural architecture proposed for the task of Argument Mining.
 Facts: 
1. A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning
2. We explore the use of residual networks and neural attention for argument mining and in particular link prediction.
 What does it mean 'attentive' network? it is not explicitly stated, but it means that the network include a neural attention module
 Facts: 
1. The model exploits neural attention
2. this paper extends the neural architecture with attention and ensemble learning
 What is the attention module used for? I'm sorry, such information is not available. Would you like to know more about the argument mining task? Yes, thank you","['The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Hello! Can you give me a general overview of what is this paper about?,The paper explais how a novel dataset of images and potentially offensive comments was created.,,"['In this paper, we present a novel dataset composed of images and comments in Italian']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Interesting! How did they select those images?,"The images were taken randomly from Instagram, but cannot be shared as pictures.",Hello! Can you give me a general overview of what is this paper about?,"['The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",How did they annotate those images? using human annotators or in some unsupervised fashion?,"The comments were made by teenagers during their school hours, then they were manually assigned to a semantic category.","Hello! Can you give me a general overview of what is this paper about? The paper explais how a novel dataset of images and potentially offensive comments was created.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian
 Interesting! How did they select those images?","['In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.', 'Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Oh interesting! and have the researchers also conducted any experiment on this dataset?,"Unfortunately, I connot provide this information, but the dataset is freely available on Github and can be used for research purposes.","Hello! Can you give me a general overview of what is this paper about? The paper explais how a novel dataset of images and potentially offensive comments was created.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian
 Interesting! How did they select those images? The images were taken randomly from Instagram, but cannot be shared as pictures.
 Facts: 
1. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 How did they annotate those images? using human annotators or in some unsupervised fashion?","['The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Why is dataset multimodal?,The dataset was analysed from different perspectives. For example the analysys could consider the presence of human subjects or the gender of the people.,"Hello! Can you give me a general overview of what is this paper about? The paper explais how a novel dataset of images and potentially offensive comments was created.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian
 Interesting! How did they select those images? The images were taken randomly from Instagram, but cannot be shared as pictures.
 Facts: 
1. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 How did they annotate those images? using human annotators or in some unsupervised fashion? The comments were made by teenagers during their school hours, then they were manually assigned to a semantic category.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
2. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category.
 Oh interesting! and have the researchers also conducted any experiment on this dataset?","['We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.', 'Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.","Thank you, is there any other aspect worth of mention?",The corpus was also compared with a similar one obtained using whatsapp. This comparison showed that comments to images have different characteristics.,"Hello! Can you give me a general overview of what is this paper about? The paper explais how a novel dataset of images and potentially offensive comments was created.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian
 Interesting! How did they select those images? The images were taken randomly from Instagram, but cannot be shared as pictures.
 Facts: 
1. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 How did they annotate those images? using human annotators or in some unsupervised fashion? The comments were made by teenagers during their school hours, then they were manually assigned to a semantic category.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
2. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category.
 Oh interesting! and have the researchers also conducted any experiment on this dataset? Unfortunately, I connot provide this information, but the dataset is freely available on Github and can be used for research purposes.
 Facts: 
1. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users.
 Why is dataset multimodal?","['We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",- Hello there! Can you provide to me some information about the paper? ,Hi! This paper explains how a dataset composed of images and comments was collected...,,"['In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.']",val
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",With what strategy were the data collected?,The images were randomly selected from Instagram and the comments were created by teenagers during their school hours.,- Hello there! Can you provide to me some information about the paper? ,"['The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.', ""The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.""]",val
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",How was the data labeled?,The comments were maually assigned to a semantic category.,"- Hello there! Can you provide to me some information about the paper?  Hi! This paper explains how a dataset composed of images and comments was collected...
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 With what strategy were the data collected?","['Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category.']",val
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.","According to the authors, are there any other datasets like this in other languages?","I cannot provide a complete answer, but collecting data from teenage users can be problematic due to privacy restrictions.","- Hello there! Can you provide to me some information about the paper?  Hi! This paper explains how a dataset composed of images and comments was collected...
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 With what strategy were the data collected? The images were randomly selected from Instagram and the comments were created by teenagers during their school hours.
 Facts: 
1. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
2. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
 How was the data labeled?","['However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.']",val
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",I think such a dataset could be too conditioned/biased by the type of annotator. What do you think about it?,"The aim of the dataset is to collect data specifically among teenagers to raise awareness on cyberbullying, but I think that collecting data while the teens know what they are doing could make them write less mean comments on purpose.","- Hello there! Can you provide to me some information about the paper?  Hi! This paper explains how a dataset composed of images and comments was collected...
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 With what strategy were the data collected? The images were randomly selected from Instagram and the comments were created by teenagers during their school hours.
 Facts: 
1. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
2. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
 How was the data labeled? The comments were maually assigned to a semantic category.
 Facts: 
1. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category.
 According to the authors, are there any other datasets like this in other languages?","[""The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.""]",val
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Is the dataset available and free to use?,"Yes, you can find the comments on github. Unfortunately the images cannot be shared as pictures.","- Hello there! Can you provide to me some information about the paper?  Hi! This paper explains how a dataset composed of images and comments was collected...
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 With what strategy were the data collected? The images were randomly selected from Instagram and the comments were created by teenagers during their school hours.
 Facts: 
1. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
2. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
 How was the data labeled? The comments were maually assigned to a semantic category.
 Facts: 
1. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category.
 According to the authors, are there any other datasets like this in other languages? I cannot provide a complete answer, but collecting data from teenage users can be problematic due to privacy restrictions.
 Facts: 
1. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.
 I think such a dataset could be too conditioned/biased by the type of annotator. What do you think about it?","['The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users.', 'The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.']",val
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Hi! What is this paper about?,The paper explains how a dataset of pictures and possibly offensive comments made by teenagers was collected.,,"['In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",How was this dataset collected?,The images were taken from Instagram and the comments were made by teenagers during their school hours.,Hi! What is this paper about?,"[""The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers."", 'The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",What was the purpose of this data collection?,The authors wanted to collect multimodal data from a specific target group.,"Hi! What is this paper about? The paper explains how a dataset of pictures and possibly offensive comments made by teenagers was collected.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 How was this dataset collected?","['However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.', 'Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",What are the limits and challenges of such collection?,"Almos all the issues are related to privacy, collecting data from teenagers means that the authorization of their parents is necessary.","Hi! What is this paper about? The paper explains how a dataset of pictures and possibly offensive comments made by teenagers was collected.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 How was this dataset collected? The images were taken from Instagram and the comments were made by teenagers during their school hours.
 Facts: 
1. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
2. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 What was the purpose of this data collection?","['However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",How do the authors used or plan to use these data?,The authors analysed the data in order to understand if there is a connection between the subject of the image and the comments.,"Hi! What is this paper about? The paper explains how a dataset of pictures and possibly offensive comments made by teenagers was collected.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 How was this dataset collected? The images were taken from Instagram and the comments were made by teenagers during their school hours.
 Facts: 
1. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
2. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 What was the purpose of this data collection? The authors wanted to collect multimodal data from a specific target group.
 Facts: 
1. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.
2. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data.
 What are the limits and challenges of such collection?","[' We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",Where can I find this dataset? Is it available?,"Yes, the comments are freely available on Github but the images cannot be released in their original form because they were taken from Instagram.","Hi! What is this paper about? The paper explains how a dataset of pictures and possibly offensive comments made by teenagers was collected.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 How was this dataset collected? The images were taken from Instagram and the comments were made by teenagers during their school hours.
 Facts: 
1. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
2. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 What was the purpose of this data collection? The authors wanted to collect multimodal data from a specific target group.
 Facts: 
1. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.
2. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data.
 What are the limits and challenges of such collection? Almos all the issues are related to privacy, collecting data from teenagers means that the authorization of their parents is necessary.
 Facts: 
1. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.
 How do the authors used or plan to use these data?","['The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users.', 'The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.']",train
A Multimodal Dataset of Images and Text to Study Abusive Language,"English. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena. Potentially offensive comments have been collected for more than 1,000 images and manually assigned to a semantic category. Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.

We also compare our corpus with a similar one obtained with What-sApp, showing that comments to images show different characteristics compared to text-only interactions. 1 In order to study abusive language online, the availability of datasets containing the linguistic phenomena of interest are of crucial importance. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data. We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.

The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers. The dataset is freely available on Github 2 and, since the comments were collected with the written consent of parents and teachers, they can be freely used for research purposes, without the ethical implications that would derive from using real data posted by teenage users. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.",What kind of subjects do the images contain?,"The images can contain any subject, the analysys considers the presence of humans and their genders.","Hi! What is this paper about? The paper explains how a dataset of pictures and possibly offensive comments made by teenagers was collected.
 Facts: 
1. In this paper, we present a novel dataset composed of images and comments in Italian, created with teenagers in classes using a simulated scenario to raise awareness on cyberbullying phenomena.
 How was this dataset collected? The images were taken from Instagram and the comments were made by teenagers during their school hours.
 Facts: 
1. The data collection was carried out in several school classes, being part of a 'living lab' to raise awareness on cyberbullying and, more generally, on the use of social media by teenagers.
2. The images, instead, are released as a ResNet-18 neural network trained on ImageNet, similar to recent NLP works (Kruk et al., 2019) , since they were taken from Instagram and cannot be shared as pictures.
 What was the purpose of this data collection? The authors wanted to collect multimodal data from a specific target group.
 Facts: 
1. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.
2. Furthermore, while text-only datasets for abusive language detection have been widely developed and used by the NLP community, limitations set by image-based social media platforms like Instagram make it difficult for researchers to experiment with multimodal data.
 What are the limits and challenges of such collection? Almos all the issues are related to privacy, collecting data from teenagers means that the authorization of their parents is necessary.
 Facts: 
1. However, when it comes to specific target groups, for example teenagers, collecting such data may be problematic due to issues with consent and privacy restrictions.
 How do the authors used or plan to use these data? The authors analysed the data in order to understand if there is a connection between the subject of the image and the comments.
 Facts: 
1.  We therefore present a novel corpus containing images and potentially offensive Italian comments and we analyse it from different perspectives, to investi-gate whether the subject of the images plays a role in triggering a comment.
 Where can I find this dataset? Is it available?","['Our analysis shows that the presence of human subjects, as well as the gender of the people present in the pictures trigger different types of comment, and provides novel insight into the connection between images posted on social media and offensive messages.']",train
Dialogue Coherence Assessment Without Explicit Dialogue Act Labels,"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment.

We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the Dai-lyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code 1 . Considering rapid progresses in developing open-domain dialogue agents Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019) , the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019) . Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016) , and the latter is timeconsuming and subjective.

A fundamental aspect of dialogue is coherence -what discriminates a Figure 1 : A high-level view of our multi-task learning approach for dialogue coherence modeling. high-quality dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998) . Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018) . A Dialogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of ""illocutionary force"", and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019) .

A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005) , and augment them with dialogue-relevant features, e.g., DA labels (Cervone et al., 2018) .

These DA labels are provided by human annotators or DA prediction models. Such coherence models suffer from the following drawbacks: (a) they curb semantic representations of utterances to entities, which are sparse in dialogue because of short utterance lengths, and (b) their performance relies on the quality of their input DA labels. We propose a novel approach to dialogue coherence assessment by utilizing dialogue act prediction as an auxiliary task for training our coherence model in a multi-task learning (MTL) scenario (Figure 1 ). Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh) , and a Dialogue Act Prediction (DAP) model.

The layers of the utterance encoder are shared between the DAP and the DiCoh model. This idea enables our Di-Coh model to learn to focus on salient information presented in utterances considering their DAs and to alleviate the need for explicit DA labels during coherence assessment. We evaluate our MTL-based approach on the DailyDialog (Li et al., 2017) and SwitchBoard (Jurafsky and Shriberg, 1997) English dialogue corpora in several discriminating experiments, where our coherence model, DiCoh, is examined to discriminate a dialogue from its perturbations (see Table 1 ). We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.

Our core contributions are: (1) proposing an MTL-based approach for dialogue coherence assessment using DAP as an auxiliary task, yielding more informative utterance representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations;

 (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-the-art coherence model on DailyDialog, and performs on par with it on SwitchBoard.","Hello there, what are some examples for explicit dialogue act labels?","Hi, The given text does not provide any specific example of DA. However, DA seems to related to what a speaker intention is of saying a sentence",,"[""A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. ""]",train
Dialogue Coherence Assessment Without Explicit Dialogue Act Labels,"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment.

We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the Dai-lyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code 1 . Considering rapid progresses in developing open-domain dialogue agents Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019) , the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019) . Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016) , and the latter is timeconsuming and subjective.

A fundamental aspect of dialogue is coherence -what discriminates a Figure 1 : A high-level view of our multi-task learning approach for dialogue coherence modeling. high-quality dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998) . Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018) . A Dialogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of ""illocutionary force"", and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019) .

A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005) , and augment them with dialogue-relevant features, e.g., DA labels (Cervone et al., 2018) .

These DA labels are provided by human annotators or DA prediction models. Such coherence models suffer from the following drawbacks: (a) they curb semantic representations of utterances to entities, which are sparse in dialogue because of short utterance lengths, and (b) their performance relies on the quality of their input DA labels. We propose a novel approach to dialogue coherence assessment by utilizing dialogue act prediction as an auxiliary task for training our coherence model in a multi-task learning (MTL) scenario (Figure 1 ). Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh) , and a Dialogue Act Prediction (DAP) model.

The layers of the utterance encoder are shared between the DAP and the DiCoh model. This idea enables our Di-Coh model to learn to focus on salient information presented in utterances considering their DAs and to alleviate the need for explicit DA labels during coherence assessment. We evaluate our MTL-based approach on the DailyDialog (Li et al., 2017) and SwitchBoard (Jurafsky and Shriberg, 1997) English dialogue corpora in several discriminating experiments, where our coherence model, DiCoh, is examined to discriminate a dialogue from its perturbations (see Table 1 ). We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.

Our core contributions are: (1) proposing an MTL-based approach for dialogue coherence assessment using DAP as an auxiliary task, yielding more informative utterance representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations;

 (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-the-art coherence model on DailyDialog, and performs on par with it on SwitchBoard.",How is dialogue coherence defined?,dialogue coherence deals with the semantic and DA's relations between sentence in a dialogue. ,"Hello there, what are some examples for explicit dialogue act labels?",['Dialogue coherence deals with semantic relations between utterances considering their dialogue acts '],train
Dialogue Coherence Assessment Without Explicit Dialogue Act Labels,"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment.

We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the Dai-lyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code 1 . Considering rapid progresses in developing open-domain dialogue agents Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019) , the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019) . Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016) , and the latter is timeconsuming and subjective.

A fundamental aspect of dialogue is coherence -what discriminates a Figure 1 : A high-level view of our multi-task learning approach for dialogue coherence modeling. high-quality dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998) . Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018) . A Dialogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of ""illocutionary force"", and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019) .

A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005) , and augment them with dialogue-relevant features, e.g., DA labels (Cervone et al., 2018) .

These DA labels are provided by human annotators or DA prediction models. Such coherence models suffer from the following drawbacks: (a) they curb semantic representations of utterances to entities, which are sparse in dialogue because of short utterance lengths, and (b) their performance relies on the quality of their input DA labels. We propose a novel approach to dialogue coherence assessment by utilizing dialogue act prediction as an auxiliary task for training our coherence model in a multi-task learning (MTL) scenario (Figure 1 ). Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh) , and a Dialogue Act Prediction (DAP) model.

The layers of the utterance encoder are shared between the DAP and the DiCoh model. This idea enables our Di-Coh model to learn to focus on salient information presented in utterances considering their DAs and to alleviate the need for explicit DA labels during coherence assessment. We evaluate our MTL-based approach on the DailyDialog (Li et al., 2017) and SwitchBoard (Jurafsky and Shriberg, 1997) English dialogue corpora in several discriminating experiments, where our coherence model, DiCoh, is examined to discriminate a dialogue from its perturbations (see Table 1 ). We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.

Our core contributions are: (1) proposing an MTL-based approach for dialogue coherence assessment using DAP as an auxiliary task, yielding more informative utterance representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations;

 (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-the-art coherence model on DailyDialog, and performs on par with it on SwitchBoard.","I see, and how do authors suggest evaluating dialogue coherence without using ecplicit labels?",The remove DA labels from the test set. They perturb the order of sentences in a dialogue. Then check how well a coherence model can distinguish an original dialogue from its perturbation..,"Hello there, what are some examples for explicit dialogue act labels? Hi, The given text does not provide any specific example of DA. However, DA seems to related to what a speaker intention is of saying a sentence
 Facts: 
1. A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. 
 How is dialogue coherence defined?","['We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.']",train
Dialogue Coherence Assessment Without Explicit Dialogue Act Labels,"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment.

We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the Dai-lyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code 1 . Considering rapid progresses in developing open-domain dialogue agents Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019) , the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019) . Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016) , and the latter is timeconsuming and subjective.

A fundamental aspect of dialogue is coherence -what discriminates a Figure 1 : A high-level view of our multi-task learning approach for dialogue coherence modeling. high-quality dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998) . Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018) . A Dialogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of ""illocutionary force"", and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019) .

A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005) , and augment them with dialogue-relevant features, e.g., DA labels (Cervone et al., 2018) .

These DA labels are provided by human annotators or DA prediction models. Such coherence models suffer from the following drawbacks: (a) they curb semantic representations of utterances to entities, which are sparse in dialogue because of short utterance lengths, and (b) their performance relies on the quality of their input DA labels. We propose a novel approach to dialogue coherence assessment by utilizing dialogue act prediction as an auxiliary task for training our coherence model in a multi-task learning (MTL) scenario (Figure 1 ). Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh) , and a Dialogue Act Prediction (DAP) model.

The layers of the utterance encoder are shared between the DAP and the DiCoh model. This idea enables our Di-Coh model to learn to focus on salient information presented in utterances considering their DAs and to alleviate the need for explicit DA labels during coherence assessment. We evaluate our MTL-based approach on the DailyDialog (Li et al., 2017) and SwitchBoard (Jurafsky and Shriberg, 1997) English dialogue corpora in several discriminating experiments, where our coherence model, DiCoh, is examined to discriminate a dialogue from its perturbations (see Table 1 ). We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.

Our core contributions are: (1) proposing an MTL-based approach for dialogue coherence assessment using DAP as an auxiliary task, yielding more informative utterance representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations;

 (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-the-art coherence model on DailyDialog, and performs on par with it on SwitchBoard.","Sounds like an interesting approach, what type of models are examined for this purpose?","A neural model consisting of three modules: sentence encoder, coherence score prediction, and DA prediction. The model is trained in MTL regime. Does it make sense to you? ","Hello there, what are some examples for explicit dialogue act labels? Hi, The given text does not provide any specific example of DA. However, DA seems to related to what a speaker intention is of saying a sentence
 Facts: 
1. A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. 
 How is dialogue coherence defined? dialogue coherence deals with the semantic and DA's relations between sentence in a dialogue. 
 Facts: 
1. Dialogue coherence deals with semantic relations between utterances considering their dialogue acts 
 I see, and how do authors suggest evaluating dialogue coherence without using ecplicit labels?","['Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh) , and a Dialogue Act Prediction (DAP) model.']",train
Dialogue Coherence Assessment Without Explicit Dialogue Act Labels,"Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels. It indicates two drawbacks, (a) semantics of utterances is limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels. We address these issues by introducing a novel approach to dialogue coherence assessment.

We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment. Our approach alleviates the need for explicit dialogue act labels during evaluation. The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the Dai-lyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence. We release our source code 1 . Considering rapid progresses in developing open-domain dialogue agents Ghazvininejad et al., 2018; Dinan et al., 2019; Li et al., 2019) , the need for models that compare these agents in various dialogue aspects becomes extremely important (Liu et al., 2016; Dinan et al., 2019) . Most available methods for dialogue evaluation rely on word-overlap metrics, e.g. BLEU, and manually collected human feedback. The former does not strongly correlate with human judgments (Liu et al., 2016) , and the latter is timeconsuming and subjective.

A fundamental aspect of dialogue is coherence -what discriminates a Figure 1 : A high-level view of our multi-task learning approach for dialogue coherence modeling. high-quality dialogue from a random sequence of dialogue utterances (Halliday and Hasan, 1976; Grosz and Sidner, 1986; Byron and Stent, 1998) . Dialogue coherence deals with semantic relations between utterances considering their dialogue acts (Perrault and Allen, 1978; Cervone et al., 2018) . A Dialogue Act (henceforth DA) gives a meaning to an utterance in a dialogue at the level of ""illocutionary force"", and therefore, constitutes the basic unit of communication (Searle, 1969; Raheja and Tetreault, 2019) .

A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. Recent approaches to dialogue coherence modeling use the coherence features designed for monologue texts, e.g. entity transitions (Barzilay and Lapata, 2005) , and augment them with dialogue-relevant features, e.g., DA labels (Cervone et al., 2018) .

These DA labels are provided by human annotators or DA prediction models. Such coherence models suffer from the following drawbacks: (a) they curb semantic representations of utterances to entities, which are sparse in dialogue because of short utterance lengths, and (b) their performance relies on the quality of their input DA labels. We propose a novel approach to dialogue coherence assessment by utilizing dialogue act prediction as an auxiliary task for training our coherence model in a multi-task learning (MTL) scenario (Figure 1 ). Our approach consists of three high-level components: an utterance encoder, a dialogue coherence model (DiCoh) , and a Dialogue Act Prediction (DAP) model.

The layers of the utterance encoder are shared between the DAP and the DiCoh model. This idea enables our Di-Coh model to learn to focus on salient information presented in utterances considering their DAs and to alleviate the need for explicit DA labels during coherence assessment. We evaluate our MTL-based approach on the DailyDialog (Li et al., 2017) and SwitchBoard (Jurafsky and Shriberg, 1997) English dialogue corpora in several discriminating experiments, where our coherence model, DiCoh, is examined to discriminate a dialogue from its perturbations (see Table 1 ). We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.

Our core contributions are: (1) proposing an MTL-based approach for dialogue coherence assessment using DAP as an auxiliary task, yielding more informative utterance representations for coherence assessment; (2) alleviating the need for DA labels for dialogue coherence assessment during evaluations;

 (3) an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-the-art coherence model on DailyDialog, and performs on par with it on SwitchBoard.","Yes, I think it makes sense. How do the models perform? Do the scores they give correlate with expectations?",the evaluation is performed on two datasets. On one the model outperforms the SoTA. On the other one the model works on par with SoTA. Do you think that alleviating the need for DA labels during evaluation is important? ,"Hello there, what are some examples for explicit dialogue act labels? Hi, The given text does not provide any specific example of DA. However, DA seems to related to what a speaker intention is of saying a sentence
 Facts: 
1. A DA captures what a speaker's intention is of saying an utterance without regard to the actual content of the utterance. For example, a DA may indicate whether the intention of stating an utterance is to ask a question or to state a piece of information. 
 How is dialogue coherence defined? dialogue coherence deals with the semantic and DA's relations between sentence in a dialogue. 
 Facts: 
1. Dialogue coherence deals with semantic relations between utterances considering their dialogue acts 
 I see, and how do authors suggest evaluating dialogue coherence without using ecplicit labels? The remove DA labels from the test set. They perturb the order of sentences in a dialogue. Then check how well a coherence model can distinguish an original dialogue from its perturbation..
 Facts: 
1. We utilize perturbation methods, like utterance ordering and utterance insertion, inherited from coherence evaluation approaches for monologue texts, and also introduce two dialoguerelevant perturbations, named utterance replacement and even utterance ordering.
 Sounds like an interesting approach, what type of models are examined for this purpose?","[' an empirical evaluation on two benchmark dialogue corpora, showing that our model substantially outperforms the state-of-the-art coherence model on DailyDialog, and performs on par with it on SwitchBoard.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Hi!,Hi,,[],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",What is the paper about?,This paper is about a novel pre-training method for QA,Hi!,['This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations'],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Can you give me more details about the method?,"Sure, it is based on the intuition that the representations of a phrase in a passage should be similar to the representations of the questions this passage can answer. ",Hi! Hi What is the paper about?,"['This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. ']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",It seems a pretty nice idea! How have you shown that the method work?,They test their method on zero-shot and few-shot QA as well as on other tasks that they frame as QA. ,"Hi! Hi What is the paper about? This paper is about a novel pre-training method for QA
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations
 Can you give me more details about the method?","['We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",That's good! Which evaluation metrics are employed?,Unfortunately I can't answer that. I wonder if this technique could be applied on other tasks. What do you think about that?,"Hi! Hi What is the paper about? This paper is about a novel pre-training method for QA
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations
 Can you give me more details about the method? Sure, it is based on the intuition that the representations of a phrase in a passage should be similar to the representations of the questions this passage can answer. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 It seems a pretty nice idea! How have you shown that the method work?",[],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",I think that can be done since the method has already demonstrated amazing results on quite different tasks and the aim is to learn general-purpose contextual representation. What should we talk next?,Would you like to know about their data augmentation strategy?,"Hi! Hi What is the paper about? This paper is about a novel pre-training method for QA
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations
 Can you give me more details about the method? Sure, it is based on the intuition that the representations of a phrase in a passage should be similar to the representations of the questions this passage can answer. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 It seems a pretty nice idea! How have you shown that the method work? They test their method on zero-shot and few-shot QA as well as on other tasks that they frame as QA. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
 That's good! Which evaluation metrics are employed?",[],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Can you give more details about it?,They use a question generation model and train it on a QA dataset to increase the training data size. Could you terminate the session? I need to go to the next one :D,"Hi! Hi What is the paper about? This paper is about a novel pre-training method for QA
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations
 Can you give me more details about the method? Sure, it is based on the intuition that the representations of a phrase in a passage should be similar to the representations of the questions this passage can answer. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 It seems a pretty nice idea! How have you shown that the method work? They test their method on zero-shot and few-shot QA as well as on other tasks that they frame as QA. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
 That's good! Which evaluation metrics are employed? Unfortunately I can't answer that. I wonder if this technique could be applied on other tasks. What do you think about that? I think that can be done since the method has already demonstrated amazing results on quite different tasks and the aim is to learn general-purpose contextual representation. What should we talk next?","['We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","Hi there, How do author suggest infusing question answering capabilities to language models during pretraining?","Yes, it is a novel pre-training method based on QA. ",,"['This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, ']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","I see, do authors use a multi task learning regieme? What kind of objective do they suggest?",Their objective is based on the notion that a question and all the passages that answer the question should have a similar representation. ,"Hi there, How do author suggest infusing question answering capabilities to language models during pretraining?",['Our intuition is that the representation for a phrase should contain enough information to identify all the questions that the phrase could answer in context.'],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","I see, do you think it makes sense for questions and passages that answer those questions to have similar representations? Or something more sophisticated is necessary?","It seems to make sense, as they report strong performance on QA and other tasks framed as QA. ","Hi there, How do author suggest infusing question answering capabilities to language models during pretraining? Yes, it is a novel pre-training method based on QA. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, 
 I see, do authors use a multi task learning regieme? What kind of objective do they suggest?","['We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Sounds impressive results. Do authors also experiment with fine-tuning the models after pretrainings?,I think they mostly do few-shot experiments. ,"Hi there, How do author suggest infusing question answering capabilities to language models during pretraining? Yes, it is a novel pre-training method based on QA. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, 
 I see, do authors use a multi task learning regieme? What kind of objective do they suggest? Their objective is based on the notion that a question and all the passages that answer the question should have a similar representation. 
 Facts: 
1. Our intuition is that the representation for a phrase should contain enough information to identify all the questions that the phrase could answer in context.
 I see, do you think it makes sense for questions and passages that answer those questions to have similar representations? Or something more sophisticated is necessary?","['In this paper, we introduce a new pre-training loss based on question answering that depends much more directly on context, and learns improved token-level representations for a range of zero-and few-shot tasks.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","I see, If I understand correctly, authors use some labelled data and perform some supervised training during the pretraining. Is this correct?","Yes, in fact they use a data augmentation approach, where they first generate questions from context passages, then train a cross-encoder to answer these questions and finally use the cross-encoder outputs to pre-train their model.","Hi there, How do author suggest infusing question answering capabilities to language models during pretraining? Yes, it is a novel pre-training method based on QA. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, 
 I see, do authors use a multi task learning regieme? What kind of objective do they suggest? Their objective is based on the notion that a question and all the passages that answer the question should have a similar representation. 
 Facts: 
1. Our intuition is that the representation for a phrase should contain enough information to identify all the questions that the phrase could answer in context.
 I see, do you think it makes sense for questions and passages that answer those questions to have similar representations? Or something more sophisticated is necessary? It seems to make sense, as they report strong performance on QA and other tasks framed as QA. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
 Sounds impressive results. Do authors also experiment with fine-tuning the models after pretrainings?","['We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",and do they also compare or combine their pretraining approach with finetuning approach? ,"I think yes, but I don't have more information on this unfortunately. Could you terminate the session, I think the next person is waiting :D Thanks!","Hi there, How do author suggest infusing question answering capabilities to language models during pretraining? Yes, it is a novel pre-training method based on QA. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, 
 I see, do authors use a multi task learning regieme? What kind of objective do they suggest? Their objective is based on the notion that a question and all the passages that answer the question should have a similar representation. 
 Facts: 
1. Our intuition is that the representation for a phrase should contain enough information to identify all the questions that the phrase could answer in context.
 I see, do you think it makes sense for questions and passages that answer those questions to have similar representations? Or something more sophisticated is necessary? It seems to make sense, as they report strong performance on QA and other tasks framed as QA. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
 Sounds impressive results. Do authors also experiment with fine-tuning the models after pretrainings? I think they mostly do few-shot experiments. 
 Facts: 
1. In this paper, we introduce a new pre-training loss based on question answering that depends much more directly on context, and learns improved token-level representations for a range of zero-and few-shot tasks.
 I see, If I understand correctly, authors use some labelled data and perform some supervised training during the pretraining. Is this correct?",[],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","thansk, that is all I wanted to know, Bye :D",Bye,"Hi there, How do author suggest infusing question answering capabilities to language models during pretraining? Yes, it is a novel pre-training method based on QA. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, 
 I see, do authors use a multi task learning regieme? What kind of objective do they suggest? Their objective is based on the notion that a question and all the passages that answer the question should have a similar representation. 
 Facts: 
1. Our intuition is that the representation for a phrase should contain enough information to identify all the questions that the phrase could answer in context.
 I see, do you think it makes sense for questions and passages that answer those questions to have similar representations? Or something more sophisticated is necessary? It seems to make sense, as they report strong performance on QA and other tasks framed as QA. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
 Sounds impressive results. Do authors also experiment with fine-tuning the models after pretrainings? I think they mostly do few-shot experiments. 
 Facts: 
1. In this paper, we introduce a new pre-training loss based on question answering that depends much more directly on context, and learns improved token-level representations for a range of zero-and few-shot tasks.
 I see, If I understand correctly, authors use some labelled data and perform some supervised training during the pretraining. Is this correct? Yes, in fact they use a data augmentation approach, where they first generate questions from context passages, then train a cross-encoder to answer these questions and finally use the cross-encoder outputs to pre-train their model.
 Facts: 
1. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.
 and do they also compare or combine their pretraining approach with finetuning approach? ",[],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","Hi, what is the subject of this article?","Hi, this article is about a novel pre-training method based on question answering. ",,"['This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations,']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",This sounds quite interesting. What is this new pre-training objective?,The new pre-training objective is based on the intuition that the representation of a phrase in a passage should be similar to the representations of the questions it answers.,"Hi, what is the subject of this article?","['This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. ']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","Ah, okay. Do they use already existing datasets or did they provide one on their own?",They use an existing dataset to train a question generation model. ,"Hi, what is the subject of this article? Hi, this article is about a novel pre-training method based on question answering. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations,
 This sounds quite interesting. What is this new pre-training objective?","['We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.","Okay. This sounds computational expensive, doesn't it?","Yes, but I think the question generation needs to be done only once to generate the pre-training data, then the model can be used in zero-shot and few-shot settings. Also it is trained as a bi-encoder, which is more efficient than a cross-encoder. ","Hi, what is the subject of this article? Hi, this article is about a novel pre-training method based on question answering. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations,
 This sounds quite interesting. What is this new pre-training objective? The new pre-training objective is based on the intuition that the representation of a phrase in a passage should be similar to the representations of the questions it answers.
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 Ah, okay. Do they use already existing datasets or did they provide one on their own?","['We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.', 'We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Okay. Do they only use RoBERTa as pre-trained language model? ,You mean as a comparison?,"Hi, what is the subject of this article? Hi, this article is about a novel pre-training method based on question answering. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations,
 This sounds quite interesting. What is this new pre-training objective? The new pre-training objective is based on the intuition that the representation of a phrase in a passage should be similar to the representations of the questions it answers.
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 Ah, okay. Do they use already existing datasets or did they provide one on their own? They use an existing dataset to train a question generation model. 
 Facts: 
1. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.
 Okay. This sounds computational expensive, doesn't it?",[],train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Yes.,In the information I have only Roberta is named. ,"Hi, what is the subject of this article? Hi, this article is about a novel pre-training method based on question answering. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations,
 This sounds quite interesting. What is this new pre-training objective? The new pre-training objective is based on the intuition that the representation of a phrase in a passage should be similar to the representations of the questions it answers.
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 Ah, okay. Do they use already existing datasets or did they provide one on their own? They use an existing dataset to train a question generation model. 
 Facts: 
1. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.
 Okay. This sounds computational expensive, doesn't it? Yes, but I think the question generation needs to be done only once to generate the pre-training data, then the model can be used in zero-shot and few-shot settings. Also it is trained as a bi-encoder, which is more efficient than a cross-encoder. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
2. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.
 Okay. Do they only use RoBERTa as pre-trained language model? ","['We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.']",train
Question Answering Infused Pre-training of General-Purpose Contextualized Representations,"This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs. By encoding QA-relevant information, the bi-encoder's token-level representations are useful for non-QA downstream tasks without extensive (or in some cases, any) fine-tuning. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets. Although masked language models build contex-
tualized word representations, they are pre-trained
with losses that minimize distance to uncontextual-
ized word embeddings (Peters et al., 2018; Devlin
et al., 2019; Liu et al., 2019). In this paper, we
introduce a new pre-training loss based on ques-
tion answering that depends much more directly on
context, and learns improved token-level represen-
tations for a range of zero- and few-shot tasks.
More specifically, we propose Question Answer-
ing Infused Pre-training (QUIP), a technique for
training highly context dependent representations.
Our intuition is that the representation for a phrase
should contain enough information to identify all
the questions that the phrase could answer in con-
text. For example, in Figure 1, the representation
for Johannes Brahms should be similar to an en-
coding of all questions it can answer, such as “Who
wrote the violin concerto?”—thereby capturing the phrase’s role in the sentence. A further advantage
is that these more contextualized representations
allow for improved zero- and few-shot learning for
many token-level tasks, especially when the tasks
can be posed as question answering (Levy et al.,
2017; McCann et al., 2018). Our approach builds
on previous work using question-answer pairs as a
meaning representation (He et al., 2015; Michael
et al., 2018), and more generally from the tradition
of using question answering ability as a proxy for
language understanding (Lehnert, 1977; Hirschman
et al., 1999; Peñas et al., 2013; Richardson et al.,
2013).
QUIP learns contextual representations with
a bi-encoder extractive QA objective. Our bi-
encoder model must independently encode pas-
sages and questions such that the representation
of each phrase in a passage is similar to the repre-
sentations of all reading comprehension questions that can be answered with that phrase. To train
such a model, we use a question generation model
to synthesize 80 million QA examples, then train
the bi-encoder to match the predictions of a stan-
dard cross-encoder QA model, which processes the
passage and question together, on these examples.
Bi-encoder QA has been used for efficient open-
domain QA via phrase retrieval (Seo et al., 2018,
2019; Lee et al., 2020, 2021), but its lower accu-
racy compared to cross-encoder QA has previously
been viewed as a drawback. We instead identify
the relative weakness of bi-encoder QA as an op-
portunity to improve contextual representations via
knowledge distillation, as self-training can be ef-
fective when the student model is forced to solve a
harder problem than the teacher (Xie et al., 2020).
In fact, using a bi-encoder student model is critical
for QUIP: a cross-encoder trained in a similar way
does not learn contextual representations that are
as useful for later few-shot learning, despite having
higher QA accuracy.
We show that QUIP token-level representations
are useful in a variety of zero-shot and few-shot
learning settings, both because the representations
directly encode useful contextual information, and
because we can often reduce downstream tasks to
QA. For few-shot paraphrase detection, QUIP with
BERTScore-based features (Zhang et al., 2020)
outperforms prior work by 9 F1 points across
four datasets. For few-shot named entity recog-
nition (NER), QUIP combined with an initializa-
tion scheme that uses question embeddings im-
proves over RoBERTa-large by 14 F1 across two
datasets. Finally, for zero-shot sentiment analy-
sis, QUIP with question prompts improves over
RoBERTa-large with MLM-style prompts by 5 ac-
curacy points across three datasets, and extracts
interpretable rationales as a side effect. Through ab-
lations, we show that using real questions, a strong
teacher model, and the bi-encoder architecture are
all crucial to the success of QUIP; on the other
hand, many other design decisions (e.g., question
generation decoding strategies) do not qualitatively
affect our main findings, pointing to the stability of
the QUIP approach.",Okay. Thank you for your time!,"Sure, thank you for your interest!","Hi, what is the subject of this article? Hi, this article is about a novel pre-training method based on question answering. 
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations,
 This sounds quite interesting. What is this new pre-training objective? The new pre-training objective is based on the intuition that the representation of a phrase in a passage should be similar to the representations of the questions it answers.
 Facts: 
1. This paper proposes a pre-training objective based on question answering (QA) for learning general-purpose contextual representations, motivated by the intuition that the representation of a phrase in a passage should encode all questions that the phrase can answer in context. 
 Ah, okay. Do they use already existing datasets or did they provide one on their own? They use an existing dataset to train a question generation model. 
 Facts: 
1. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.
 Okay. This sounds computational expensive, doesn't it? Yes, but I think the question generation needs to be done only once to generate the pre-training data, then the model can be used in zero-shot and few-shot settings. Also it is trained as a bi-encoder, which is more efficient than a cross-encoder. 
 Facts: 
1. We show large improvements over both RoBERTa-large and previous state-of-theart results on zero-shot and few-shot paraphrase detection on four datasets, few-shot named entity recognition on two datasets, and zero-shot sentiment analysis on three datasets.
2. We accomplish this goal by training a bi-encoder QA model, which independently encodes passages and questions, to match the predictions of a more accurate cross-encoder model on 80 million synthesized QA pairs.
 Okay. Do they only use RoBERTa as pre-trained language model?  You mean as a comparison? Yes.",[],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.","Hi, what is this paper about?",Hi! This papers presents an ensemble of three deep learning models for extracting interactions between drugs and medical entities. The interactions can be within sentence or across sentences. ,,"['This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.', 'We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",This sounds quite interesting. You wrote that the interactions can be across sentences. How much sentences can such an interaction span?,"Unfortunately, the presented text doesn't include such information. You can find it in the full paper. Do you want to know which models are used for intra- and inter-sentence relation extraction?","Hi, what is this paper about?",[],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.","Yes, sure! Which models are used? I can imagine that they build upon pre-trained language models, did they?","Not at all, these models only use static pre-trained word representations. The intra-sentence models are BiLSTM-based with attention mechanisms. The inter-sentence model is a Transformer-based network but not initialized by a pre-trained model.","Hi, what is this paper about? Hi! This papers presents an ensemble of three deep learning models for extracting interactions between drugs and medical entities. The interactions can be within sentence or across sentences. 
 Facts: 
1. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
2. We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.
 This sounds quite interesting. You wrote that the interactions can be across sentences. How much sentences can such an interaction span?","['The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.', 'For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",Okay. Did they use existing datasets or do they provide one on their own?,The data is from the n2c2 shared-task.,"Hi, what is this paper about? Hi! This papers presents an ensemble of three deep learning models for extracting interactions between drugs and medical entities. The interactions can be within sentence or across sentences. 
 Facts: 
1. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
2. We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.
 This sounds quite interesting. You wrote that the interactions can be across sentences. How much sentences can such an interaction span? Unfortunately, the presented text doesn't include such information. You can find it in the full paper. Do you want to know which models are used for intra- and inter-sentence relation extraction? Yes, sure! Which models are used? I can imagine that they build upon pre-trained language models, did they?",['This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.'],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.","In the title, it is said, that they are using an ensemble of models but from our discussion I understood that they just trained different approaches and compared them with each other. Is that correct?",An ensemble approach is used to combine these models. More details are described in another section.,"Hi, what is this paper about? Hi! This papers presents an ensemble of three deep learning models for extracting interactions between drugs and medical entities. The interactions can be within sentence or across sentences. 
 Facts: 
1. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
2. We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.
 This sounds quite interesting. You wrote that the interactions can be across sentences. How much sentences can such an interaction span? Unfortunately, the presented text doesn't include such information. You can find it in the full paper. Do you want to know which models are used for intra- and inter-sentence relation extraction? Yes, sure! Which models are used? I can imagine that they build upon pre-trained language models, did they? Not at all, these models only use static pre-trained word representations. The intra-sentence models are BiLSTM-based with attention mechanisms. The inter-sentence model is a Transformer-based network but not initialized by a pre-trained model.
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 Okay. Did they use existing datasets or do they provide one on their own?",['Our ensemble effectively takes advantages from our proposed models.'],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.","Ah, okay. Thanks for your time!",Thanks!,"Hi, what is this paper about? Hi! This papers presents an ensemble of three deep learning models for extracting interactions between drugs and medical entities. The interactions can be within sentence or across sentences. 
 Facts: 
1. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
2. We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.
 This sounds quite interesting. You wrote that the interactions can be across sentences. How much sentences can such an interaction span? Unfortunately, the presented text doesn't include such information. You can find it in the full paper. Do you want to know which models are used for intra- and inter-sentence relation extraction? Yes, sure! Which models are used? I can imagine that they build upon pre-trained language models, did they? Not at all, these models only use static pre-trained word representations. The intra-sentence models are BiLSTM-based with attention mechanisms. The inter-sentence model is a Transformer-based network but not initialized by a pre-trained model.
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 Okay. Did they use existing datasets or do they provide one on their own? The data is from the n2c2 shared-task.
 Facts: 
1. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
 In the title, it is said, that they are using an ensemble of models but from our discussion I understood that they just trained different approaches and compared them with each other. Is that correct?",[],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",Hi! Can you explain the main idea of the paper?,This paper presents an ensemble of different models for the extraction of relations between drugs and medical entities.,,"['We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities.', 'This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",Interesting! I was wondering what kinds of model are you doing with the ensemble method?,We use different models for intra- and inter-sentence relation extraction. The intra-sentence models are based on BiLSTM with attention mechanisms. The inter-sentence model is a Transformer-based one. ,Hi! Can you explain the main idea of the paper?,"['The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.', 'For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",I see. How do you ensemble model/model predictions?,"Unfortunately, the presented text doesn't have information regarding the ensemble method. ","Hi! Can you explain the main idea of the paper? This paper presents an ensemble of different models for the extraction of relations between drugs and medical entities.
 Facts: 
1. We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities.
2. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
 Interesting! I was wondering what kinds of model are you doing with the ensemble method?",[],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",No problem. Would you like to explain how you do the analysis/ablation study if the information is available?,"We have analysis showing that our approach is more generalized than systems with more training data. We also show that the use of implicit drug-drug interactions, which are not annotated, improve the performance of extracting relations between drugs and medical entities.","Hi! Can you explain the main idea of the paper? This paper presents an ensemble of different models for the extraction of relations between drugs and medical entities.
 Facts: 
1. We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities.
2. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
 Interesting! I was wondering what kinds of model are you doing with the ensemble method? We use different models for intra- and inter-sentence relation extraction. The intra-sentence models are based on BiLSTM with attention mechanisms. The inter-sentence model is a Transformer-based one. 
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 I see. How do you ensemble model/model predictions?","['Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques.', 'Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs.']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",Good to hear about systems with more training data. Did you check how the pretraining affect this generalization?,"Sorry, the current text cannot address this question. ","Hi! Can you explain the main idea of the paper? This paper presents an ensemble of different models for the extraction of relations between drugs and medical entities.
 Facts: 
1. We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities.
2. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records.
 Interesting! I was wondering what kinds of model are you doing with the ensemble method? We use different models for intra- and inter-sentence relation extraction. The intra-sentence models are based on BiLSTM with attention mechanisms. The inter-sentence model is a Transformer-based one. 
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 I see. How do you ensemble model/model predictions? Unfortunately, the presented text doesn't have information regarding the ensemble method.  No problem. Would you like to explain how you do the analysis/ablation study if the information is available?",[],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.","Hi, interesting paper title. What kind of deep learning methods did you use?",We use BiLSTM with attention mechanisms in intra-sentence models and Transformer-based network for inter-sentence relation extraction. ,,"['The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.', 'For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",Relation extraction usually requires that you find named entities. Do you assume the named entities are given?,We have two scenarios. One uses named entity recognition models to obtain entities.,"Hi, interesting paper title. What kind of deep learning methods did you use?",['We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.'],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",And why do you use ensemble methods? Is a simple deep network not enough?,"We use separate models to deal with intra- and inter-sentence relations, which performs better than using a single model. ","Hi, interesting paper title. What kind of deep learning methods did you use? We use BiLSTM with attention mechanisms in intra-sentence models and Transformer-based network for inter-sentence relation extraction. 
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 Relation extraction usually requires that you find named entities. Do you assume the named entities are given?",['We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.'],train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",How well are these methods doing?,Our approach gets 94.72% in terms of micro-averaged F1 score for relation extraction and 87.65% for end-to-end RE.,"Hi, interesting paper title. What kind of deep learning methods did you use? We use BiLSTM with attention mechanisms in intra-sentence models and Transformer-based network for inter-sentence relation extraction. 
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 Relation extraction usually requires that you find named entities. Do you assume the named entities are given? We have two scenarios. One uses named entity recognition models to obtain entities.
 Facts: 
1. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.
 And why do you use ensemble methods? Is a simple deep network not enough?","['Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).']",train
Adverse drug events and medication relation extraction in electronic health records with ensemble deep learning methods,"Objective: Identification of drugs, associated medication entities, and interactions among them are crucial to prevent unwanted effects of drug therapy, known as adverse drug events. This article describes our participation to the n2c2 shared-task in extracting relations between medication-related entities in electronic health records. Materials and Methods: We proposed an ensemble approach for relation extraction and classification between drugs and medication-related entities. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.

We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences. Results: Our team ranked third with a micro-averaged F1 score of 94.72% and 87.65% for relation and end-toend relation extraction, respectively (Tracks 2 and 3).

Our ensemble effectively takes advantages from our proposed models. Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques. Conclusions: We proposed a relation extraction system to identify relations between drugs and medicationrelated entities. The proposed approach is independent of external syntactic tools.

Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs. The interactions between drugs and medication-related entities are crucial to avoid harmful consequences of pharmaceuticals. In particular, adverse drug events (ADEs) reflect how much certain drugs can affect patients by causing undesirable side effects. 1 Clinical narratives and electronic health records (EHRs) constitute a rich source for ADE evidence. Hence, careful examination of clinical narratives can provide helpful information for pharmacovigilance.

However, the large amount of EHRs, as well as their informal and unstructured nature, makes the mining of interesting interactions related to ADEs a challenging task for clinicians. To tackle this issue, natural language processing (NLP) techniques have been widely applied on EHRs to automatically extract ADE-related information using relation extraction (RE) methods.",This is a high F1 score. Are there any other advantages you observed during the experiments?,"Yes, there are some interesting findings. We show that our approach is more generalized than systems with more training data or additional processing techniques. We also present the usefulness of implicit drug-drug interactions (which are not annotated) for extracting relations between drugs and medical entities.","Hi, interesting paper title. What kind of deep learning methods did you use? We use BiLSTM with attention mechanisms in intra-sentence models and Transformer-based network for inter-sentence relation extraction. 
 Facts: 
1. The intra-sentence models rely on bidirectional long short-term memory networks and attention mechanisms and are able to capture dependencies between multiple related pairs in the same sentence.
2. For the inter-sentence relations, we adopted a neural architecture that utilizes the Transformer network to improve performance in longer sequences.
 Relation extraction usually requires that you find named entities. Do you assume the named entities are given? We have two scenarios. One uses named entity recognition models to obtain entities.
 Facts: 
1. We incorporated state-of-the-art named-entity recognition (NER) models based on bidirectional long short-term memory (BiLSTM) networks and conditional random fields (CRF) for end-to-end extraction.
 And why do you use ensemble methods? Is a simple deep network not enough? We use separate models to deal with intra- and inter-sentence relations, which performs better than using a single model. 
 Facts: 
1. We additionally developed separate models for intra-and inter-sentence relation extraction and combined them using an ensemble method.
 How well are these methods doing?","['Analysis of the reported results indicated that our proposed approach is more generalizable than the top-performing system, which employs additional training data-and corpus-driven processing techniques.', 'Analysis showed that by using latent Drug-Drug interactions we were able to significantly improve the performance of non-Drug-Drug pairs in EHRs.']",train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Hi!,Hi!,,[],train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",What is the paper about?,The paper is about position models in the Transformer architecture.,Hi!,"['In this paper, we provide an overview of common methods to incorporate position information into Transformer models.']",train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Can you give me more information about what a position model is?,The Transformer itself cannot see the order of its input tokens. A position model is what helps the Transformer to see the order. This can be embeddings or other manipulations of the self-attention.,Hi! Hi! What is the paper about?,"['Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.', 'A Transformer model itself is invariant with respect to re-orderings of the input.']",train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","So embeddings and self-attention manipulations are the main position modes, is it correct?","Yes, this is correct. There are a lot of variants but these are the two main categories.","Hi! Hi! What is the paper about? The paper is about position models in the Transformer architecture.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
 Can you give me more information about what a position model is?",['Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.'],train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Does the authors provide a common benchmark to compare the methods?,"No, there is no empirical evaluation mentioned. But they compare the methods on a theoretical level. They also characterize which position model should go with what type of application. Do you think this is useful without a new benchmark?","Hi! Hi! What is the paper about? The paper is about position models in the Transformer architecture.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
 Can you give me more information about what a position model is? The Transformer itself cannot see the order of its input tokens. A position model is what helps the Transformer to see the order. This can be embeddings or other manipulations of the self-attention.
 Facts: 
1. Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.
2. A Transformer model itself is invariant with respect to re-orderings of the input.
 So embeddings and self-attention manipulations are the main position modes, is it correct?","['enabling the reader to compare existing methods by providing a unified notation and meaningful clustering', 'indicating what characteristics of an application should be taken into account when selecting a position encoding']",train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",I think the theoretical comparison is much more important than the experimental one. Which one of two categories of position models is better?,"I agree that the theoretical comparison can give a lot of insights. I do not think that it can really answer the question which position model is ""best"". What criterion for ""being best"" do you have in mind?","Hi! Hi! What is the paper about? The paper is about position models in the Transformer architecture.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
 Can you give me more information about what a position model is? The Transformer itself cannot see the order of its input tokens. A position model is what helps the Transformer to see the order. This can be embeddings or other manipulations of the self-attention.
 Facts: 
1. Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.
2. A Transformer model itself is invariant with respect to re-orderings of the input.
 So embeddings and self-attention manipulations are the main position modes, is it correct? Yes, this is correct. There are a lot of variants but these are the two main categories.
 Facts: 
1. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.
 Does the authors provide a common benchmark to compare the methods?",[],train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",I was wondering if one of the two is more spread than the other.,I do not have information about the frequency that the different types of position models occur. I believe that showing the large variety of models and analyzing their differences and common points is the main contribution of the paper.,"Hi! Hi! What is the paper about? The paper is about position models in the Transformer architecture.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
 Can you give me more information about what a position model is? The Transformer itself cannot see the order of its input tokens. A position model is what helps the Transformer to see the order. This can be embeddings or other manipulations of the self-attention.
 Facts: 
1. Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.
2. A Transformer model itself is invariant with respect to re-orderings of the input.
 So embeddings and self-attention manipulations are the main position modes, is it correct? Yes, this is correct. There are a lot of variants but these are the two main categories.
 Facts: 
1. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.
 Does the authors provide a common benchmark to compare the methods? No, there is no empirical evaluation mentioned. But they compare the methods on a theoretical level. They also characterize which position model should go with what type of application. Do you think this is useful without a new benchmark?
 Facts: 
1. enabling the reader to compare existing methods by providing a unified notation and meaningful clustering
2. indicating what characteristics of an application should be taken into account when selecting a position encoding
 I think the theoretical comparison is much more important than the experimental one. Which one of two categories of position models is better?",['The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area;'],train
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","Hi, what is this work about?","Hi, it is about the Transformer model and specifically about ways to encode position information.",,"['In this paper, we provide an overview of common methods to incorporate position information into Transformer models.', 'Transformers are arguably the main workhorse in recent Natural Language Processing research.']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",What do you mean with position information? Like indices of tokens?,"Yes, I mean word order. A sentence only makes sense in the right order. A bag of words is ambiguous.","Hi, what is this work about?","['text data is inherently sequential', 'Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }).']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",So what approaches of encoding position information does this work investigate?,There is a large variety of position models that is investigated. One contribution of the paper is to show this variety. Shall I give you some examples?,"Hi, what is this work about? Hi, it is about the Transformer model and specifically about ways to encode position information.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 What do you mean with position information? Like indices of tokens?","['The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area;', 'Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","Yes, some examples would be great!","Position embeddings representing absolute token index or the relative distance between tokens is an example. Another is injecting this information into the self-attention matrix. One work also uses an RNN to ""positionalize"" the input.","Hi, what is this work about? Hi, it is about the Transformer model and specifically about ways to encode position information.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 What do you mean with position information? Like indices of tokens? Yes, I mean word order. A sentence only makes sense in the right order. A bag of words is ambiguous.
 Facts: 
1. text data is inherently sequential
2. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }).
 So what approaches of encoding position information does this work investigate?","['Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","So between those two, what works better and for which tasks?","Unfortunately, I do not have access to that information. But there is a discussion of matching position models to tasks later in the paper.","Hi, what is this work about? Hi, it is about the Transformer model and specifically about ways to encode position information.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 What do you mean with position information? Like indices of tokens? Yes, I mean word order. A sentence only makes sense in the right order. A bag of words is ambiguous.
 Facts: 
1. text data is inherently sequential
2. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }).
 So what approaches of encoding position information does this work investigate? There is a large variety of position models that is investigated. One contribution of the paper is to show this variety. Shall I give you some examples?
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area;
2. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.
 Yes, some examples would be great!",['indicating what characteristics of an application should be taken into account when selecting a position encoding'],val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Then let me ask a different question; is there a possibility of including positional information can hurt the model's performance? Has this been observed?,"This is an interesting question. For natural language, this does not seem to be the case. What do you think about that?","Hi, what is this work about? Hi, it is about the Transformer model and specifically about ways to encode position information.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 What do you mean with position information? Like indices of tokens? Yes, I mean word order. A sentence only makes sense in the right order. A bag of words is ambiguous.
 Facts: 
1. text data is inherently sequential
2. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }).
 So what approaches of encoding position information does this work investigate? There is a large variety of position models that is investigated. One contribution of the paper is to show this variety. Shall I give you some examples?
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area;
2. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.
 Yes, some examples would be great! Position embeddings representing absolute token index or the relative distance between tokens is an example. Another is injecting this information into the self-attention matrix. One work also uses an RNN to ""positionalize"" the input.
 Facts: 
1. Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.
 So between those two, what works better and for which tasks?",['it should be beneficial to incorporate this essential inductive bias into any model that processes text data.'],val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","Good question, I'd expect that in situations where positional information does not matter, models may get biased with additional noise.",I agree. A task like this should have a lightweight position model if any.,"Hi, what is this work about? Hi, it is about the Transformer model and specifically about ways to encode position information.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 What do you mean with position information? Like indices of tokens? Yes, I mean word order. A sentence only makes sense in the right order. A bag of words is ambiguous.
 Facts: 
1. text data is inherently sequential
2. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }).
 So what approaches of encoding position information does this work investigate? There is a large variety of position models that is investigated. One contribution of the paper is to show this variety. Shall I give you some examples?
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area;
2. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.
 Yes, some examples would be great! Position embeddings representing absolute token index or the relative distance between tokens is an example. Another is injecting this information into the self-attention matrix. One work also uses an RNN to ""positionalize"" the input.
 Facts: 
1. Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.
 So between those two, what works better and for which tasks? Unfortunately, I do not have access to that information. But there is a discussion of matching position models to tasks later in the paper.
 Facts: 
1. indicating what characteristics of an application should be taken into account when selecting a position encoding
 Then let me ask a different question; is there a possibility of including positional information can hurt the model's performance? Has this been observed?",[],val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Maybe an attention based approach that is explicitly trained in both kind of tasks could handle them separately. What do you think?,"In my opinion, the invariance to order can also be a strength for certain tasks. But I think it would be harder for an attention-based approach to ignore the position model if it is once trained without and once trained with it. Was this your suggestion?","Hi, what is this work about? Hi, it is about the Transformer model and specifically about ways to encode position information.
 Facts: 
1. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 What do you mean with position information? Like indices of tokens? Yes, I mean word order. A sentence only makes sense in the right order. A bag of words is ambiguous.
 Facts: 
1. text data is inherently sequential
2. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }).
 So what approaches of encoding position information does this work investigate? There is a large variety of position models that is investigated. One contribution of the paper is to show this variety. Shall I give you some examples?
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area;
2. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.
 Yes, some examples would be great! Position embeddings representing absolute token index or the relative distance between tokens is an example. Another is injecting this information into the self-attention matrix. One work also uses an RNN to ""positionalize"" the input.
 Facts: 
1. Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network.
 So between those two, what works better and for which tasks? Unfortunately, I do not have access to that information. But there is a discussion of matching position models to tasks later in the paper.
 Facts: 
1. indicating what characteristics of an application should be taken into account when selecting a position encoding
 Then let me ask a different question; is there a possibility of including positional information can hurt the model's performance? Has this been observed? This is an interesting question. For natural language, this does not seem to be the case. What do you think about that?
 Facts: 
1. it should be beneficial to incorporate this essential inductive bias into any model that processes text data.
 Good question, I'd expect that in situations where positional information does not matter, models may get biased with additional noise.",['a Transformer is invariant with respect to reorderings of the input'],val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","Hi, the title sounds interesting, can you tell me more about the paper?","Hi, I agree that the title sounds interesting. The main topic of the paper is the Transformer model. Do you need information on this model?",,"['The Transformer model as introduced by Vaswani et al. (2017)', 'Transformers are arguably the main workhorse in recent Natural Language Processing research.']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","No thanks, I am familiar with transformers. Is it a pure literature review of different ways to encode position in transformers, or do the authors also conduct experiments?",It is a survey paper. The main contribution is a theoretical overview of existing position models and no empirical analysis.,"Hi, the title sounds interesting, can you tell me more about the paper?",['The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering;'],val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.","Ah I see, so they state if there are better and worse ones?",I do not think that the main goal is to define better or worse. It is more about identifying similarities and differences (supported by the unified notation). They do have a discussion on matching a given task to a certain kind of application though.,"Hi, the title sounds interesting, can you tell me more about the paper? Hi, I agree that the title sounds interesting. The main topic of the paper is the Transformer model. Do you need information on this model?
 Facts: 
1. The Transformer model as introduced by Vaswani et al. (2017)
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 No thanks, I am familiar with transformers. Is it a pure literature review of different ways to encode position in transformers, or do the authors also conduct experiments?","['broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them', 'indicating what characteristics of an application should be taken into account when selecting a position encoding']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Do they state if there are particular things to look out for when working with long documents?,I do not have access to any concrete findings. But the position models of long document Transformers are certainly part of the survey. The authors state that they aim to complete the paper with newly published papers.,"Hi, the title sounds interesting, can you tell me more about the paper? Hi, I agree that the title sounds interesting. The main topic of the paper is the Transformer model. Do you need information on this model?
 Facts: 
1. The Transformer model as introduced by Vaswani et al. (2017)
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 No thanks, I am familiar with transformers. Is it a pure literature review of different ways to encode position in transformers, or do the authors also conduct experiments? It is a survey paper. The main contribution is a theoretical overview of existing position models and no empirical analysis.
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering;
 Ah I see, so they state if there are better and worse ones?",['We plan to actively continue adding papers that we missed or that are newly published.'],val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Ah that is interesting. Is there a specific purpose for the paper?,"They main purpose is to highlight the similarities and differences in existing position models. In that way, new possibilities for position encodings might be more evident. I think this work can be useful for the creators of new models.","Hi, the title sounds interesting, can you tell me more about the paper? Hi, I agree that the title sounds interesting. The main topic of the paper is the Transformer model. Do you need information on this model?
 Facts: 
1. The Transformer model as introduced by Vaswani et al. (2017)
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 No thanks, I am familiar with transformers. Is it a pure literature review of different ways to encode position in transformers, or do the authors also conduct experiments? It is a survey paper. The main contribution is a theoretical overview of existing position models and no empirical analysis.
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering;
 Ah I see, so they state if there are better and worse ones? I do not think that the main goal is to define better or worse. It is more about identifying similarities and differences (supported by the unified notation). They do have a discussion on matching a given task to a certain kind of application though.
 Facts: 
1. broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them
2. indicating what characteristics of an application should be taken into account when selecting a position encoding
 Do they state if there are particular things to look out for when working with long documents?","['enabling the reader to compare existing methods by providing a unified notation and meaningful clustering', 'providing stimuli for future research']",val
Work in progress POSITION INFORMATION IN TRANSFORMERS: AN OVERVIEW,"Transformers are arguably the main workhorse in recent Natural Language Processing research. By definition a Transformer is invariant with respect to reorderings of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this paper, we provide an overview of common methods to incorporate position information into Transformer models.

The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering; iii) indicate what characteristics of an application should be taken into account when selecting a position encoding; iv) provide stimuli for future research. The Transformer model as introduced by Vaswani et al. (2017) has been found to perform well for many tasks, such as machine translation or language modeling. With the rise of pretrained language models (PLMs) (Peters et al., 2018; Howard & Ruder, 2018; Devlin et al., 2019; Brown et al., 2020) Transformer models have become even more popular. As a result they are at the core of many state of the art natural language processing (NLP) models.

A Transformer model consists of several layers, or blocks. Each layer is a self-attention (Vaswani et al., 2017) module followed by a feed-forward layer. Layer normalization and residual connections are additional components of a layer. A Transformer model itself is invariant with respect to re-orderings of the input.

However, text data is inherently sequential. Without position information the meaning of a sentence is not well-defined (e.g., the sequence ""the cat chases the dog"" vs. the multi-set { the, the, dog, chases, cat }). Clearly it should be beneficial to incorporate this essential inductive bias into any model that processes text data. Therefore, there is a range of different methods to incorporate position information into NLP models, especially PLMs that are based on Transformer models.

Adding position information can be done by using position embeddings, manipulating attention matrices, or preprocessing the input with a recurrent neural network. Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model. Similarly, many papers analyze and compare a subset of position embedding variants. But, to the best of our knowledge, there is no broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them.

The objective of this paper is to provide an overview of methods that incorporate and analyze position information in Transformer models. More specifically we aim at i) showcasing that position information in Transformer is a vibrant and extensive research area ii) enabling the reader to compare existing methods by providing a unified notation and meaningful clustering iii) indicating what characteristics of an application should be taken into account when selecting a position encoding iv) providing stimuli for future research. This paper is work in progress. We plan to actively continue adding papers that we missed or that are newly published.

If you want to contribute, spot an error or miss a paper, please do reach out to us. Figure 1 : A rough overview of a plain Transformer Encoder Block (grey block) without any position information. They grey block is usually repeated for l layers. An overview of the actual attention computation is shown on the right.",Is there a predominant method for position encodings that most people use?,"No, not really. There is a lot going on and even ""old"" position models sometimes come back in a slight variant because they have a specific property that is useful for a new model or task.","Hi, the title sounds interesting, can you tell me more about the paper? Hi, I agree that the title sounds interesting. The main topic of the paper is the Transformer model. Do you need information on this model?
 Facts: 
1. The Transformer model as introduced by Vaswani et al. (2017)
2. Transformers are arguably the main workhorse in recent Natural Language Processing research.
 No thanks, I am familiar with transformers. Is it a pure literature review of different ways to encode position in transformers, or do the authors also conduct experiments? It is a survey paper. The main contribution is a theoretical overview of existing position models and no empirical analysis.
 Facts: 
1. The objectives of this survey are to i) showcase that position information in Transformer is a vibrant and extensive research area; ii) enable the reader to compare existing methods by providing a unified notation and meaningful clustering;
 Ah I see, so they state if there are better and worse ones? I do not think that the main goal is to define better or worse. It is more about identifying similarities and differences (supported by the unified notation). They do have a discussion on matching a given task to a certain kind of application though.
 Facts: 
1. broad overview of relevant work on position information in Transformers that systematically aggregates and categorizes existing approaches and analyzes the differences between them
2. indicating what characteristics of an application should be taken into account when selecting a position encoding
 Do they state if there are particular things to look out for when working with long documents? I do not have access to any concrete findings. But the position models of long document Transformers are certainly part of the survey. The authors state that they aim to complete the paper with newly published papers.
 Facts: 
1. We plan to actively continue adding papers that we missed or that are newly published.
 Ah that is interesting. Is there a specific purpose for the paper?",['Overall there is a huge variety of methods that add both absolute and relative position information to Transformer model.'],val
A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information.

We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate. Machines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human's need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; . However, most of those benchmarks focus on an ""open domain"" setting where the questions are not anchored in any particular user context.

The result is an emphasison generic factoid questions, rather than the full range of information needs people have. We present QASPER, an information-seeking question answering (QA) dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in the rest of the paper, along with evidence required to arrive at it. This setup results in questions requiring
more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger questions by their target readers that require supporting
or refuting claims. This evidence may be spread across the paper, including tables and figures, often resulting in complex entailment problems. The example in Figure 1 illustrates one such case where we need to retrieve information from paragraphs in three different sections to answer the question.

QASPER contains 5,049 questions over 1,585 natural language processing (NLP) papers, asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners. Each paper has an average of 3.2 questions, up to a maximum of
12 questions for a single paper. In addition to providing answers when the questions are answerable, the annotators were asked to select text, tables, or figures as evidence required for answering the questions. 55.5% of the questions require evidence from
multiple paragraphs in the paper and 13% require tables or figures. To the best of our knowledge, QASPER is the first QA dataset in the academic research domain focusing on entire papers, and not just abstracts.

To quantify the difficulty of the tasks in QASPER, we apply state-of-the-art document-level Transformer (Vaswani et al., 2017) models to the tasks of selecting evidence and generating answers, and show that the best model performance lags behind humans by 27 F1 points at answering questions from entire papers, and 32 F1 points at selecting the paragraphs that provide evidence to answer the questions, indicating that these are both unsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful.","Hi, what are information-seeking questions?",Hi! Information seeking questions are goal-priented questions that seek for specific information in a specific paper unlike e.g. open domain questions.,,"['Readers of academic research papers often read with the goal of answering specific questions', 'xisting informationseeking question answering datasets usually contain questions about generic factoid-type information.']",train
A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information.

We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate. Machines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human's need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; . However, most of those benchmarks focus on an ""open domain"" setting where the questions are not anchored in any particular user context.

The result is an emphasison generic factoid questions, rather than the full range of information needs people have. We present QASPER, an information-seeking question answering (QA) dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in the rest of the paper, along with evidence required to arrive at it. This setup results in questions requiring
more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger questions by their target readers that require supporting
or refuting claims. This evidence may be spread across the paper, including tables and figures, often resulting in complex entailment problems. The example in Figure 1 illustrates one such case where we need to retrieve information from paragraphs in three different sections to answer the question.

QASPER contains 5,049 questions over 1,585 natural language processing (NLP) papers, asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners. Each paper has an average of 3.2 questions, up to a maximum of
12 questions for a single paper. In addition to providing answers when the questions are answerable, the annotators were asked to select text, tables, or figures as evidence required for answering the questions. 55.5% of the questions require evidence from
multiple paragraphs in the paper and 13% require tables or figures. To the best of our knowledge, QASPER is the first QA dataset in the academic research domain focusing on entire papers, and not just abstracts.

To quantify the difficulty of the tasks in QASPER, we apply state-of-the-art document-level Transformer (Vaswani et al., 2017) models to the tasks of selecting evidence and generating answers, and show that the best model performance lags behind humans by 27 F1 points at answering questions from entire papers, and 32 F1 points at selecting the paragraphs that provide evidence to answer the questions, indicating that these are both unsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful.",Can you give an example for that?,"Sure! An example would be your initial question ""What are information-seeking questions?"". For this data a set of NLP experts was asked to come up with  questions on a paper based on title and abstract. Would you like to know more about the annotation process?","Hi, what are information-seeking questions?","[' Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text.']",train
A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information.

We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate. Machines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human's need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; . However, most of those benchmarks focus on an ""open domain"" setting where the questions are not anchored in any particular user context.

The result is an emphasison generic factoid questions, rather than the full range of information needs people have. We present QASPER, an information-seeking question answering (QA) dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in the rest of the paper, along with evidence required to arrive at it. This setup results in questions requiring
more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger questions by their target readers that require supporting
or refuting claims. This evidence may be spread across the paper, including tables and figures, often resulting in complex entailment problems. The example in Figure 1 illustrates one such case where we need to retrieve information from paragraphs in three different sections to answer the question.

QASPER contains 5,049 questions over 1,585 natural language processing (NLP) papers, asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners. Each paper has an average of 3.2 questions, up to a maximum of
12 questions for a single paper. In addition to providing answers when the questions are answerable, the annotators were asked to select text, tables, or figures as evidence required for answering the questions. 55.5% of the questions require evidence from
multiple paragraphs in the paper and 13% require tables or figures. To the best of our knowledge, QASPER is the first QA dataset in the academic research domain focusing on entire papers, and not just abstracts.

To quantify the difficulty of the tasks in QASPER, we apply state-of-the-art document-level Transformer (Vaswani et al., 2017) models to the tasks of selecting evidence and generating answers, and show that the best model performance lags behind humans by 27 F1 points at answering questions from entire papers, and 32 F1 points at selecting the paragraphs that provide evidence to answer the questions, indicating that these are both unsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful.","Yes, please tell me more about how the data was collected and annotated.",For a limited set of papers the first set of NLP experts would come up with the mentioned information-seeking questions. Another set of experts was then presented the questions and had to write an answer and provide evidence from the paper (if possible).,"Hi, what are information-seeking questions? Hi! Information seeking questions are goal-priented questions that seek for specific information in a specific paper unlike e.g. open domain questions.
 Facts: 
1. Readers of academic research papers often read with the goal of answering specific questions
2. xisting informationseeking question answering datasets usually contain questions about generic factoid-type information.
 Can you give an example for that?","['a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner ', 'questions are then answered by a separate set of NLP practitioners who also provide supporting evidence']",train
A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information.

We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate. Machines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human's need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; . However, most of those benchmarks focus on an ""open domain"" setting where the questions are not anchored in any particular user context.

The result is an emphasison generic factoid questions, rather than the full range of information needs people have. We present QASPER, an information-seeking question answering (QA) dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in the rest of the paper, along with evidence required to arrive at it. This setup results in questions requiring
more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger questions by their target readers that require supporting
or refuting claims. This evidence may be spread across the paper, including tables and figures, often resulting in complex entailment problems. The example in Figure 1 illustrates one such case where we need to retrieve information from paragraphs in three different sections to answer the question.

QASPER contains 5,049 questions over 1,585 natural language processing (NLP) papers, asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners. Each paper has an average of 3.2 questions, up to a maximum of
12 questions for a single paper. In addition to providing answers when the questions are answerable, the annotators were asked to select text, tables, or figures as evidence required for answering the questions. 55.5% of the questions require evidence from
multiple paragraphs in the paper and 13% require tables or figures. To the best of our knowledge, QASPER is the first QA dataset in the academic research domain focusing on entire papers, and not just abstracts.

To quantify the difficulty of the tasks in QASPER, we apply state-of-the-art document-level Transformer (Vaswani et al., 2017) models to the tasks of selecting evidence and generating answers, and show that the best model performance lags behind humans by 27 F1 points at answering questions from entire papers, and 32 F1 points at selecting the paragraphs that provide evidence to answer the questions, indicating that these are both unsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful.",Was there any quality assurance process included between both steps?,"I cannot answer this from abstract and introduction. But you might be interested in model performance on the data, right?","Hi, what are information-seeking questions? Hi! Information seeking questions are goal-priented questions that seek for specific information in a specific paper unlike e.g. open domain questions.
 Facts: 
1. Readers of academic research papers often read with the goal of answering specific questions
2. xisting informationseeking question answering datasets usually contain questions about generic factoid-type information.
 Can you give an example for that? Sure! An example would be your initial question ""What are information-seeking questions?"". For this data a set of NLP experts was asked to come up with  questions on a paper based on title and abstract. Would you like to know more about the annotation process?
 Facts: 
1.  Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text.
 Yes, please tell me more about how the data was collected and annotated.",[],train
A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information.

We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate. Machines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human's need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; . However, most of those benchmarks focus on an ""open domain"" setting where the questions are not anchored in any particular user context.

The result is an emphasison generic factoid questions, rather than the full range of information needs people have. We present QASPER, an information-seeking question answering (QA) dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in the rest of the paper, along with evidence required to arrive at it. This setup results in questions requiring
more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger questions by their target readers that require supporting
or refuting claims. This evidence may be spread across the paper, including tables and figures, often resulting in complex entailment problems. The example in Figure 1 illustrates one such case where we need to retrieve information from paragraphs in three different sections to answer the question.

QASPER contains 5,049 questions over 1,585 natural language processing (NLP) papers, asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners. Each paper has an average of 3.2 questions, up to a maximum of
12 questions for a single paper. In addition to providing answers when the questions are answerable, the annotators were asked to select text, tables, or figures as evidence required for answering the questions. 55.5% of the questions require evidence from
multiple paragraphs in the paper and 13% require tables or figures. To the best of our knowledge, QASPER is the first QA dataset in the academic research domain focusing on entire papers, and not just abstracts.

To quantify the difficulty of the tasks in QASPER, we apply state-of-the-art document-level Transformer (Vaswani et al., 2017) models to the tasks of selecting evidence and generating answers, and show that the best model performance lags behind humans by 27 F1 points at answering questions from entire papers, and 32 F1 points at selecting the paragraphs that provide evidence to answer the questions, indicating that these are both unsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful.","Hm, too bad. If we don't know how good the data is, doesn't this make it difficult to assess the performance of any model?","Yes, this is very true. In fact, the model performance is rather low, suggesting possibly a very high complexity of the task - or possibly an ill-defined task setup.","Hi, what are information-seeking questions? Hi! Information seeking questions are goal-priented questions that seek for specific information in a specific paper unlike e.g. open domain questions.
 Facts: 
1. Readers of academic research papers often read with the goal of answering specific questions
2. xisting informationseeking question answering datasets usually contain questions about generic factoid-type information.
 Can you give an example for that? Sure! An example would be your initial question ""What are information-seeking questions?"". For this data a set of NLP experts was asked to come up with  questions on a paper based on title and abstract. Would you like to know more about the annotation process?
 Facts: 
1.  Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text.
 Yes, please tell me more about how the data was collected and annotated. For a limited set of papers the first set of NLP experts would come up with the mentioned information-seeking questions. Another set of experts was then presented the questions and had to write an answer and provide evidence from the paper (if possible).
 Facts: 
1. a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner 
2. questions are then answered by a separate set of NLP practitioners who also provide supporting evidence
 Was there any quality assurance process included between both steps?","['We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers']",train
A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers,"Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing informationseeking question answering datasets usually contain questions about generic factoid-type information.

We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F 1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate. Machines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human's need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; . However, most of those benchmarks focus on an ""open domain"" setting where the questions are not anchored in any particular user context.

The result is an emphasison generic factoid questions, rather than the full range of information needs people have. We present QASPER, an information-seeking question answering (QA) dataset over academic research papers. Each question is written as a followup to the title and abstract of a particular paper, and the answer, if present, is identified in the rest of the paper, along with evidence required to arrive at it. This setup results in questions requiring
more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger questions by their target readers that require supporting
or refuting claims. This evidence may be spread across the paper, including tables and figures, often resulting in complex entailment problems. The example in Figure 1 illustrates one such case where we need to retrieve information from paragraphs in three different sections to answer the question.

QASPER contains 5,049 questions over 1,585 natural language processing (NLP) papers, asked by regular readers of NLP papers, and answered by a separate set of NLP practitioners. Each paper has an average of 3.2 questions, up to a maximum of
12 questions for a single paper. In addition to providing answers when the questions are answerable, the annotators were asked to select text, tables, or figures as evidence required for answering the questions. 55.5% of the questions require evidence from
multiple paragraphs in the paper and 13% require tables or figures. To the best of our knowledge, QASPER is the first QA dataset in the academic research domain focusing on entire papers, and not just abstracts.

To quantify the difficulty of the tasks in QASPER, we apply state-of-the-art document-level Transformer (Vaswani et al., 2017) models to the tasks of selecting evidence and generating answers, and show that the best model performance lags behind humans by 27 F1 points at answering questions from entire papers, and 32 F1 points at selecting the paragraphs that provide evidence to answer the questions, indicating that these are both unsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful.","Hm, interesting. What models were investigated and on which QA tasks were they evaluated?",LED and IR-baselines were used. The dataset combines multiple question types that require yes/no answers or full sentences from the paper.,"Hi, what are information-seeking questions? Hi! Information seeking questions are goal-priented questions that seek for specific information in a specific paper unlike e.g. open domain questions.
 Facts: 
1. Readers of academic research papers often read with the goal of answering specific questions
2. xisting informationseeking question answering datasets usually contain questions about generic factoid-type information.
 Can you give an example for that? Sure! An example would be your initial question ""What are information-seeking questions?"". For this data a set of NLP experts was asked to come up with  questions on a paper based on title and abstract. Would you like to know more about the annotation process?
 Facts: 
1.  Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text.
 Yes, please tell me more about how the data was collected and annotated. For a limited set of papers the first set of NLP experts would come up with the mentioned information-seeking questions. Another set of experts was then presented the questions and had to write an answer and provide evidence from the paper (if possible).
 Facts: 
1. a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner 
2. questions are then answered by a separate set of NLP practitioners who also provide supporting evidence
 Was there any quality assurance process included between both steps? I cannot answer this from abstract and introduction. But you might be interested in model performance on the data, right? Hm, too bad. If we don't know how good the data is, doesn't this make it difficult to assess the performance of any model?",[],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.",Hi! What is the main idea of the paper?,The main idea of this paper is to train annotators by sorting annotations by e.g. their difficulty so that they are gradually  introduced to the annotation project. Do you think that is a reasonable idea?,,['The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.'],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Yes, that sounds very smart! Is the effect of potential biases introduced this way investigated?",We conducted a carefully constructed user study with 40 participants that showed improvements using our methods compared to not sorting and we saw no big impact in annotation quality while reducing annotation speed. Does this answer your question?,Hi! What is the main idea of the paper?,"['Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. ', 'The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. ']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Yes, it does! I see that the underlying data domain are tweets on Covid-19 -- can you tell me more about the dataset used??","This dataset is about misconceptions about Covid-19, e.g. that Ginger Ale can cure it. Annotators were presented a tweet that mentioned such a misconception and were asked to select the most fitting category from a list of 4. Do you think that is a good task to evaluate on? ","Hi! What is the main idea of the paper? The main idea of this paper is to train annotators by sorting annotations by e.g. their difficulty so that they are gradually  introduced to the annotation project. Do you think that is a reasonable idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Yes, that sounds very smart! Is the effect of potential biases introduced this way investigated?","['Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic.']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","I see! I think it seems like a moderately difficult task, although I could imagine there are some ambiguities. I would say it is fitting. Can you tell me how the different sorting strategies looked like?",We sort by difficulty where difficulty comes either from heuristics like readability or masked language model score or learn to regress annotation times. Do you think annotation time is a good proxy for difficulty?,"Hi! What is the main idea of the paper? The main idea of this paper is to train annotators by sorting annotations by e.g. their difficulty so that they are gradually  introduced to the annotation project. Do you think that is a reasonable idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Yes, that sounds very smart! Is the effect of potential biases introduced this way investigated? We conducted a carefully constructed user study with 40 participants that showed improvements using our methods compared to not sorting and we saw no big impact in annotation quality while reducing annotation speed. Does this answer your question?
 Facts: 
1. Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. 
2. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. 
 Yes, it does! I see that the underlying data domain are tweets on Covid-19 -- can you tell me more about the dataset used??","['We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Yes, while it might not cover the whole complexity of difficulty this is the quantity we are most interested in the end. Time and hereby money for annotation. I am wondering, did the annotation quality of the ""non-trained"" annotators increase towards the end of the study too?","The annotation quality stayed mostly the same on a high level for all groups, we just measured improvement in annotation time. Do you have any more question?","Hi! What is the main idea of the paper? The main idea of this paper is to train annotators by sorting annotations by e.g. their difficulty so that they are gradually  introduced to the annotation project. Do you think that is a reasonable idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Yes, that sounds very smart! Is the effect of potential biases introduced this way investigated? We conducted a carefully constructed user study with 40 participants that showed improvements using our methods compared to not sorting and we saw no big impact in annotation quality while reducing annotation speed. Does this answer your question?
 Facts: 
1. Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. 
2. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. 
 Yes, it does! I see that the underlying data domain are tweets on Covid-19 -- can you tell me more about the dataset used?? This dataset is about misconceptions about Covid-19, e.g. that Ginger Ale can cure it. Annotators were presented a tweet that mentioned such a misconception and were asked to select the most fitting category from a list of 4. Do you think that is a good task to evaluate on? 
 Facts: 
1. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic.
 I see! I think it seems like a moderately difficult task, although I could imagine there are some ambiguities. I would say it is fitting. Can you tell me how the different sorting strategies looked like?",['The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality.'],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Hello, could you please tell me what is Annotation Curricula?",Annotation curricula mean that we sort the instances to be annotated by their difficulty to gradually introduce annotators to the task and data. Do you think that is a good idea?,,['The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.'],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.",Sounds good but I'm not sure. What kind of task does it use for?,"We use it to speed up annotation or sentence and paragraph level annotation tasks, e.g. POS, NER or sentence classification. We also conduct a user study for that. Do you want to hear more about the user study?","Hello, could you please tell me what is Annotation Curricula?",['this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks'],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.",Sure tell me more about that.,We conduct a user study to test wheter annotation curricula work. The data is about selecting the most fitting misconception about Covid-19 from a list. We show that using our method significantly reduces annotation time while not hurting annotation quality. ,"Hello, could you please tell me what is Annotation Curricula? Annotation curricula mean that we sort the instances to be annotated by their difficulty to gradually introduce annotators to the task and data. Do you think that is a good idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Sounds good but I'm not sure. What kind of task does it use for?","['Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. ']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.",That's excellent. So how do you measure annotation quality?,"We have the gold labels, so we compute accuracy. Does this sound reasonable?","Hello, could you please tell me what is Annotation Curricula? Annotation curricula mean that we sort the instances to be annotated by their difficulty to gradually introduce annotators to the task and data. Do you think that is a good idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Sounds good but I'm not sure. What kind of task does it use for? We use it to speed up annotation or sentence and paragraph level annotation tasks, e.g. POS, NER or sentence classification. We also conduct a user study for that. Do you want to hear more about the user study?
 Facts: 
1. this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks
 Sure tell me more about that.",['The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. '],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Yes. And you also said you reduced annotation time, what's the baseline you compare to?",Our user study has 40 participants some get annotation curriula and others get random orders. We also have a warmup phase with a fixed order that is the same for all groups. Do you have any questions left?,"Hello, could you please tell me what is Annotation Curricula? Annotation curricula mean that we sort the instances to be annotated by their difficulty to gradually introduce annotators to the task and data. Do you think that is a good idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Sounds good but I'm not sure. What kind of task does it use for? We use it to speed up annotation or sentence and paragraph level annotation tasks, e.g. POS, NER or sentence classification. We also conduct a user study for that. Do you want to hear more about the user study?
 Facts: 
1. this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks
 Sure tell me more about that. We conduct a user study to test wheter annotation curricula work. The data is about selecting the most fitting misconception about Covid-19 from a list. We show that using our method significantly reduces annotation time while not hurting annotation quality. 
 Facts: 
1. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. 
 That's excellent. So how do you measure annotation quality?","['Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic.', 'The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. ']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","One last question, how many time did you reduce?",We were able to reduce the time by around 20% when using gold order.,"Hello, could you please tell me what is Annotation Curricula? Annotation curricula mean that we sort the instances to be annotated by their difficulty to gradually introduce annotators to the task and data. Do you think that is a good idea?
 Facts: 
1. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.
 Sounds good but I'm not sure. What kind of task does it use for? We use it to speed up annotation or sentence and paragraph level annotation tasks, e.g. POS, NER or sentence classification. We also conduct a user study for that. Do you want to hear more about the user study?
 Facts: 
1. this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks
 Sure tell me more about that. We conduct a user study to test wheter annotation curricula work. The data is about selecting the most fitting misconception about Covid-19 from a list. We show that using our method significantly reduces annotation time while not hurting annotation quality. 
 Facts: 
1. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. 
 That's excellent. So how do you measure annotation quality? We have the gold labels, so we compute accuracy. Does this sound reasonable?
 Facts: 
1. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. 
 Yes. And you also said you reduced annotation time, what's the baseline you compare to?",[' The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. '],train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Hi!,Hi! Is there something you want to know about the article?,,[],train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.","Yes, please. How does explaination is provided?","I unfortunately cannot provide any detail about the explanation, but I can tell you that the algorithm, called LIME, can explain the prediction of any classifier",Hi!,"['LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Can you tell me more about the experimental setup?,"The material I have mostly concerns the utility of this tool, I can only tell you that it was tested with human subjects that decided whether to trust a prediction or not based on the explanations provided by the tool.","Hi! Hi! Is there something you want to know about the article? Yes, please. How does explaination is provided?","['We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",What should we talk about now?,"I can tell you more about specific use cases of this tool. For example, it could be very beneficial when it comes to trusting machine learning for mediacal diagnosis.","Hi! Hi! Is there something you want to know about the article? Yes, please. How does explaination is provided? I unfortunately cannot provide any detail about the explanation, but I can tell you that the algorithm, called LIME, can explain the prediction of any classifier
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 Can you tell me more about the experimental setup?","['When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Do they say something about comparison with other methods?,"Unfortunately not, they propose two different methods of their own.","Hi! Hi! Is there something you want to know about the article? Yes, please. How does explaination is provided? I unfortunately cannot provide any detail about the explanation, but I can tell you that the algorithm, called LIME, can explain the prediction of any classifier
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 Can you tell me more about the experimental setup? The material I have mostly concerns the utility of this tool, I can only tell you that it was tested with human subjects that decided whether to trust a prediction or not based on the explanations provided by the tool.
 Facts: 
1. We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted
 What should we talk about now?","['• LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Which one is the best among the two methods?,"They are different, but I think LIME is the most complete one as it was used by non experts to pick the best out of two classifiers and improve an untrustworthy one.","Hi! Hi! Is there something you want to know about the article? Yes, please. How does explaination is provided? I unfortunately cannot provide any detail about the explanation, but I can tell you that the algorithm, called LIME, can explain the prediction of any classifier
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 Can you tell me more about the experimental setup? The material I have mostly concerns the utility of this tool, I can only tell you that it was tested with human subjects that decided whether to trust a prediction or not based on the explanations provided by the tool.
 Facts: 
1. We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted
 What should we talk about now? I can tell you more about specific use cases of this tool. For example, it could be very beneficial when it comes to trusting machine learning for mediacal diagnosis.
 Facts: 
1. When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic.
 Do they say something about comparison with other methods?","['In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world.', 'Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",What is this article about?,The article explains why understanding the reasons behind a prediction made by a machine learning model can make a difference in its actual use and proposes a new explanation technique.,,"['Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.', 'In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Is the approach validated with experiments involving human users?,"Yes, using LIME, non experts were able to choose the best one between different classifiers and to improve an untrustworthy one.",What is this article about?,"['In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world.', 'Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",How many datasets are used for validation?,"Unfortunately I cannot provide this information, but LIME can be used to explain the prediction of any classifier or regressor.","What is this article about? The article explains why understanding the reasons behind a prediction made by a machine learning model can make a difference in its actual use and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 Is the approach validated with experiments involving human users?","['LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",What style of interpretable model is used to explain the predictions?,I can only tell you that LIME provides this information in an interpretable and faithful manner by learning an interpretable model locally around the prediction.,"What is this article about? The article explains why understanding the reasons behind a prediction made by a machine learning model can make a difference in its actual use and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 Is the approach validated with experiments involving human users? Yes, using LIME, non experts were able to choose the best one between different classifiers and to improve an untrustworthy one.
 Facts: 
1. In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world.
2. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.
 How many datasets are used for validation?","['In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.","Does the article give a formal definition of ""interpretable"" and ""faithful""?",Unfortunately not in the material at my disposal.,"What is this article about? The article explains why understanding the reasons behind a prediction made by a machine learning model can make a difference in its actual use and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 Is the approach validated with experiments involving human users? Yes, using LIME, non experts were able to choose the best one between different classifiers and to improve an untrustworthy one.
 Facts: 
1. In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world.
2. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.
 How many datasets are used for validation? Unfortunately I cannot provide this information, but LIME can be used to explain the prediction of any classifier or regressor.
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 What style of interpretable model is used to explain the predictions?",[],train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Is the approach applied to natural language tasks?,LIME was used to improve an untrustworthy classifier trained on newsgroups.,"What is this article about? The article explains why understanding the reasons behind a prediction made by a machine learning model can make a difference in its actual use and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 Is the approach validated with experiments involving human users? Yes, using LIME, non experts were able to choose the best one between different classifiers and to improve an untrustworthy one.
 Facts: 
1. In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world.
2. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.
 How many datasets are used for validation? Unfortunately I cannot provide this information, but LIME can be used to explain the prediction of any classifier or regressor.
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 What style of interpretable model is used to explain the predictions? I can only tell you that LIME provides this information in an interpretable and faithful manner by learning an interpretable model locally around the prediction.
 Facts: 
1. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 Does the article give a formal definition of ""interpretable"" and ""faithful""?","['Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.']",train
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.","Hello, what is this paper about?","Hi! The paper explains why, in some cases, it is crucial to understand why a machine learning model made a prediction and proposes a new explanation technique.",,"['Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.', 'In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.']",val
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",I think this is really a very important issue in AI. How does the proposed technique work? Do you extract logic rules?,"Unfortunately I cannot provide detailed information, LIME approximates the predictions of any classifier or regressor by approximating it locally with an interpretable model.","Hello, what is this paper about?","['LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.']",val
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",On which tasks did you apply LIME?,"Lime was used by non-experts to pick the best one between two classifiers, to improve an untrustworthy one and for the understanding of the predictions of a neural network on images.","Hello, what is this paper about? Hi! The paper explains why, in some cases, it is crucial to understand why a machine learning model made a prediction and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 I think this is really a very important issue in AI. How does the proposed technique work? Do you extract logic rules?","['In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.', 'We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.']",val
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.","Thank you, this is very interesting indeed. Which do you think is the strongest point of your approach?","I think that knowing the reasons behind a prediction could make people trust machine learning systems more, mostly in fields such as the medical one.","Hello, what is this paper about? Hi! The paper explains why, in some cases, it is crucial to understand why a machine learning model made a prediction and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 I think this is really a very important issue in AI. How does the proposed technique work? Do you extract logic rules? Unfortunately I cannot provide detailed information, LIME approximates the predictions of any classifier or regressor by approximating it locally with an interpretable model.
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 On which tasks did you apply LIME?","['When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic.']",val
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.",Did you measure performance of the interpretable patterns extracted by LIME?,Unfortunately this information is not in the material at my disposal. But LIME was evaluated with simulated and human subjects.,"Hello, what is this paper about? Hi! The paper explains why, in some cases, it is crucial to understand why a machine learning model made a prediction and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 I think this is really a very important issue in AI. How does the proposed technique work? Do you extract logic rules? Unfortunately I cannot provide detailed information, LIME approximates the predictions of any classifier or regressor by approximating it locally with an interpretable model.
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 On which tasks did you apply LIME? Lime was used by non-experts to pick the best one between two classifiers, to improve an untrustworthy one and for the understanding of the predictions of a neural network on images.
 Facts: 
1. In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.
2. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.
 Thank you, this is very interesting indeed. Which do you think is the strongest point of your approach?","['Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.']",val
"""Why Should I Trust You?"" Explaining the Predictions of Any Classifier","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem.

We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted. Machine learning is at the core of many recent advances in science and technology. Unfortunately, the important role of humans is an oft-overlooked aspect in the field. Whether humans are directly using machine learning classifiers as tools, or are deploying models within other products, a vital concern remains: if the users do not trust a model or a prediction, they will not use it. It is important to differentiate between two different (but related) definitions of trust: (1) trusting a prediction, i.e.

whether a user trusts an individual prediction sufficiently to take some action based on it, and (2) trusting a model, i.e. whether the user trusts a model to behave in reasonable ways if deployed. Both are directly impacted by Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.

Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Determining trust in individual predictions is an important problem when the model is used for decision making.

When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic. Apart from trusting individual predictions, there is also a need to evaluate the model as a whole before deploying it ""in the wild"". To make this decision, users need to be confident that the model will perform well on real-world data, according to the metrics of interest. Currently, models are evaluated using accuracy metrics on an available validation dataset.

However, real-world data is often significantly different, and further, the evaluation metric may not be indicative of the product's goal. Inspecting individual predictions and their explanations is a worthwhile solution, in addition to such metrics. In this case, it is important to aid users by suggesting which instances to inspect, especially for large datasets. In this paper, we propose providing explanations for individual predictions as a solution to the ""trusting a prediction"" problem, and selecting multiple such predictions (and explanations) as a solution to the ""trusting the model"" problem.

Our main contributions are summarized as follows. • LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model. • SP-LIME, a method that selects a set of representative instances with explanations to address the ""trusting the model"" problem, via submodular optimization. • Comprehensive evaluation with simulated and human subjects, where we measure the impact of explanations on trust and associated tasks.

In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.","Thank you, this is all I wanted to know!",Thank you for your interest! Do you think that a tool like LIME could be helpful in other situations?,"Hello, what is this paper about? Hi! The paper explains why, in some cases, it is crucial to understand why a machine learning model made a prediction and proposes a new explanation technique.
 Facts: 
1. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model.
2. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction.
 I think this is really a very important issue in AI. How does the proposed technique work? Do you extract logic rules? Unfortunately I cannot provide detailed information, LIME approximates the predictions of any classifier or regressor by approximating it locally with an interpretable model.
 Facts: 
1. LIME, an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.
 On which tasks did you apply LIME? Lime was used by non-experts to pick the best one between two classifiers, to improve an untrustworthy one and for the understanding of the predictions of a neural network on images.
 Facts: 
1. In our experiments, non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. Further, they are able to greatly improve an untrustworthy classifier trained on 20 newsgroups, by doing feature engineering using LIME.
2. We also show how understanding the predictions of a neural network on images helps practitioners know when and why they should not trust a model.
 Thank you, this is very interesting indeed. Which do you think is the strongest point of your approach? I think that knowing the reasons behind a prediction could make people trust machine learning systems more, mostly in fields such as the medical one.
 Facts: 
1. When using machine learning for medical diagnosis [6] or terrorism detection, for example, predictions cannot be acted upon on blind faith, as the consequences may be catastrophic.
 Did you measure performance of the interpretable patterns extracted by LIME?",[],val
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Hi!,Hi! do you want to talk about multi-task residual networks for argument mining?,,['Multi-Task Attentive Residual Networks for Argument Mining'],train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.","Yes, please. What is the paper about?","The paper discusses residual network for several argumentative mining tasks, including link prediction",Hi!,"['We explore the use of residual networks and neural attention for argument mining and in particular link prediction.', 'We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble.']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Can you say something more about the method used to solve the tasks?,Sure. All tasks are tackled using a single neural network with an attention module and ensemble learning. All tasks are performed jointly. there is no feature engineering.,"Hi! Hi! do you want to talk about multi-task residual networks for argument mining?
 Facts: 
1. Multi-Task Attentive Residual Networks for Argument Mining
 Yes, please. What is the paper about?","['this paper extends the neural architecture with attention and ensemble learning', 'Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Is the method compared to other ones?,"Sure, the paper offers comparisons with other state-of-the-art systems.","Hi! Hi! do you want to talk about multi-task residual networks for argument mining?
 Facts: 
1. Multi-Task Attentive Residual Networks for Argument Mining
 Yes, please. What is the paper about? The paper discusses residual network for several argumentative mining tasks, including link prediction
 Facts: 
1. We explore the use of residual networks and neural attention for argument mining and in particular link prediction.
2. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble.
 Can you say something more about the method used to solve the tasks?","['this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora']",train
Multi-Task Attentive Residual Networks for Argument Mining,"We explore the use of residual networks and neural attention for argument mining and in particular link prediction. The method we propose makes no assumptions on document or argument structure. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble. We evaluate it on a challenging data set consisting of usergenerated comments, as well as on two other datasets consisting of scientific publications.

On the user-generated content dataset, our model outperforms state-of-the-art methods that rely on domain knowledge. On the scientific literature datasets it achieves results comparable to those yielded by BERT-based approaches but with a much smaller model size. A RGUMENT mining (AM) is an emerging research area in natural language processing (NLP) which aims to extract arguments from text collections [1] . AM consists of several different tasks, that include argument detection, stance classification, topic-based argumentative content retrieval, and many others [2] . In this work we focus on the challenging problem of assembling the structure of the argumentation graph of a given input document. Such problem comprises the detection of both argument components, and relations (or links) amongst them, and is thus one of the most difficult steps for AM systems.

While there is no unique definition of an argument, one of the most popular ones was proposed by Douglas Walton [3] , who defined an argument as the collection of three parts: (i) a claim, or assertion, about a given topic; (ii) a set of premises supporting the claim; (iii) the inference between the premises and the claim. Relations between arguments, or argument components, typically consist of either support or attack links. AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation. It is very often the case that AM systems build upon sets of handcrafted features which encode information about the underlying argument model, the genre, or the topic of interest.

These approaches typically make some assumptions on the argumentative structure of the given input document, thus constraining the resulting argument graph. We propose a general-purpose neural architecture that is domain-agnostic, and that does not rely on specific genre-or topic-dependent features. The model exploits neural attention and multi-task learning, jointly addressing the problems of identifying the category of argument components, and predicting the relations among them. Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.

They also suggest that, when background information about the structure of annotations in a corpus is given, ad-hoc approaches may yield better performance. Our main contributions are:

 • A novel approach to AM, which extends our previous work [7] by introducing an attention module and ensemble learning. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance. • An analytical evaluation of the contribution of each added module through an ablation study and a validation of our model on a challenging corpus, indicating that our proposed model improves state-of-the-art results in all the tasks we address.

• A set of experiments designed to assess generality, whereby we test our approach on three additional corpora that vary in domain, style of writing, formatting, length, and annotation model. With respect to our previous work [7] , this paper extends the neural architecture with attention and ensemble learning, and presents a more thorough and extensive experimental evaluation, offering comparisons with state-of-the-art systems across four different argument mining corpora. All the code used in our experiments is publicly available. 1 The paper is organized as follows.

We present related work in Section II. Section III introduces our architectures. Section IV describes the data used for evaluation. Section V describes our experimental setting, whereas results are presented and discussed in Section VI.

Section VII concludes.",Are the results better than state-of-the-art cited in the paper?,"This is not clear from the information I have. The method has good performance, but the main advantage seems to be generality.","Hi! Hi! do you want to talk about multi-task residual networks for argument mining?
 Facts: 
1. Multi-Task Attentive Residual Networks for Argument Mining
 Yes, please. What is the paper about? The paper discusses residual network for several argumentative mining tasks, including link prediction
 Facts: 
1. We explore the use of residual networks and neural attention for argument mining and in particular link prediction.
2. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble.
 Can you say something more about the method used to solve the tasks? Sure. All tasks are tackled using a single neural network with an attention module and ensemble learning. All tasks are performed jointly. there is no feature engineering.
 Facts: 
1. this paper extends the neural architecture with attention and ensemble learning
2. Such a model performs multiple AM tasks at the same time, and does not rely on ad-hoc features or rich contextual information, but only on GloVe embeddings and on a widely applicable notion of distance
 Is the method compared to other ones?","['Experimental results conducted on a variety of different corpora show that the model is robust and achieves good performance across the considered data sets.', 'AM approaches are very often tailored to specific corpora or genres [4] - [6] , with solutions that are seldom general enough to be directly applied to different data sets without the need of any adaptation.']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",Hi!,Hi. Do you want to know something about unfairness detection in consumer contracts?,,['Detecting and explaining unfairness in consumer contracts through memory networks'],train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.","Yes, please. What are memory networks?","Memory networks or MANNs are neural architectures that include a ""memory"" components where knowledge can be stored and used to generate the output",Hi!,"['The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.', 'A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query.']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",How are MANNs employed to detect and explain unfairness?,The memories could be used to store justifications of unfairness given by legal experts. Then these justifications can be used to explain the output,"Hi! Hi. Do you want to know something about unfairness detection in consumer contracts?
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 Yes, please. What are memory networks?","['our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair.', 'if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",It seems very nice idea! Maybe it can be applied to other kind of tasks. What do you think?,"Yes indeed. For example, memory networks have been used for question-answering.","Hi! Hi. Do you want to know something about unfairness detection in consumer contracts?
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 Yes, please. What are memory networks? Memory networks or MANNs are neural architectures that include a ""memory"" components where knowledge can be stored and used to generate the output
 Facts: 
1. The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.
2. A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query.
 How are MANNs employed to detect and explain unfairness?","['Consider for instance the following story:   Joe went to the kitchen.  Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.', 'Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .  A MANN can answer these questions']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",Can you give me more details about the experimental setup?,The authors created a dataset of annotated Terms of Service and compared the performance of MANNs against relevant baselines. Do you want to know about qualitative evaluation too?,"Hi! Hi. Do you want to know something about unfairness detection in consumer contracts?
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 Yes, please. What are memory networks? Memory networks or MANNs are neural architectures that include a ""memory"" components where knowledge can be stored and used to generate the output
 Facts: 
1. The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.
2. A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query.
 How are MANNs employed to detect and explain unfairness? The memories could be used to store justifications of unfairness given by legal experts. Then these justifications can be used to explain the output
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair.
2. if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN
 It seems very nice idea! Maybe it can be applied to other kind of tasks. What do you think?","['The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations.', 'We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks.']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.","Yes, please.","The explanations were also evaluated by domain experts. If you are interested, I can tell you about the outcome of the evaluation","Hi! Hi. Do you want to know something about unfairness detection in consumer contracts?
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 Yes, please. What are memory networks? Memory networks or MANNs are neural architectures that include a ""memory"" components where knowledge can be stored and used to generate the output
 Facts: 
1. The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.
2. A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query.
 How are MANNs employed to detect and explain unfairness? The memories could be used to store justifications of unfairness given by legal experts. Then these justifications can be used to explain the output
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair.
2. if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN
 It seems very nice idea! Maybe it can be applied to other kind of tasks. What do you think? Yes indeed. For example, memory networks have been used for question-answering.
 Facts: 
1. Consider for instance the following story:   Joe went to the kitchen.  Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.
2. Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .  A MANN can answer these questions
 Can you give me more details about the experimental setup?",['We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context.'],train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.","Yes, I am really interested in it.","Results are encouraging, and some baselines were outperformed by a significant margin. One interesting thing in this paper is the application of strong supervision. Do you want to know about it?","Hi! Hi. Do you want to know something about unfairness detection in consumer contracts?
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 Yes, please. What are memory networks? Memory networks or MANNs are neural architectures that include a ""memory"" components where knowledge can be stored and used to generate the output
 Facts: 
1. The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.
2. A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query.
 How are MANNs employed to detect and explain unfairness? The memories could be used to store justifications of unfairness given by legal experts. Then these justifications can be used to explain the output
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair.
2. if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN
 It seems very nice idea! Maybe it can be applied to other kind of tasks. What do you think? Yes indeed. For example, memory networks have been used for question-answering.
 Facts: 
1. Consider for instance the following story:   Joe went to the kitchen.  Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.
2. Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .  A MANN can answer these questions
 Can you give me more details about the experimental setup? The authors created a dataset of annotated Terms of Service and compared the performance of MANNs against relevant baselines. Do you want to know about qualitative evaluation too?
 Facts: 
1. The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations.
2. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks.
 Yes, please.","['The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin.', 'unlike all other baselines, the MANN could provide meaningful references to the relevant rationales']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",I think it's enough. Thanks a lot!,Bye,"Hi! Hi. Do you want to know something about unfairness detection in consumer contracts?
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 Yes, please. What are memory networks? Memory networks or MANNs are neural architectures that include a ""memory"" components where knowledge can be stored and used to generate the output
 Facts: 
1. The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to.
2. A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query.
 How are MANNs employed to detect and explain unfairness? The memories could be used to store justifications of unfairness given by legal experts. Then these justifications can be used to explain the output
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair.
2. if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN
 It seems very nice idea! Maybe it can be applied to other kind of tasks. What do you think? Yes indeed. For example, memory networks have been used for question-answering.
 Facts: 
1. Consider for instance the following story:   Joe went to the kitchen.  Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.
2. Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .  A MANN can answer these questions
 Can you give me more details about the experimental setup? The authors created a dataset of annotated Terms of Service and compared the performance of MANNs against relevant baselines. Do you want to know about qualitative evaluation too?
 Facts: 
1. The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations.
2. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks.
 Yes, please. The explanations were also evaluated by domain experts. If you are interested, I can tell you about the outcome of the evaluation
 Facts: 
1. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context.
 Yes, I am really interested in it.",[],train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",Hi! What is this article about?,This article is an application of memory networks for detecting unfairness in consumer contracts,,['Detecting and explaining unfairness in consumer contracts through memory networks'],train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",How were memory networks used for this task?,memories are used to store rationales. Do you want to know more about rationales?,Hi! What is this article about?,"['We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.', 'we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.","Yes, thank you",rationales are justifications provided by experts. Anything else you wish to know?,"Hi! What is this article about? This article is an application of memory networks for detecting unfairness in consumer contracts
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 How were memory networks used for this task?","['our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",Why are rationales useful?,"The hypothesis is that rationales are useful for explainability. Indeed, initial results are promising. What do you think about the importance of explainability?","Hi! What is this article about? This article is an application of memory networks for detecting unfairness in consumer contracts
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 How were memory networks used for this task? memories are used to store rationales. Do you want to know more about rationales?
 Facts: 
1. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.
2. we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels
 Yes, thank you","['in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes.', 'These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",I think explainability is very useful in AI. Anything else you would like to tell me about the article?,Yes. Something I like about this study is that the method is able to provide references to relevant rationales.,"Hi! What is this article about? This article is an application of memory networks for detecting unfairness in consumer contracts
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 How were memory networks used for this task? memories are used to store rationales. Do you want to know more about rationales?
 Facts: 
1. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.
2. we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels
 Yes, thank you rationales are justifications provided by experts. Anything else you wish to know?
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair
 Why are rationales useful?","['the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision']",train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",That's interesting! Can you tell me something more about this?,"I don't have further information about strong supervision, but the method seems promising","Hi! What is this article about? This article is an application of memory networks for detecting unfairness in consumer contracts
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 How were memory networks used for this task? memories are used to store rationales. Do you want to know more about rationales?
 Facts: 
1. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.
2. we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels
 Yes, thank you rationales are justifications provided by experts. Anything else you wish to know?
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair
 Why are rationales useful? The hypothesis is that rationales are useful for explainability. Indeed, initial results are promising. What do you think about the importance of explainability?
 Facts: 
1. in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes.
2. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.
 I think explainability is very useful in AI. Anything else you would like to tell me about the article?",['These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.'],train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.","Ok, thank you! That's all I wanted to know",That's great. Take care,"Hi! What is this article about? This article is an application of memory networks for detecting unfairness in consumer contracts
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 How were memory networks used for this task? memories are used to store rationales. Do you want to know more about rationales?
 Facts: 
1. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.
2. we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels
 Yes, thank you rationales are justifications provided by experts. Anything else you wish to know?
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair
 Why are rationales useful? The hypothesis is that rationales are useful for explainability. Indeed, initial results are promising. What do you think about the importance of explainability?
 Facts: 
1. in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes.
2. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.
 I think explainability is very useful in AI. Anything else you would like to tell me about the article? Yes. Something I like about this study is that the method is able to provide references to relevant rationales.
 Facts: 
1. the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision
 That's interesting! Can you tell me something more about this?",[],train
Detecting and explaining unfairness in consumer contracts through memory networks,"Recent work has demonstrated how data-driven AI methods can leverage consumer protection by supporting the automated analysis of legal documents. However, a shortcoming of data-driven approaches is poor explainability. We posit that in this domain useful explanations of classifier outcomes can be provided by resorting to legal rationales. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.

Our results show that rationales not only contribute to improve the classification accuracy, but are also able to offer meaningful, natural language explanations of otherwise opaque classifier outcomes. Terms of service (ToS), also known as terms and conditions or simply terms, are consumer contracts governing the relation between providers and users. Terms that cause a significant imbalance in the parties' rights and obligations, to the detriment of the consumer, are deemed unfair by Consumer Law. Despite substantive law in place, and despite the competence of enforcers for abstract control, providers of online services still tend to use unfair and unlawful clauses in these documents (Loos and Luzak 2016; Micklitz et al. 2017) .

Consumers often cannot do anything about it. To begin with, they rarely read the contracts they are required to accept (Obar and Oeldorf-Hirsch 2016) . Then, even if they did, a seemingly insurmountable knowledge barrier creates a clear unbalance. Legal knowledge is difficult, if not impossible, to access for individual consumers, as it is also difficult for consumers to know what data practices are implemented by companies and, therefore, to pinpoint unfair or unlawful conduct (Lippi et al.

2020) . Finally, even if consumers had sufficient knowledge and awareness to take legal action, there is still the insurmountable difference between the financial resources of the average customer and those of the average provider. To help mitigate such an unbalance, consumer protection organizations have the competence to initiate judicial or administrative proceedings. However, they do not have the resources to fight against each unlawful practice.

It was thus suggested that Artificial Intelligence (AI) and AI-based tools can aid consumer protection organizations and leverage consumer empowerment, for example by supporting the automatic analysis and exposure of unfair ToS clauses (Lippi et al. 2019a ). Among other initiatives, the CLAUDETTE project 1 undertook the challenge of consumer empowerment via AI, by investigating ways to automate reading and legal assessment of online consumer contracts and privacy policies with natural language processing techniques, so as to evaluate their compliance with EU consumer and data protection law. A web service developed and maintained by the project automatically analyzes any ToS a user may feed it, and returns an annotated version of the same document, which highlights the potentially unfair clauses it contains (Lippi et al.

2019b ). 2 While this constitutes a noteworthy first step, it suffers from poor transparency. In other words, however accurate a system like CLAUDETTE may be, it can hardly explain its output. This shortcoming is not specific to this particular system.

Indeed, in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes. This adds to the growing concern that data-driven machine-learning systems may exasperate existing biases and social inequalities (O'Neil 2016; Lippi et al. 2020 ). The debate is very lively as it involves thinkers with all sorts of backgrounds and complementary perspectives, governments, and, to some extent, the entire civil society.

There are good reasons for such a great interest. Research in social science suggests that providing explanations for recommended actions deeply influences users' confidence in, and acceptance of, AI-based decisions and recommendations (Cramer et al. 2008) . From this viewpoint, consumers, their organizations, and legal experts want to understand why a certain conclusion is made before accepting AI response.

In our opinion, a promising approach to associating explanations to the outcomes of neural-network classifiers could be enabled by Memory-Augmented Neural Networks or MANNs (Sukhbaatar et al. 2015) . The basic idea behind MANNs is to combine the successful learning strategies developed in the machine learning literature for inference with a memory component that can be read and written to. Consider for instance the following story:

 Joe went to the kitchen.

Fred went to the kitchen. Joe picked up the milk. Joe travelled to the office. Joe left the milk.

Joe went to the bathroom. Where is the milk now? Answering the question requires comprehension of the actions ""picked up"" and ""left"" as well as of the time elements of the story (Weston et al. 2014) .

A MANN can answer these questions by storing in dedicated parts of the network, called memories, all previously seen sentences, so as to retrieve the most relevant facts to a given query. The list of memories used to answer a given query, for example ""Joe travelled to the office"" and ""Joe left the milk"" constitutes, in a way, an explanation to the answer ""The milk is in the kitchen"". We believe that these tasks present similarities with the problem we are tackling, of providing an explanation to why a given clause has been labeled as potentially unfair. In particular, our hypothesis is that useful explanations may be given in terms of rationales, i.e.

ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair. Accordingly, if we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels, then a possible explanation of an unfairness prediction could be constructed based on the list of memories, i.e., the rationales, used by the MANN. Such explanations could be especially useful to legal experts and consumers because, rather than aiming to explain an underlying logical model or uncover the role of particular neural network connections, they would be more in line with a dialectical and communicative viewpoint, as advocated by Miller (2019) . Consider for example a unilateral termination clause, giving the provider the right to suspend or terminate the service and/or the contract.

In general, this provision could be unfair because, from the consumer's perspective, it could undermine the whole purpose of entering into the contract, and it may skew the balance of power between the parties involved. Indeed, the detection of a ""unilateral termination"" clause ""with 98.8 percent confidence"" could be a useful piece of information. However, the reason why a specific unilateral termination clause would be potentially unfair may not be self-evident. Instead, a more specific rationale such as ""the clause mentions the contract or access may be terminated but does not state the grounds for termination"" could provide a more compelling argument in that regard.

It would explain why a clause has been labeled as unfair, and would go in the direction of causal explanations, which are arguably more effective, in this context, than ""opaque"" confidence measures. This paper describes our approach to exposing unfairness by providing rationales using a MANN trained on a large corpus of online ToS. Since MANNs enable us to accommodate unstructured knowledge and easily embed raw text, we envision arbitrary knowledge integration as a middle ground between traditional structured information injection and natural language comprehension tasks. Thus, we find this type of memory-augmented architecture quite suitable for our purposes.

The system we built relies on an extensive study made on all the possible rationales associated with 5 major categories of unfair clauses, which we explicitly stated in the form of self-contained English sentences. This exercise served two purposes. The first one was to build a knowledge base that could help the laymen understand the possible motivations behind unfairness in the general case, and hopefully, to a broader extent, also guide service providers in defining fair terms of services. The other purpose was to be able to train MANN classifiers in detecting unfair clauses by encoding legal rationales in the memories.

The knowledge base of rationales constituted the basis for creating a corpus of 100 annotated ToS, which we used to train different MANN architecture configurations. We evaluated their performance with respect to relevant baselines, including support vector machine classifiers, convolutional neural networks and long shortmemory networks. We also run an initial qualitative evaluation with domain experts in order to understand the explanatory efficacy of rationales in this context. The novel corpus, as well as all the code needed to reproduce our experiments, are made available for research purposes.

3 The results on the new corpus are encouraging. The MANN architectures were able to match or outperform the baselines on all categories of unfair clauses, in some cases by a significant margin. Moreover, unlike all other baselines, the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.

A pilot study on the use of MANN for detecting and explaining unfair clauses in consumer contracts was recently presented by Lagioia et al. (2019) , and it gave promising results. Compared to it, the present study relies on a significantly extended dataset, with several unfairness categories and related rationales. Additionally, we evaluate multiple MANN configurations, limited to single-hop reasoning regarding task-related assumptions, and explore the benefits of strong supervision (see Sect.

4), from both the classification performance and the model explainability perspectives. The rest of this paper is organized as follows. In Sect. 2 we briefly discuss other machine-learning approaches in the consumer law domain, and the state of the art of machine-learning techniques used to address related problems.

In Sect. 3 we describe our corpus and the rationales used to annotate it. In Sect. 4 we introduce the MANN architectures used in our study and the experimental methodology we adopted.

Results are discussed in Sect. 5. Section 6 concludes.",Bye,Bye,"Hi! What is this article about? This article is an application of memory networks for detecting unfairness in consumer contracts
 Facts: 
1. Detecting and explaining unfairness in consumer contracts through memory networks
 How were memory networks used for this task? memories are used to store rationales. Do you want to know more about rationales?
 Facts: 
1. We thus consider several configurations of memory-augmented neural networks where rationales are given a special role in the modeling of context knowledge.
2. we train a MANN classifier to identify unfair clauses by using as facts the rationales behind unfairness labels
 Yes, thank you rationales are justifications provided by experts. Anything else you wish to know?
 Facts: 
1. our hypothesis is that useful explanations may be given in terms of rationales, i.e.  ad-hoc justifications provided by legal experts motivating their conclusion to consider a given clause as unfair
 Why are rationales useful? The hypothesis is that rationales are useful for explainability. Indeed, initial results are promising. What do you think about the importance of explainability?
 Facts: 
1. in recent years a rich debate has flourished around the opacity of AI systems that, in terms of accuracy, offer unprecedented results, but at the same time cannot be easily inspected in order to find reasons behind blatant and even possibly dangerous mistakes.
2. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.
 I think explainability is very useful in AI. Anything else you would like to tell me about the article? Yes. Something I like about this study is that the method is able to provide references to relevant rationales.
 Facts: 
1. the MANN could provide meaningful references to the relevant rationales, especially if during training the MANN is fed with the information of which rationales are related to which clause, a technique known as strong supervision
 That's interesting! Can you tell me something more about this? I don't have further information about strong supervision, but the method seems promising
 Facts: 
1. These results suggest that MANN are a promising way to address the problem of explaining unfairness in consumer contracts and pave the way to their extensive use in other areas of automated legal text analytics.
 Ok, thank you! That's all I wanted to know",[],train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.",What do you mean with low resource domain?,"We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?",,"['Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. ']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.","Interesting, I agree. So how does this approach work?","We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?",What do you mean with low resource domain?,"['For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.', 'We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.","Can you provide a more specific answer, like what algorithms are used and how the support comes in? ",We help users to find spans that contain named entities and we help them to select the right one from the knowledge base search by reranking. Models are trained interactively in the loop. Does this clarify things?,"What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work?","['We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: ', 'we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.",Only partly. Are the spans identified by the model as well or by the user? ,"The annotator is still in charge and needs to acutally select spans, the machine learning models are just there to give suggestions.","What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work? We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?
 Facts: 
1. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.
2. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.
 Can you provide a more specific answer, like what algorithms are used and how the support comes in? ","['we use recommenders that suggest potential concepts and adaptive candidate ranking', 'we therefore add interactive machine learning annotation support that helps the user find entities in the text']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.",so the input is the selected span and the model provides a ranking for the labels?,"For the reranking part, the user selects a span and then searches for the right entity. These are both used as inputs for the reranking.","What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work? We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?
 Facts: 
1. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.
2. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.
 Can you provide a more specific answer, like what algorithms are used and how the support comes in?  We help users to find spans that contain named entities and we help them to select the right one from the knowledge base search by reranking. Models are trained interactively in the loop. Does this clarify things?
 Facts: 
1. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: 
2. we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.
 Only partly. Are the spans identified by the model as well or by the user? ","['e use recommenders that suggest potential concepts and adaptive candidate ranking', 'we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.",Sounds promising! So how well do the models perform?,"Using these in a user study, we measured an annotation time improvement of up to 35%. Do you think this is useful?","What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work? We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?
 Facts: 
1. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.
2. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.
 Can you provide a more specific answer, like what algorithms are used and how the support comes in?  We help users to find spans that contain named entities and we help them to select the right one from the knowledge base search by reranking. Models are trained interactively in the loop. Does this clarify things?
 Facts: 
1. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: 
2. we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.
 Only partly. Are the spans identified by the model as well or by the user?  The annotator is still in charge and needs to acutally select spans, the machine learning models are just there to give suggestions.
 Facts: 
1. we use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text
 so the input is the selected span and the model provides a ranking for the labels?",['We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.'],train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.","It depends, improvement of 35% compared to what baseline?",To not using annotation support. Do you have any more questions?,"What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work? We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?
 Facts: 
1. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.
2. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.
 Can you provide a more specific answer, like what algorithms are used and how the support comes in?  We help users to find spans that contain named entities and we help them to select the right one from the knowledge base search by reranking. Models are trained interactively in the loop. Does this clarify things?
 Facts: 
1. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: 
2. we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.
 Only partly. Are the spans identified by the model as well or by the user?  The annotator is still in charge and needs to acutally select spans, the machine learning models are just there to give suggestions.
 Facts: 
1. we use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text
 so the input is the selected span and the model provides a ranking for the labels? For the reranking part, the user selects a span and then searches for the right entity. These are both used as inputs for the reranking.
 Facts: 
1. e use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them
 Sounds promising! So how well do the models perform?","['In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.",Could you quantify if there was any bias introduced due to the recommendations?,That we did not measure.,"What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work? We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?
 Facts: 
1. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.
2. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.
 Can you provide a more specific answer, like what algorithms are used and how the support comes in?  We help users to find spans that contain named entities and we help them to select the right one from the knowledge base search by reranking. Models are trained interactively in the loop. Does this clarify things?
 Facts: 
1. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: 
2. we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.
 Only partly. Are the spans identified by the model as well or by the user?  The annotator is still in charge and needs to acutally select spans, the machine learning models are just there to give suggestions.
 Facts: 
1. we use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text
 so the input is the selected span and the model provides a ranking for the labels? For the reranking part, the user selects a span and then searches for the right entity. These are both used as inputs for the reranking.
 Facts: 
1. e use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them
 Sounds promising! So how well do the models perform? Using these in a user study, we measured an annotation time improvement of up to 35%. Do you think this is useful?
 Facts: 
1. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.
 It depends, improvement of 35% compared to what baseline?","['In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.']",train
From Zero to Hero: Human-In-The-Loop Entity Linking in Low Resource Domains,"Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). It is crucial in a considerable number of fields like humanities, technical writing and biomedical sciences to enrich texts with semantics and discover more knowledge. The use of EL in such domains requires handling noisy texts, low resource settings and domain-specific KBs. Existing approaches are mostly inappropriate for this, as they depend on training data.

However, in the above scenario, there exists hardly annotated data, and it needs to be created from scratch. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users. We evaluate our ranking approach in a simulation on difficult texts and show that it greatly outperforms a strong baseline in ranking accuracy. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system.

An open-source and ready-to-use implementation based on the text annotation platform INCEpTION 1 is made available 2 . Entity linking (EL) describes the task of disambiguating entity mentions in a text by linking them to a knowledge base (KB), e.g. the text span Earl of Orrery can be linked to the KB entry John Boyle, 5. Earl of Cork, thereby disambiguating it. EL is highly beneficial in many fields like digital humanities, classics, technical writing or biomedical sciences for applications like search (Meij et al., 1 https://inception-project.github.io 2 https://github.com/UKPLab/ acl2020-interactive-entity-linking Figure 1 : Difficult entity mentions with their linked entities: 1) Name variations, 2) Spelling Variation, 3) Ambiguity 2014), semantic enrichment (Schlögl and Lejtovicz, 2017) or information extraction (Nooralahzadeh and Øvrelid, 2018) .

These are overwhelmingly low-resource settings: often, no data annotated exists; coverage of open-domain knowledge bases like Wikipedia or DBPedia is low. Therefore, entity linking is frequently performed against domainspecific knowledge bases (Munnelly and Lawless, 2018a; Bartsch, 2004) . In these scenarios, the first crucial step is to obtain annotated data. This data can then be either directly used by researchers for their downstream task or to train machine learning models for automatic annotation.

For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach. Manual annotation is laborious and often prohibitively expensive. To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. The more entities are annotated, the better the annotation support will be.

Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. In this domain, texts are noisy as they were written in times where orthography was rather incidental or due to OCR and transcription errors (see Fig. 1 ). Tools like named entity recognizers are unavailable or perform poorly (Erdmann et al., 2019) .

We demonstrate the effectiveness of our approach with extensive simulation as well as a user study on different, challenging datasets. We implement our approach based on the open-source annotation platform INCEpTION (Klie et al., 2018) and publish all datasets and code. Our contributions are the following:

 1. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.

We validate our approach extensively in a simulation and in a user study. 2. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.",But couldn't it be that users were wrongly biased towards what the model suggested?,"The model suggest spans, but it does only rerank the candidates, so while it could bias I do not think that it did much","What do you mean with low resource domain? We say low resource domains are domains were data is scare, e.g. because of the language or because of the content. In our case, that is old English texts. Is that what you also understand as low resource?
 Facts: 
1. Throughout this work, we focus on texts from digital humanities, to be more precise, texts written in Early Modern English texts, including poems, biographies, novels as well as legal documents. 
 Interesting, I agree. So how does this approach work? We need to annotate data as it does not exist yet. As annotating is annoying, we support users with machine learning algorithms during that. Do you want to hear more about the support our system can provide?
 Facts: 
1. For this initial data creation step, we developed a novel Human-In-The-Loop (HITL) annotation approach.
2. We present a generic, KB-agnostic annotation approach for low-resource settings and provide a ready-to-use implementation so that researchers can easily annotate data for their use cases.
 Can you provide a more specific answer, like what algorithms are used and how the support comes in?  We help users to find spans that contain named entities and we help them to select the right one from the knowledge base search by reranking. Models are trained interactively in the loop. Does this clarify things?
 Facts: 
1. We therefore present a novel domain-agnostic Human-In-The-Loop annotation approach: 
2. we use recommenders that suggest potential concepts and adaptive candidate ranking, thereby speeding up the overall annotation process and making it less tedious for users.
 Only partly. Are the spans identified by the model as well or by the user?  The annotator is still in charge and needs to acutally select spans, the machine learning models are just there to give suggestions.
 Facts: 
1. we use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text
 so the input is the selected span and the model provides a ranking for the labels? For the reranking part, the user selects a span and then searches for the right entity. These are both used as inputs for the reranking.
 Facts: 
1. e use recommenders that suggest potential concepts and adaptive candidate ranking
2. we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them
 Sounds promising! So how well do the models perform? Using these in a user study, we measured an annotation time improvement of up to 35%. Do you think this is useful?
 Facts: 
1. We show that statistical machine learning models can be used in an interactive entity linking setting to improve annotation speed by over 35%.
 It depends, improvement of 35% compared to what baseline? To not using annotation support. Do you have any more questions?
 Facts:
1. In a user study, the annotation speed improves by 35 % compared to annotating without interactive support; users report that they strongly prefer our system. Could you quantify if there was any bias introduced due to the recommendations?","['To improve annotation speed and quality, we therefore add interactive machine learning annotation support that helps the user find entities in the text and select the correct knowledge base entries for them. ']",train
Cross-lingual Annotation Projection in Legal Texts,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods.

Our results indicate that a combination of word embeddings and dynamic time warping performs best. The European Union considers cultural and linguistic diversity, and in particular multilinguality, as some of its fundamental principles. So much so, that all the regulations and laws given by the Parliament are published in all of EU's twenty-four official languages. These could be an immense resource towards transparency, egalitarianism, accountability and democracy, giving the EU citizen access to legislative and policy proposals in their own and also in other languages (Steinberger et al., 2014) . However, to transform such resources into assets for the citizen, linguistic tools are needed, that can automatically analyze textual sources and yield actionable information (Lippi et al., 2019a) .

Unfortunately, producing the annotated corpora required to train such linguistic tools is, even for a single language, notoriously expensive. Moreover, the vast majority of linguistic resources and tools focus on English. Likewise, the workforce of professionals needed for annotating legal documents may not be readily available in each language. We thus investigate methods for automatically transferring the annotations made on legal documents in a language with significant linguistic resources and domain experts, such as English, onto the corresponding versions of the same documents in a target language, where such resources and expertise may be lacking.

Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language.This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language. We chose to focus on the detection of unfair clauses in online Terms of Service (ToS) and Privacy Policies (PP), which are usually published in multiple languages. However, the domain of interest of our study spans across many other types of legal texts. Other EU official documents published in multiple languages include, for instance, EU Parliament laws and regulations, documents of the EU Court of Justice, policy documents, documents for public consultations, and so on.

Therefore, the potential import of effective methods for facilitating cross-lingual legal text analysis is considerable. Reasons for focusing on ToS and PP are the interest for such documents from a consumer protection perspective, especially since the recent adoption of the European General Data Protection Regulation (GDPR), as well as the availability of tools for the analysis of such documents. One such tool is CLAUDETTE (Lippi et al., 2019b) , a web server for the automatic detection of potentially unfair clauses in ToS. At the time of writing, CLAUDETTE is only available for the English language.

An effective method for cross-lingual annotation projection could extend its scope to a variety of languages other than English, without having to resort to domain expertise in any of these languages. To carry out the present study, we built the first English-German parallel corpus for the task at hand. It turned out that the corpus is asymmetric, meaning a one-to-one sentence-by-sentence correspondence between source and target document is not always guaranteed. This aspect makes the problem particularly challenging, since we need to ensure a correct behaviour when a sentence does not appear in one of the documents or it has been integrated in another sentence: a property of sentence alignment called robustness (Simard and Plamondon, 1998) .

For example, we found source-document sentences corresponding to multiple target-document sentences, as well as significant rephrasing, resulting in the same sentence being annotated differently in the two documents. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series. We evaluate performance with several combinations of dissimilarity scores, with or without dynamic time warping. Our results indicate that the best performance is achieved by a combination of word embedding-based dissimilarity and dynamic time warping.

The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic. Sections 2 and 3 formalize the problem and describe the architecture of our solution; 4 and 5 present corpus and experiments; 6 discusses related work in the areas of sentence alignment and annotation projection; 7 concludes.",Hi! Why is annotation projection useful?,"Hi! Because you can avoid re-constructing a corpus from scratch, but you can leverage the information coming from the same corpus in another language",,"['the workforce of professionals needed for annotating legal documents may not be readily available in each language', 'Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language']",train
Cross-lingual Annotation Projection in Legal Texts,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods.

Our results indicate that a combination of word embeddings and dynamic time warping performs best. The European Union considers cultural and linguistic diversity, and in particular multilinguality, as some of its fundamental principles. So much so, that all the regulations and laws given by the Parliament are published in all of EU's twenty-four official languages. These could be an immense resource towards transparency, egalitarianism, accountability and democracy, giving the EU citizen access to legislative and policy proposals in their own and also in other languages (Steinberger et al., 2014) . However, to transform such resources into assets for the citizen, linguistic tools are needed, that can automatically analyze textual sources and yield actionable information (Lippi et al., 2019a) .

Unfortunately, producing the annotated corpora required to train such linguistic tools is, even for a single language, notoriously expensive. Moreover, the vast majority of linguistic resources and tools focus on English. Likewise, the workforce of professionals needed for annotating legal documents may not be readily available in each language. We thus investigate methods for automatically transferring the annotations made on legal documents in a language with significant linguistic resources and domain experts, such as English, onto the corresponding versions of the same documents in a target language, where such resources and expertise may be lacking.

Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language.This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language. We chose to focus on the detection of unfair clauses in online Terms of Service (ToS) and Privacy Policies (PP), which are usually published in multiple languages. However, the domain of interest of our study spans across many other types of legal texts. Other EU official documents published in multiple languages include, for instance, EU Parliament laws and regulations, documents of the EU Court of Justice, policy documents, documents for public consultations, and so on.

Therefore, the potential import of effective methods for facilitating cross-lingual legal text analysis is considerable. Reasons for focusing on ToS and PP are the interest for such documents from a consumer protection perspective, especially since the recent adoption of the European General Data Protection Regulation (GDPR), as well as the availability of tools for the analysis of such documents. One such tool is CLAUDETTE (Lippi et al., 2019b) , a web server for the automatic detection of potentially unfair clauses in ToS. At the time of writing, CLAUDETTE is only available for the English language.

An effective method for cross-lingual annotation projection could extend its scope to a variety of languages other than English, without having to resort to domain expertise in any of these languages. To carry out the present study, we built the first English-German parallel corpus for the task at hand. It turned out that the corpus is asymmetric, meaning a one-to-one sentence-by-sentence correspondence between source and target document is not always guaranteed. This aspect makes the problem particularly challenging, since we need to ensure a correct behaviour when a sentence does not appear in one of the documents or it has been integrated in another sentence: a property of sentence alignment called robustness (Simard and Plamondon, 1998) .

For example, we found source-document sentences corresponding to multiple target-document sentences, as well as significant rephrasing, resulting in the same sentence being annotated differently in the two documents. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series. We evaluate performance with several combinations of dissimilarity scores, with or without dynamic time warping. Our results indicate that the best performance is achieved by a combination of word embedding-based dissimilarity and dynamic time warping.

The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic. Sections 2 and 3 formalize the problem and describe the architecture of our solution; 4 and 5 present corpus and experiments; 6 discusses related work in the areas of sentence alignment and annotation projection; 7 concludes.",Is this approach applicable only in legal texts?,"No, it could be applied to any domain whether you have different versions of the same document.",Hi! Why is annotation projection useful?,"['This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language']",train
Cross-lingual Annotation Projection in Legal Texts,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods.

Our results indicate that a combination of word embeddings and dynamic time warping performs best. The European Union considers cultural and linguistic diversity, and in particular multilinguality, as some of its fundamental principles. So much so, that all the regulations and laws given by the Parliament are published in all of EU's twenty-four official languages. These could be an immense resource towards transparency, egalitarianism, accountability and democracy, giving the EU citizen access to legislative and policy proposals in their own and also in other languages (Steinberger et al., 2014) . However, to transform such resources into assets for the citizen, linguistic tools are needed, that can automatically analyze textual sources and yield actionable information (Lippi et al., 2019a) .

Unfortunately, producing the annotated corpora required to train such linguistic tools is, even for a single language, notoriously expensive. Moreover, the vast majority of linguistic resources and tools focus on English. Likewise, the workforce of professionals needed for annotating legal documents may not be readily available in each language. We thus investigate methods for automatically transferring the annotations made on legal documents in a language with significant linguistic resources and domain experts, such as English, onto the corresponding versions of the same documents in a target language, where such resources and expertise may be lacking.

Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language.This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language. We chose to focus on the detection of unfair clauses in online Terms of Service (ToS) and Privacy Policies (PP), which are usually published in multiple languages. However, the domain of interest of our study spans across many other types of legal texts. Other EU official documents published in multiple languages include, for instance, EU Parliament laws and regulations, documents of the EU Court of Justice, policy documents, documents for public consultations, and so on.

Therefore, the potential import of effective methods for facilitating cross-lingual legal text analysis is considerable. Reasons for focusing on ToS and PP are the interest for such documents from a consumer protection perspective, especially since the recent adoption of the European General Data Protection Regulation (GDPR), as well as the availability of tools for the analysis of such documents. One such tool is CLAUDETTE (Lippi et al., 2019b) , a web server for the automatic detection of potentially unfair clauses in ToS. At the time of writing, CLAUDETTE is only available for the English language.

An effective method for cross-lingual annotation projection could extend its scope to a variety of languages other than English, without having to resort to domain expertise in any of these languages. To carry out the present study, we built the first English-German parallel corpus for the task at hand. It turned out that the corpus is asymmetric, meaning a one-to-one sentence-by-sentence correspondence between source and target document is not always guaranteed. This aspect makes the problem particularly challenging, since we need to ensure a correct behaviour when a sentence does not appear in one of the documents or it has been integrated in another sentence: a property of sentence alignment called robustness (Simard and Plamondon, 1998) .

For example, we found source-document sentences corresponding to multiple target-document sentences, as well as significant rephrasing, resulting in the same sentence being annotated differently in the two documents. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series. We evaluate performance with several combinations of dissimilarity scores, with or without dynamic time warping. Our results indicate that the best performance is achieved by a combination of word embedding-based dissimilarity and dynamic time warping.

The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic. Sections 2 and 3 formalize the problem and describe the architecture of our solution; 4 and 5 present corpus and experiments; 6 discusses related work in the areas of sentence alignment and annotation projection; 7 concludes.",Can you tell me something more about how the method works?,"We exploit automatic translation tools and a distance between embeddings to find correspondences between the same document, available in to languages.","Hi! Why is annotation projection useful? Hi! Because you can avoid re-constructing a corpus from scratch, but you can leverage the information coming from the same corpus in another language
 Facts: 
1. the workforce of professionals needed for annotating legal documents may not be readily available in each language
2. Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language
 Is this approach applicable only in legal texts?","['The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series', 'To carry out the present study, we built the first English-German parallel corpus for the task at hand']",train
Cross-lingual Annotation Projection in Legal Texts,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods.

Our results indicate that a combination of word embeddings and dynamic time warping performs best. The European Union considers cultural and linguistic diversity, and in particular multilinguality, as some of its fundamental principles. So much so, that all the regulations and laws given by the Parliament are published in all of EU's twenty-four official languages. These could be an immense resource towards transparency, egalitarianism, accountability and democracy, giving the EU citizen access to legislative and policy proposals in their own and also in other languages (Steinberger et al., 2014) . However, to transform such resources into assets for the citizen, linguistic tools are needed, that can automatically analyze textual sources and yield actionable information (Lippi et al., 2019a) .

Unfortunately, producing the annotated corpora required to train such linguistic tools is, even for a single language, notoriously expensive. Moreover, the vast majority of linguistic resources and tools focus on English. Likewise, the workforce of professionals needed for annotating legal documents may not be readily available in each language. We thus investigate methods for automatically transferring the annotations made on legal documents in a language with significant linguistic resources and domain experts, such as English, onto the corresponding versions of the same documents in a target language, where such resources and expertise may be lacking.

Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language.This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language. We chose to focus on the detection of unfair clauses in online Terms of Service (ToS) and Privacy Policies (PP), which are usually published in multiple languages. However, the domain of interest of our study spans across many other types of legal texts. Other EU official documents published in multiple languages include, for instance, EU Parliament laws and regulations, documents of the EU Court of Justice, policy documents, documents for public consultations, and so on.

Therefore, the potential import of effective methods for facilitating cross-lingual legal text analysis is considerable. Reasons for focusing on ToS and PP are the interest for such documents from a consumer protection perspective, especially since the recent adoption of the European General Data Protection Regulation (GDPR), as well as the availability of tools for the analysis of such documents. One such tool is CLAUDETTE (Lippi et al., 2019b) , a web server for the automatic detection of potentially unfair clauses in ToS. At the time of writing, CLAUDETTE is only available for the English language.

An effective method for cross-lingual annotation projection could extend its scope to a variety of languages other than English, without having to resort to domain expertise in any of these languages. To carry out the present study, we built the first English-German parallel corpus for the task at hand. It turned out that the corpus is asymmetric, meaning a one-to-one sentence-by-sentence correspondence between source and target document is not always guaranteed. This aspect makes the problem particularly challenging, since we need to ensure a correct behaviour when a sentence does not appear in one of the documents or it has been integrated in another sentence: a property of sentence alignment called robustness (Simard and Plamondon, 1998) .

For example, we found source-document sentences corresponding to multiple target-document sentences, as well as significant rephrasing, resulting in the same sentence being annotated differently in the two documents. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series. We evaluate performance with several combinations of dissimilarity scores, with or without dynamic time warping. Our results indicate that the best performance is achieved by a combination of word embedding-based dissimilarity and dynamic time warping.

The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic. Sections 2 and 3 formalize the problem and describe the architecture of our solution; 4 and 5 present corpus and experiments; 6 discusses related work in the areas of sentence alignment and annotation projection; 7 concludes.",Is the quality of the translation tool an issue?,"It might be, but this needs further investigation. For sure, the method is language-agnostic.","Hi! Why is annotation projection useful? Hi! Because you can avoid re-constructing a corpus from scratch, but you can leverage the information coming from the same corpus in another language
 Facts: 
1. the workforce of professionals needed for annotating legal documents may not be readily available in each language
2. Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language
 Is this approach applicable only in legal texts? No, it could be applied to any domain whether you have different versions of the same document.
 Facts: 
1. This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language
 Can you tell me something more about how the method works?",['The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic'],train
Cross-lingual Annotation Projection in Legal Texts,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods.

Our results indicate that a combination of word embeddings and dynamic time warping performs best. The European Union considers cultural and linguistic diversity, and in particular multilinguality, as some of its fundamental principles. So much so, that all the regulations and laws given by the Parliament are published in all of EU's twenty-four official languages. These could be an immense resource towards transparency, egalitarianism, accountability and democracy, giving the EU citizen access to legislative and policy proposals in their own and also in other languages (Steinberger et al., 2014) . However, to transform such resources into assets for the citizen, linguistic tools are needed, that can automatically analyze textual sources and yield actionable information (Lippi et al., 2019a) .

Unfortunately, producing the annotated corpora required to train such linguistic tools is, even for a single language, notoriously expensive. Moreover, the vast majority of linguistic resources and tools focus on English. Likewise, the workforce of professionals needed for annotating legal documents may not be readily available in each language. We thus investigate methods for automatically transferring the annotations made on legal documents in a language with significant linguistic resources and domain experts, such as English, onto the corresponding versions of the same documents in a target language, where such resources and expertise may be lacking.

Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language.This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language. We chose to focus on the detection of unfair clauses in online Terms of Service (ToS) and Privacy Policies (PP), which are usually published in multiple languages. However, the domain of interest of our study spans across many other types of legal texts. Other EU official documents published in multiple languages include, for instance, EU Parliament laws and regulations, documents of the EU Court of Justice, policy documents, documents for public consultations, and so on.

Therefore, the potential import of effective methods for facilitating cross-lingual legal text analysis is considerable. Reasons for focusing on ToS and PP are the interest for such documents from a consumer protection perspective, especially since the recent adoption of the European General Data Protection Regulation (GDPR), as well as the availability of tools for the analysis of such documents. One such tool is CLAUDETTE (Lippi et al., 2019b) , a web server for the automatic detection of potentially unfair clauses in ToS. At the time of writing, CLAUDETTE is only available for the English language.

An effective method for cross-lingual annotation projection could extend its scope to a variety of languages other than English, without having to resort to domain expertise in any of these languages. To carry out the present study, we built the first English-German parallel corpus for the task at hand. It turned out that the corpus is asymmetric, meaning a one-to-one sentence-by-sentence correspondence between source and target document is not always guaranteed. This aspect makes the problem particularly challenging, since we need to ensure a correct behaviour when a sentence does not appear in one of the documents or it has been integrated in another sentence: a property of sentence alignment called robustness (Simard and Plamondon, 1998) .

For example, we found source-document sentences corresponding to multiple target-document sentences, as well as significant rephrasing, resulting in the same sentence being annotated differently in the two documents. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series. We evaluate performance with several combinations of dissimilarity scores, with or without dynamic time warping. Our results indicate that the best performance is achieved by a combination of word embedding-based dissimilarity and dynamic time warping.

The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic. Sections 2 and 3 formalize the problem and describe the architecture of our solution; 4 and 5 present corpus and experiments; 6 discusses related work in the areas of sentence alignment and annotation projection; 7 concludes.",Was the method tested with other language pairs besides English-German?,"Not in this paper, but we are currently testing two additional languages. Do you think this is an important problem?","Hi! Why is annotation projection useful? Hi! Because you can avoid re-constructing a corpus from scratch, but you can leverage the information coming from the same corpus in another language
 Facts: 
1. the workforce of professionals needed for annotating legal documents may not be readily available in each language
2. Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language
 Is this approach applicable only in legal texts? No, it could be applied to any domain whether you have different versions of the same document.
 Facts: 
1. This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language
 Can you tell me something more about how the method works? We exploit automatic translation tools and a distance between embeddings to find correspondences between the same document, available in to languages.
 Facts: 
1. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series
2. To carry out the present study, we built the first English-German parallel corpus for the task at hand
 Is the quality of the translation tool an issue?",[],train
Cross-lingual Annotation Projection in Legal Texts,"We study annotation projection in text classification problems where source documents are published in multiple languages and may not be an exact translation of one another. In particular, we focus on the detection of unfair clauses in privacy policies and terms of service. We present the first English-German parallel asymmetric corpus for the task at hand. We study and compare several language-agnostic sentence-level projection methods.

Our results indicate that a combination of word embeddings and dynamic time warping performs best. The European Union considers cultural and linguistic diversity, and in particular multilinguality, as some of its fundamental principles. So much so, that all the regulations and laws given by the Parliament are published in all of EU's twenty-four official languages. These could be an immense resource towards transparency, egalitarianism, accountability and democracy, giving the EU citizen access to legislative and policy proposals in their own and also in other languages (Steinberger et al., 2014) . However, to transform such resources into assets for the citizen, linguistic tools are needed, that can automatically analyze textual sources and yield actionable information (Lippi et al., 2019a) .

Unfortunately, producing the annotated corpora required to train such linguistic tools is, even for a single language, notoriously expensive. Moreover, the vast majority of linguistic resources and tools focus on English. Likewise, the workforce of professionals needed for annotating legal documents may not be readily available in each language. We thus investigate methods for automatically transferring the annotations made on legal documents in a language with significant linguistic resources and domain experts, such as English, onto the corresponding versions of the same documents in a target language, where such resources and expertise may be lacking.

Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language.This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language. We chose to focus on the detection of unfair clauses in online Terms of Service (ToS) and Privacy Policies (PP), which are usually published in multiple languages. However, the domain of interest of our study spans across many other types of legal texts. Other EU official documents published in multiple languages include, for instance, EU Parliament laws and regulations, documents of the EU Court of Justice, policy documents, documents for public consultations, and so on.

Therefore, the potential import of effective methods for facilitating cross-lingual legal text analysis is considerable. Reasons for focusing on ToS and PP are the interest for such documents from a consumer protection perspective, especially since the recent adoption of the European General Data Protection Regulation (GDPR), as well as the availability of tools for the analysis of such documents. One such tool is CLAUDETTE (Lippi et al., 2019b) , a web server for the automatic detection of potentially unfair clauses in ToS. At the time of writing, CLAUDETTE is only available for the English language.

An effective method for cross-lingual annotation projection could extend its scope to a variety of languages other than English, without having to resort to domain expertise in any of these languages. To carry out the present study, we built the first English-German parallel corpus for the task at hand. It turned out that the corpus is asymmetric, meaning a one-to-one sentence-by-sentence correspondence between source and target document is not always guaranteed. This aspect makes the problem particularly challenging, since we need to ensure a correct behaviour when a sentence does not appear in one of the documents or it has been integrated in another sentence: a property of sentence alignment called robustness (Simard and Plamondon, 1998) .

For example, we found source-document sentences corresponding to multiple target-document sentences, as well as significant rephrasing, resulting in the same sentence being annotated differently in the two documents. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series. We evaluate performance with several combinations of dissimilarity scores, with or without dynamic time warping. Our results indicate that the best performance is achieved by a combination of word embedding-based dissimilarity and dynamic time warping.

The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic. Sections 2 and 3 formalize the problem and describe the architecture of our solution; 4 and 5 present corpus and experiments; 6 discusses related work in the areas of sentence alignment and annotation projection; 7 concludes.",Yes I think the problem is very important. Thank you for the chat.,Thank you!,"Hi! Why is annotation projection useful? Hi! Because you can avoid re-constructing a corpus from scratch, but you can leverage the information coming from the same corpus in another language
 Facts: 
1. the workforce of professionals needed for annotating legal documents may not be readily available in each language
2. Our ultimate goal is to use automatically generated annotations for training linguistic tools for the target language without resorting to expert annotators in that language
 Is this approach applicable only in legal texts? No, it could be applied to any domain whether you have different versions of the same document.
 Facts: 
1. This would leverage the creation of classifiers that can leverage the linguistic resources available in the target language, to analyze documents in that language
 Can you tell me something more about how the method works? We exploit automatic translation tools and a distance between embeddings to find correspondences between the same document, available in to languages.
 Facts: 
1. The solution we propose makes use of an automated translation of the source document into the target document, obtained via a third-party tool, of a sentence-wise dissimilarity metric, and of a method for finding an alignment between warped time series
2. To carry out the present study, we built the first English-German parallel corpus for the task at hand
 Is the quality of the translation tool an issue? It might be, but this needs further investigation. For sure, the method is language-agnostic.
 Facts: 
1. The main novelty of our study lies in the confluence of three aspects: the projection is performed at sentence-level; the multi-lingual corpus is asymmetric; and the proposed methods are language-agnostic
 Was the method tested with other language pairs besides English-German?",[],train
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.",Hi! What is this paper about?,Hi! The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.,,"['The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. ', 'In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.']",val
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.",What architecture is used for question generation?,The exact architecture of the question generation model is not described in the Introduction.,Hi! What is this paper about?,[],val
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.",How about the metrics? How are results measured?,It is mentioned that their approach achieves a new state-of-the-art on SQuAD using the metrics METEOR and ROUGE-L. What do you think about their hypothesis? Does it make sense for you to generate the interrogative word independently?,"Hi! What is this paper about? Hi! The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.
 Facts: 
1. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. 
2. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.
 What architecture is used for question generation?","['The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.']",val
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.","I think it makes sense, and it also seems to work. Anything else worth mentioning about this paper?",I think that's all. They mentioned that their hypothesis makes the generation of the interrogative word easier since it's a classification task.,"Hi! What is this paper about? Hi! The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.
 Facts: 
1. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. 
2. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.
 What architecture is used for question generation? The exact architecture of the question generation model is not described in the Introduction. How about the metrics? How are results measured?",['the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model'],val
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.",Thank you for the clarification. The intuition makes sense. Bye,Thank you. Good bye,"Hi! What is this paper about? Hi! The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.
 Facts: 
1. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. 
2. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.
 What architecture is used for question generation? The exact architecture of the question generation model is not described in the Introduction. How about the metrics? How are results measured? It is mentioned that their approach achieves a new state-of-the-art on SQuAD using the metrics METEOR and ROUGE-L. What do you think about their hypothesis? Does it make sense for you to generate the interrogative word independently?
 Facts: 
1. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.
 I think it makes sense, and it also seems to work. Anything else worth mentioning about this paper?",[],val
Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning,"Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document (Lippi and Torroni, 2016) . The majority of AM systems follows a pipeline scheme, starting with simpler tasks, such as argument component detection, down to more complex tasks, such as argumentation structure prediction. Recent years have seen the development of a large number of techniques in this area, on the wake of the advancements produced by deep learning on the whole research field of natural language processing (NLP). Yet, it is widely recognized that the existing AM systems still have a large margin of improvement, as good results have been obtained with some genres where prior knowledge on the structure of the text eases some AM tasks, but other genres, such as legal cases and social media documents still require more work (Cabrio and Villata, 2018) .

Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine. Understanding whether a given piece of evidence supports a given claim, or whether two claims attack each other, are complex problems that humans can address thanks to their ability to exploit commonsense knowledge, and to perform reasoning and inference. Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi et al., 2018) communities.

This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015) . In computational argumentation, structured arguments have been studied and formalized for decades using models that can be expressed in a logic framework (Bench-Capon and Dunne, 2007) . At the same time, AM has rapidly evolved by exploiting state-of-the-art neural architectures coming from deep learning. So far, these two worlds have progressed largely independently of each other.

Only recently, a few works have taken some steps toward the integration of such methods, by applying techniques combining sub-symbolic classifiers with knowledge expressed in the form of rules and constraints to AM. For instance, Niculae et al. (2017) adopted structured support vector machines and recurrent neural networks to collectively classify argument components and their relations in short documents, by hardcoding contextual dependencies and constraints of the argument model in a factor graph. A joint inference approach for argument component classification and relation identification was instead proposed by Persing and Ng (2016) , following a pipeline scheme where integer linear programming is used to enforce mathematical constraints on the outcomes of a first-stage set of classifiers.

More recently, Cocarascu and Toni (2018) combined a deep network for relation extraction with an argumentative reasoning system that computes the dialectical strength of arguments, for the task of determining whether a review is truthful or deceptive. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process.","hello there, what exactly is the main contribution of the paper?",Hello! This is a position paper which proposes to use neuro-symbolic and relational learning techniques to address the tasks in argumentation mining,,"['we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal', 'Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks']",train
Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning,"Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document (Lippi and Torroni, 2016) . The majority of AM systems follows a pipeline scheme, starting with simpler tasks, such as argument component detection, down to more complex tasks, such as argumentation structure prediction. Recent years have seen the development of a large number of techniques in this area, on the wake of the advancements produced by deep learning on the whole research field of natural language processing (NLP). Yet, it is widely recognized that the existing AM systems still have a large margin of improvement, as good results have been obtained with some genres where prior knowledge on the structure of the text eases some AM tasks, but other genres, such as legal cases and social media documents still require more work (Cabrio and Villata, 2018) .

Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine. Understanding whether a given piece of evidence supports a given claim, or whether two claims attack each other, are complex problems that humans can address thanks to their ability to exploit commonsense knowledge, and to perform reasoning and inference. Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi et al., 2018) communities.

This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015) . In computational argumentation, structured arguments have been studied and formalized for decades using models that can be expressed in a logic framework (Bench-Capon and Dunne, 2007) . At the same time, AM has rapidly evolved by exploiting state-of-the-art neural architectures coming from deep learning. So far, these two worlds have progressed largely independently of each other.

Only recently, a few works have taken some steps toward the integration of such methods, by applying techniques combining sub-symbolic classifiers with knowledge expressed in the form of rules and constraints to AM. For instance, Niculae et al. (2017) adopted structured support vector machines and recurrent neural networks to collectively classify argument components and their relations in short documents, by hardcoding contextual dependencies and constraints of the argument model in a factor graph. A joint inference approach for argument component classification and relation identification was instead proposed by Persing and Ng (2016) , following a pipeline scheme where integer linear programming is used to enforce mathematical constraints on the outcomes of a first-stage set of classifiers.

More recently, Cocarascu and Toni (2018) combined a deep network for relation extraction with an argumentative reasoning system that computes the dialectical strength of arguments, for the task of determining whether a review is truthful or deceptive. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process.",do you have some sort of definition of argument mining to give me some background?,Sure! Argument mining is a research area in NLP which aims to extract arguments from unstructured text.,"hello there, what exactly is the main contribution of the paper?","['The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document', 'Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine']",train
Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning,"Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document (Lippi and Torroni, 2016) . The majority of AM systems follows a pipeline scheme, starting with simpler tasks, such as argument component detection, down to more complex tasks, such as argumentation structure prediction. Recent years have seen the development of a large number of techniques in this area, on the wake of the advancements produced by deep learning on the whole research field of natural language processing (NLP). Yet, it is widely recognized that the existing AM systems still have a large margin of improvement, as good results have been obtained with some genres where prior knowledge on the structure of the text eases some AM tasks, but other genres, such as legal cases and social media documents still require more work (Cabrio and Villata, 2018) .

Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine. Understanding whether a given piece of evidence supports a given claim, or whether two claims attack each other, are complex problems that humans can address thanks to their ability to exploit commonsense knowledge, and to perform reasoning and inference. Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi et al., 2018) communities.

This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015) . In computational argumentation, structured arguments have been studied and formalized for decades using models that can be expressed in a logic framework (Bench-Capon and Dunne, 2007) . At the same time, AM has rapidly evolved by exploiting state-of-the-art neural architectures coming from deep learning. So far, these two worlds have progressed largely independently of each other.

Only recently, a few works have taken some steps toward the integration of such methods, by applying techniques combining sub-symbolic classifiers with knowledge expressed in the form of rules and constraints to AM. For instance, Niculae et al. (2017) adopted structured support vector machines and recurrent neural networks to collectively classify argument components and their relations in short documents, by hardcoding contextual dependencies and constraints of the argument model in a factor graph. A joint inference approach for argument component classification and relation identification was instead proposed by Persing and Ng (2016) , following a pipeline scheme where integer linear programming is used to enforce mathematical constraints on the outcomes of a first-stage set of classifiers.

More recently, Cocarascu and Toni (2018) combined a deep network for relation extraction with an argumentative reasoning system that computes the dialectical strength of arguments, for the task of determining whether a review is truthful or deceptive. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process.",many thanks for that. i was wondering how previously mentioned techniques could be employed in the context of argument mining.,"Many argument graphs follow some typical rules (i.e., a premise supports a claim, but not the other way round). These rules could be easily encoded in some symbolic language, and used in combination with neural networks or other classifiers.","hello there, what exactly is the main contribution of the paper? Hello! This is a position paper which proposes to use neuro-symbolic and relational learning techniques to address the tasks in argumentation mining
 Facts: 
1. we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal
2. Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks
 do you have some sort of definition of argument mining to give me some background?","['We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi', 'We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks']",train
Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning,"Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document (Lippi and Torroni, 2016) . The majority of AM systems follows a pipeline scheme, starting with simpler tasks, such as argument component detection, down to more complex tasks, such as argumentation structure prediction. Recent years have seen the development of a large number of techniques in this area, on the wake of the advancements produced by deep learning on the whole research field of natural language processing (NLP). Yet, it is widely recognized that the existing AM systems still have a large margin of improvement, as good results have been obtained with some genres where prior knowledge on the structure of the text eases some AM tasks, but other genres, such as legal cases and social media documents still require more work (Cabrio and Villata, 2018) .

Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine. Understanding whether a given piece of evidence supports a given claim, or whether two claims attack each other, are complex problems that humans can address thanks to their ability to exploit commonsense knowledge, and to perform reasoning and inference. Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi et al., 2018) communities.

This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015) . In computational argumentation, structured arguments have been studied and formalized for decades using models that can be expressed in a logic framework (Bench-Capon and Dunne, 2007) . At the same time, AM has rapidly evolved by exploiting state-of-the-art neural architectures coming from deep learning. So far, these two worlds have progressed largely independently of each other.

Only recently, a few works have taken some steps toward the integration of such methods, by applying techniques combining sub-symbolic classifiers with knowledge expressed in the form of rules and constraints to AM. For instance, Niculae et al. (2017) adopted structured support vector machines and recurrent neural networks to collectively classify argument components and their relations in short documents, by hardcoding contextual dependencies and constraints of the argument model in a factor graph. A joint inference approach for argument component classification and relation identification was instead proposed by Persing and Ng (2016) , following a pipeline scheme where integer linear programming is used to enforce mathematical constraints on the outcomes of a first-stage set of classifiers.

More recently, Cocarascu and Toni (2018) combined a deep network for relation extraction with an argumentative reasoning system that computes the dialectical strength of arguments, for the task of determining whether a review is truthful or deceptive. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process.","therefore the idea is to encoded relations in a symbolic way and learn the importance of these relations along with the data-driven approach of deep learning, is that correct?","Yes, indeed. Do you think this might be an interesting research direction?","hello there, what exactly is the main contribution of the paper? Hello! This is a position paper which proposes to use neuro-symbolic and relational learning techniques to address the tasks in argumentation mining
 Facts: 
1. we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal
2. Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks
 do you have some sort of definition of argument mining to give me some background? Sure! Argument mining is a research area in NLP which aims to extract arguments from unstructured text.
 Facts: 
1. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document
2. Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine
 many thanks for that. i was wondering how previously mentioned techniques could be employed in the context of argument mining.","['Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers', 'Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process']",train
Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning,"Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document (Lippi and Torroni, 2016) . The majority of AM systems follows a pipeline scheme, starting with simpler tasks, such as argument component detection, down to more complex tasks, such as argumentation structure prediction. Recent years have seen the development of a large number of techniques in this area, on the wake of the advancements produced by deep learning on the whole research field of natural language processing (NLP). Yet, it is widely recognized that the existing AM systems still have a large margin of improvement, as good results have been obtained with some genres where prior knowledge on the structure of the text eases some AM tasks, but other genres, such as legal cases and social media documents still require more work (Cabrio and Villata, 2018) .

Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine. Understanding whether a given piece of evidence supports a given claim, or whether two claims attack each other, are complex problems that humans can address thanks to their ability to exploit commonsense knowledge, and to perform reasoning and inference. Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi et al., 2018) communities.

This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015) . In computational argumentation, structured arguments have been studied and formalized for decades using models that can be expressed in a logic framework (Bench-Capon and Dunne, 2007) . At the same time, AM has rapidly evolved by exploiting state-of-the-art neural architectures coming from deep learning. So far, these two worlds have progressed largely independently of each other.

Only recently, a few works have taken some steps toward the integration of such methods, by applying techniques combining sub-symbolic classifiers with knowledge expressed in the form of rules and constraints to AM. For instance, Niculae et al. (2017) adopted structured support vector machines and recurrent neural networks to collectively classify argument components and their relations in short documents, by hardcoding contextual dependencies and constraints of the argument model in a factor graph. A joint inference approach for argument component classification and relation identification was instead proposed by Persing and Ng (2016) , following a pipeline scheme where integer linear programming is used to enforce mathematical constraints on the outcomes of a first-stage set of classifiers.

More recently, Cocarascu and Toni (2018) combined a deep network for relation extraction with an argumentative reasoning system that computes the dialectical strength of arguments, for the task of determining whether a review is truthful or deceptive. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process.","this is indeed quite interesting, i think i've seen many frameworks in this direction. I was wondering if suggested approach (at least current draft) scales well with samples and the number of relations. This is indeed one crucial point to me","True: scalability is an issue in most neuro-symbolic applications. Unfortunately, we do not present experiments in this work, which is just a position paper.","hello there, what exactly is the main contribution of the paper? Hello! This is a position paper which proposes to use neuro-symbolic and relational learning techniques to address the tasks in argumentation mining
 Facts: 
1. we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal
2. Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks
 do you have some sort of definition of argument mining to give me some background? Sure! Argument mining is a research area in NLP which aims to extract arguments from unstructured text.
 Facts: 
1. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document
2. Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine
 many thanks for that. i was wondering how previously mentioned techniques could be employed in the context of argument mining. Many argument graphs follow some typical rules (i.e., a premise supports a claim, but not the other way round). These rules could be easily encoded in some symbolic language, and used in combination with neural networks or other classifiers.
 Facts: 
1. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi
2. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks
 therefore the idea is to encoded relations in a symbolic way and learn the importance of these relations along with the data-driven approach of deep learning, is that correct?","['In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal']",train
Neural-Symbolic Argumentation Mining: An Argument in Favor of Deep Learning and Reasoning,"Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks. In this position paper, we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document (Lippi and Torroni, 2016) . The majority of AM systems follows a pipeline scheme, starting with simpler tasks, such as argument component detection, down to more complex tasks, such as argumentation structure prediction. Recent years have seen the development of a large number of techniques in this area, on the wake of the advancements produced by deep learning on the whole research field of natural language processing (NLP). Yet, it is widely recognized that the existing AM systems still have a large margin of improvement, as good results have been obtained with some genres where prior knowledge on the structure of the text eases some AM tasks, but other genres, such as legal cases and social media documents still require more work (Cabrio and Villata, 2018) .

Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine. Understanding whether a given piece of evidence supports a given claim, or whether two claims attack each other, are complex problems that humans can address thanks to their ability to exploit commonsense knowledge, and to perform reasoning and inference. Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi et al., 2018) communities.

This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015) . In computational argumentation, structured arguments have been studied and formalized for decades using models that can be expressed in a logic framework (Bench-Capon and Dunne, 2007) . At the same time, AM has rapidly evolved by exploiting state-of-the-art neural architectures coming from deep learning. So far, these two worlds have progressed largely independently of each other.

Only recently, a few works have taken some steps toward the integration of such methods, by applying techniques combining sub-symbolic classifiers with knowledge expressed in the form of rules and constraints to AM. For instance, Niculae et al. (2017) adopted structured support vector machines and recurrent neural networks to collectively classify argument components and their relations in short documents, by hardcoding contextual dependencies and constraints of the argument model in a factor graph. A joint inference approach for argument component classification and relation identification was instead proposed by Persing and Ng (2016) , following a pipeline scheme where integer linear programming is used to enforce mathematical constraints on the outcomes of a first-stage set of classifiers.

More recently, Cocarascu and Toni (2018) combined a deep network for relation extraction with an argumentative reasoning system that computes the dialectical strength of arguments, for the task of determining whether a review is truthful or deceptive. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process.","i see, do you have any more detail that you repute important? ","It is worth mentioning that this goes into the direction widely recognized nowadays in AI of not relying just on deep learning for many tasks (even in NLP) but to introduce techniques that can exploit domain knowledge, and perform reasoning.","hello there, what exactly is the main contribution of the paper? Hello! This is a position paper which proposes to use neuro-symbolic and relational learning techniques to address the tasks in argumentation mining
 Facts: 
1. we posit that neural-symbolic and statistical relational learning could play a crucial role in the integration of symbolic and sub-symbolic methods to achieve this goal
2. Deep learning is bringing remarkable contributions to the field of argumentation mining, but the existing approaches still need to fill the gap toward performing advanced reasoning tasks
 do you have some sort of definition of argument mining to give me some background? Sure! Argument mining is a research area in NLP which aims to extract arguments from unstructured text.
 Facts: 
1. The goal of argumentation mining (AM) is to automatically extract arguments and their relations from a given document
2. Performing and understanding argumentation requires advanced reasoning capabilities, which are natural human skills, but are difficult to learn for a machine
 many thanks for that. i was wondering how previously mentioned techniques could be employed in the context of argument mining. Many argument graphs follow some typical rules (i.e., a premise supports a claim, but not the other way round). These rules could be easily encoded in some symbolic language, and used in combination with neural networks or other classifiers.
 Facts: 
1. We envisage that a significant advancement in AM could come from the combination of symbolic and sub-symbolic approaches, such as those developed in the Neural Symbolic (NeSy) (Garcez et al., 2015) or Statistical Relational Learning (SRL) (Getoor and Taskar, 2007; De Raedt et al., 2016; Kordjamshidi
2. We propose to exploit the potential of both symbolic and sub-symbolic approaches for AM, by combining both results in systems that are capable of modeling knowledge and constraints with a logic formalism, while maintaining the computational power of deep networks
 therefore the idea is to encoded relations in a symbolic way and learn the importance of these relations along with the data-driven approach of deep learning, is that correct? Yes, indeed. Do you think this might be an interesting research direction?
 Facts: 
1. Differently from existing approaches, we advocate the use of a logic-based language for the definition of contextual dependencies and constraints, independently of the structure of the underlying classifiers
2. Most importantly, the approaches we outline do not exploit a pipeline scheme, but rather perform joint detection of argument components and relations through a single learning process
 this is indeed quite interesting, i think i've seen many frameworks in this direction. I was wondering if suggested approach (at least current draft) scales well with samples and the number of relations. This is indeed one crucial point to me","['Despite the remarkable impact of deep neural networks in NLP, we argue that these techniques alone will not suffice to address such complex issues', 'This issue is also widely recognized as one of the major challenges for the whole field of artificial intelligence in the coming years (LeCun et al., 2015)']",train
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.","hi, is the paper about question generation? ",Hi! Yes! It is. They propose a pipelined composed of two parts: an interrogative word classifier and a question generation model,,"['In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.']",train
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.",Why is interrogative classifier used in this model?,That is a good question! They argue that classifying the interrogative word is easier that generating it along with the rest of the question. What do you think about this intuition? Does it make sense for you?,"hi, is the paper about question generation? ","['The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.', 'the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model']",train
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.","Yes, makes sense to me. What data set is used to train a classifier? ",They conduct experiments on SQuAD. Do you think SQuAD is a good benchmark for this?,"hi, is the paper about question generation?  Hi! Yes! It is. They propose a pipelined composed of two parts: an interrogative word classifier and a question generation model
 Facts: 
1. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.
 Why is interrogative classifier used in this model?","['The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.']",train
Let Me Know What to Ask: Interrogative-Word-Aware Question Generation,"Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants. Existing models focus on generating a question based on a text and possibly the answer to the generated question. They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.

The first module predicts the interrogative word that is provided to the second module to create the question. Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language. This is an important task for Question Answering (QA) since it can help create QA datasets. It is also useful for conversational systems like Amazon Alexa. Due to the surge of interests in these systems, QG is also drawing the attention of the research community.

One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) . Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution. ingful QG can play a key role in the advances of QA (Lewis et al., 2019) . QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.

This task is considered to have two parts: what to ask and how to ask. The first one refers to the identification of relevant portions of the text to ask about. This requires machine reading comprehension since the system has to understand the text. The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.

Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) . There are different settings for QG. Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created. Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .

We assume this setting and consider that the answer is a span of the passage, as in SQuAD. Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance. We posit that the interrogative word must be predicted by a well-trained classifier.

We consider that selecting the right interrogative word is the key to generate high-quality questions. For example, a question with a wrong interrogative word for the answer ""the owner"" is: ""what produces a list of requirements for a project?"". However, with the right interrogative word, who, the question would be: ""who produces a list of requirements for a project? "", which is clear that is more adequate regarding the answer than the first one.

According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder. With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word. Figure 1 shows a highlevel overview of our approach. The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.","As long as I know, SQuAD is a question-answering dataset. I'm not sure if this dataset is suitable for question word clarification. What do you think?","It is not clarification, but prediction. Since SQuAD includes questions, answers, and context, it would be possible to use the answers to create questions for them.","hi, is the paper about question generation?  Hi! Yes! It is. They propose a pipelined composed of two parts: an interrogative word classifier and a question generation model
 Facts: 
1. In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.
 Why is interrogative classifier used in this model? That is a good question! They argue that classifying the interrogative word is easier that generating it along with the rest of the question. What do you think about this intuition? Does it make sense for you?
 Facts: 
1. The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.
2. the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model
 Yes, makes sense to me. What data set is used to train a classifier? ","['We assume this setting and consider that the answer is a span of the passage, as in SQuAD.']",train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.","Hi, what exactly is meant by lexical inference?","Hi, lexical inference means that the inference is focused on lexical semantics.",,"['Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression.', 'It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics.']",train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.","Ok, so the labels are the classical entailment labels (entailment, neutral, contradiction)?",That is a good question. Let me clarify: It is a binary classification task. So the labels are entailment vs. non-entailment.,"Hi, what exactly is meant by lexical inference?",['Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items.'],train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.","Ok, so do they build a dataset for that?","No, they propose new models based on pretrained language models for the task. They evaluate them on two existing benchmarks.","Hi, what exactly is meant by lexical inference? Hi, lexical inference means that the inference is focused on lexical semantics.
 Facts: 
1. Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression.
2. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics.
 Ok, so the labels are the classical entailment labels (entailment, neutral, contradiction)?","['We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task', 'our experiments on two established LIiC benchmarks']",train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.",What makes their new models special?,The task inputs are fed to the language models in a context of textual patterns corresponding the two classes of the task. This outperforms approaches that do not use these patterns. What do you think about this pattern-based approach?,"Hi, what exactly is meant by lexical inference? Hi, lexical inference means that the inference is focused on lexical semantics.
 Facts: 
1. Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression.
2. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics.
 Ok, so the labels are the classical entailment labels (entailment, neutral, contradiction)? That is a good question. Let me clarify: It is a binary classification task. So the labels are entailment vs. non-entailment.
 Facts: 
1. Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items.
 Ok, so do they build a dataset for that?","['We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC.', 'all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC.']",train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.","Before I can answer your question, I need to clarify something. The patterns are a form of ""prompts""? Like additional tokens prepended or appended to the input?","Yes, this is like ""prompts"". The additional tokens can be before the two sentences, between them, after them, or a combination of the three.","Hi, what exactly is meant by lexical inference? Hi, lexical inference means that the inference is focused on lexical semantics.
 Facts: 
1. Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression.
2. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics.
 Ok, so the labels are the classical entailment labels (entailment, neutral, contradiction)? That is a good question. Let me clarify: It is a binary classification task. So the labels are entailment vs. non-entailment.
 Facts: 
1. Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items.
 Ok, so do they build a dataset for that? No, they propose new models based on pretrained language models for the task. They evaluate them on two existing benchmarks.
 Facts: 
1. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task
2. our experiments on two established LIiC benchmarks
 What makes their new models special?",['a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference'],train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.",Ah ok. It is interesting that such simple techniques help. This is not a few-shot setting right? So the model has full access to the training data?,There are no large-scale training set for the LIiC task.,"Hi, what exactly is meant by lexical inference? Hi, lexical inference means that the inference is focused on lexical semantics.
 Facts: 
1. Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression.
2. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics.
 Ok, so the labels are the classical entailment labels (entailment, neutral, contradiction)? That is a good question. Let me clarify: It is a binary classification task. So the labels are entailment vs. non-entailment.
 Facts: 
1. Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items.
 Ok, so do they build a dataset for that? No, they propose new models based on pretrained language models for the task. They evaluate them on two existing benchmarks.
 Facts: 
1. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task
2. our experiments on two established LIiC benchmarks
 What makes their new models special? The task inputs are fed to the language models in a context of textual patterns corresponding the two classes of the task. This outperforms approaches that do not use these patterns. What do you think about this pattern-based approach?
 Facts: 
1. We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC.
2. all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC.
 Before I can answer your question, I need to clarify something. The patterns are a form of ""prompts""? Like additional tokens prepended or appended to the input?","['supervised NLI models need large amounts of training data, which is unavailable for LIiC']",train
Language Models for Lexical Inference in Context,"Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC.

In an extensive analysis, we investigate factors of success and failure of our three approaches. 1 Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items. It is therefore related to the detection of other lexical relations like hyponymy between nouns (Hearst, 1992) , e.g., dog ⇒ animal, or troponymy between verbs (Fellbaum and Miller, 1990) , e.g., to traipse ⇒ to walk. Lexical inference in context (LIiC) adds the problem of disambiguating the pair of lexical items in a given context before reasoning about the inference question. This type of LI is particularly interesting for entailments between verbs and verbal expressions because their meaning -and therefore their implications -can drastically change with different arguments.

Consider, e.g., run ⇒ lead in a PERSON / COMPANY context (""Bezos runs Amazon"") vs. run ⇒ execute in a COMPUTER / SOFTWARE context (""My mac runs macOS""). LIiC is thus also closely related to the task of natural language inference (NLI) -also called recognizing textual entailment (Dagan et al., 2013) -and can be seen as a focused variant of it. Besides the important use case of evaluating NLI systems, this kind of predicate entailment has also been shown useful for question answering (Schoenmackers et al., 2010) , event coreference (Shwartz et al., 2017; Meged et al., 2020) , and link prediction in knowledge graphs (Hosseini et al., 2019) . Despite its NLI nature, previous systems for LIiC have primarily been models of lexical similarity (Levy and Dagan, 2016) or models based on verb argument inclusion (Hosseini et al., 2019) .

The reason is probably that supervised NLI models need large amounts of training data, which is unavailable for LIiC, and that systems trained on available large-scale NLI benchmarks (e.g., Williams et al., 2018) have been reported to insufficiently cover lexical phenomena (Glockner et al., 2018; Schmitt and Schütze, 2019) . Recently, transfer learning has become ubiquitous in NLP; Transformer (Vaswani et al., 2017) language models (LMs) pretrained on large amounts of textual data (? Liu et al., 2019) form the basis of a lot of current state-of-the-art models. Besides zero-and few-shot capabilities (Radford et al., 2019; Brown et al., 2020) , pretrained LMs have also been found to acquire factual and relational knowledge during pretraining (Petroni et al., 2019; Bouraoui et al., 2020) .

The entailment relation certainly stands out among previously explored semantic relations -such as the relation between a country and its capital -because it is very rarely stated explicitly and often involves reasoning about both the meaning of verbs and additional knowledge (Schmitt and Schütze, 2019) . It is unclear whether implicit clues during pretraining are enough to learn about LIiC and what the best way is to harness any such implicit knowledge. Regarding these questions, we make the follow-ing contributions: (1) This work is the first to explore the use of pretrained LMs for the LIiC task. (2) We formulate three approaches and evaluate them using the publicly available pretrained RoBERTa LM (Liu et al., 2019; Wolf et al., 2019) :

 (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus.

(3) We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC. (4) In our experiments on two established LIiC benchmarks, Levy/Holt's dataset (Levy and Dagan, 2016; and SherLIiC (Schmitt and Schütze, 2019) , all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC. (5) In contrast to previous work on relation induction (Bouraoui et al., 2020) , automatically retrieved patterns do not outperform handcrafted ones for LIiC. A qualitative analysis of patterns and errors identifies possible reasons for this finding.","I see. Thanks for the interesting conversation, I am terminating now.",Thank you for your interest!,"Hi, what exactly is meant by lexical inference? Hi, lexical inference means that the inference is focused on lexical semantics.
 Facts: 
1. Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression.
2. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics.
 Ok, so the labels are the classical entailment labels (entailment, neutral, contradiction)? That is a good question. Let me clarify: It is a binary classification task. So the labels are entailment vs. non-entailment.
 Facts: 
1. Lexical inference (LI) denotes the task of deciding whether or not an entailment relation holds between two lexical items.
 Ok, so do they build a dataset for that? No, they propose new models based on pretrained language models for the task. They evaluate them on two existing benchmarks.
 Facts: 
1. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task
2. our experiments on two established LIiC benchmarks
 What makes their new models special? The task inputs are fed to the language models in a context of textual patterns corresponding the two classes of the task. This outperforms approaches that do not use these patterns. What do you think about this pattern-based approach?
 Facts: 
1. We introduce the concept of antipatterns, patterns that express non-entailment, and evaluate their usefulness for LIiC.
2. all our approaches consistently outperform previous work, thus setting a new state of the art for LIiC.
 Before I can answer your question, I need to clarify something. The patterns are a form of ""prompts""? Like additional tokens prepended or appended to the input? Yes, this is like ""prompts"". The additional tokens can be before the two sentences, between them, after them, or a combination of the three.
 Facts: 
1. a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference
 Ah ok. It is interesting that such simple techniques help. This is not a few-shot setting right? So the model has full access to the training data?",[],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Hi, what is the paper about?","Hi, the paper is about investigating strategies that implicitly train annotators during their annotations.",,"['To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators.']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","That is interesting, how are the annotators then training during the annotation?",The basic idea comes from curriculum learning that aims to provide exercises to a (human) learner in an ordering that matches their learning proficiency. Our goal is to implement such a learning curriculum for annotation studies by assessing the difficulty of annotated instances.,"Hi, what is the paper about?","[""The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 )."", 'Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.",So the difficulty of their annotations increment over time. Do you think this comes with an increase in total annotation time for the coder?,"Sorry, I am not sure if I follow the question. By coder, do you mean the person setting up the study? If you mean the annotator; we restricted our experiments to a finite set of instances that is annotated. So the total annotation time for annotating these instances does actually decrease with a good curriculum.","Hi, what is the paper about? Hi, the paper is about investigating strategies that implicitly train annotators during their annotations.
 Facts: 
1. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators.
 That is interesting, how are the annotators then training during the annotation?",['The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality.'],train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.","Ok, that's what I wanted to know. Another point I would be interested is how do you assess the difficulty of a sample? Like how do you determine the order of data you show the annotator?","To estimate the difficulty, we investigate several methods. Non-adaptive ones like heuristics (e.g., sentence length) and adaptive ones that are models that are trained interactively to predict the annotation time (we use this as a proxy for difficulty). Finally, in this work we only investigate easy-instances-first strategies, so the annotated instances are sorted in ascending difficulty.","Hi, what is the paper about? Hi, the paper is about investigating strategies that implicitly train annotators during their annotations.
 Facts: 
1. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators.
 That is interesting, how are the annotators then training during the annotation? The basic idea comes from curriculum learning that aims to provide exercises to a (human) learner in an ordering that matches their learning proficiency. Our goal is to implement such a learning curriculum for annotation studies by assessing the difficulty of annotated instances.
 Facts: 
1. The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ).
2. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.
 So the difficulty of their annotations increment over time. Do you think this comes with an increase in total annotation time for the coder?","['evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.', 'We instantiate AC with an ""easy-instancesfirst"" strategy ']",train
Annotation Curricula to Implicitly Train Non-Expert Annotators,"Annotation studies often require annotators to familiarize themselves with the task, its annotation scheme, and the data domain. This can be overwhelming in the beginning, mentally taxing, and induce errors into the resulting annotations; especially in citizen science or crowd sourcing scenarios where domain expertise is not required and only annotation guidelines are provided. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators. The goal is to gradually introduce annotators into the task by ordering instances that are annotated according to a learning curriculum.

To do so, this work formalizes annotation curricula for sentence-and paragraph-level annotation tasks, defines an ordering strategy, and identifies well-performing heuristics and interactively trained models on three existing English datasets. Finally, a user study is conducted with 40 voluntary participants who are asked to identify the most fitting misconception for English tweets about the Covid-19 pandemic. The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality. Annotation curricula thus can provide a novel way to improve data collection.

To facilitate future research, all code and data from the user study consisting of 2,400 annotations is made available. 1 Supervised learning and, consequently, annotated corpora are crucial for many downstream tasks to train and develop well-performing models. Despite increasing performances of models trained in a semi-or unsupervised fashion (Peters et al. 2018; Devlin et al. 2019) , they still substantially benefit from labeled data (Peters, Ruder, and Smith 2019; Gururangan et al.

2020 ). However, labels are costly to obtain and require domain experts or a large crowd of non-expert annotators (Snow et al. 2008) . Past research has mainly investigated two approaches to reduce annotation cost and effort (often approximated by annotation time); namely, active learning and label suggestions.

Active learning assumes that resources for annotating data are limited and aims to reduce the number of labeled instances by only annotating those which contribute most to model training (Lewis and Gale 1994; Settles 2012 ). This often results in sampled instances that are more difficult to annotate, putting an increased cognitive load on annotators, and potentially leading to a lower agreement or an increased annotation time (Settles, Craven, and Friedland 2008) . Label suggestions directly target annotators by providing them with suggestions from a pre-trained model. Although they are capable of effectively reducing the annotation time (Schulz et al.

2019; Klie, Eckart de Castilho, and Gurevych 2020) , they bear the risk of biasing annotators towards the (possibly erroneous) suggested label (Fort and Sagot 2010) . The above-called shortcomings render existing approaches better suited for domain-expert annotators who are less burdened by difficult annotation instances and are less prone to receiving erroneous label suggestions than non-expert annotators. Overall, we can identify a lack of approaches that (1) are less distracting or biasing than label suggestions and (2) focus on easing the annotation process for non-expert annotators. At the same time, the rising popularity of large-scale datasets that are annotated by non-expert annotators (""Bowman et al.

2015; Sakaguchi et al. 2020) , further increases the need for better training methods targeting non-expert annotators (Geva, Goldberg, and Berant 2019; Nie et al. 2020; Rogers 2021) . One key element that has so far not been investigated in annotation studies is the use of a curriculum to implicitly teach the task to annotators during annotation.

The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ). Moreover, Kelly (2009) show that such learning curricula can also be used to teach learners implicitly. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.

We conjecture that a good annotation curriculum can implicitly teach the task to annotators -for instance, by showing easier annotation instances before more difficult ones -consequently reducing the cognitive strain and improving annotation speed and quality. In contrast to active learning that may result in only sampling instances that are difficult to annotate, they explicitly emphasize the needs of a human annotator and gradually familiarize them with the annotation task. Compared to label suggestions, they are less distracting as they do not bear the risk of providing erroneous suggestions from imperfect models which makes them well-suited for non-expert annotation scenarios. Furthermore, AC does not require study conductors to adapt existing annotator training processes or annotation guidelines and hence, can complement their annotation project.

To provide a first assessment for the viability of such annotation curricula, we investigate the following three research questions: RQ1. Does the order in which instances are annotated impact the annotations in terms of annotation time and quality? RQ2. Do traditional heuristics and recent methods for assessing the reading difficulty already suffice to generate curricula that improve annotation time or quality?

RQ3. Can the generation of annotation curricula be further alleviated by interactively trained models? We first identify and formalize two essential parts to deploy AC: (1) a ""strategy"" that defines how instances should be ordered (e.g., by annotation difficulty) and (2) an ""estimator"" that ranks them accordingly. We instantiate AC with an ""easy-instancesfirst"" strategy and evaluate heuristic and interactively trained estimators on three English datasets that provide annotation time which we use as an approximation of the annotation difficulty for evaluation.

Finally, we apply our strategy and its best estimators in a carefully designed user study with 40 participants for annotating English tweets about the Covid-19 pandemic. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality. Our contributions are: C1. A novel approach for training non-expert annotators that is easy to implement and is complementary to existing annotator training approaches.",How do the non-adaptive measures perform compared to the adaptive ones?,"Overall, we find that the heuristic ones perform quite well. In fact, our best performing estimator was using the BERT mlm loss (this is not in the displayed text, but an analysis is provided in the results section). ","Hi, what is the paper about? Hi, the paper is about investigating strategies that implicitly train annotators during their annotations.
 Facts: 
1. To alleviate these issues, this work proposes annotation curricula, a novel approach to implicitly train annotators.
 That is interesting, how are the annotators then training during the annotation? The basic idea comes from curriculum learning that aims to provide exercises to a (human) learner in an ordering that matches their learning proficiency. Our goal is to implement such a learning curriculum for annotation studies by assessing the difficulty of annotated instances.
 Facts: 
1. The learning curriculum is a fundamental concept in educational research that proposes to order exercises to match a learner's proficiency (Vygotsky 1978; Krashen 1982) and has even motivated training strategies for machine learning models (Bengio et al. 2009 ).
2. Similarly, the goal of annotation curricula (AC) is to provide an ordering of instances during annotation that is optimized for learning the task.
 So the difficulty of their annotations increment over time. Do you think this comes with an increase in total annotation time for the coder? Sorry, I am not sure if I follow the question. By coder, do you mean the person setting up the study? If you mean the annotator; we restricted our experiments to a finite set of instances that is annotated. So the total annotation time for annotating these instances does actually decrease with a good curriculum.
 Facts: 
1. The results show that annotators who receive the same instances in an optimized order require significantly less annotation time while retaining a high annotation quality.
 Ok, that's what I wanted to know. Another point I would be interested is how do you assess the difficulty of a sample? Like how do you determine the order of data you show the annotator?",['The results show that using a simple heuristic to order instances can already significantly reduce the total annotation time while preserving a high annotation quality.'],train
