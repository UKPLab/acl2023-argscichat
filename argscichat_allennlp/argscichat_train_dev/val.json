{
    "Analyzing Political Parody in Social Media": {
        "id": "Analyzing Political Parody in Social Media",
        "title": "Analyzing Political Parody in Social Media",
        "content": [
            "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.",
            "In this paper, we present the first computational study of parody.",
            "We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.",
            "We run a battery of supervised machine learning models for automatically detecting parody tweets with an emphasis on robustness by testing on tweets from accounts unseen in training, across different genders and across countries.",
            "Our results show that political parody tweets can be predicted with an accuracy up to 90%.",
            "Finally, we identify the markers of parody through a linguistic analysis.",
            "Beyond research in linguistics and political communication, accurately and automatically detecting parody is important to improving fact checking for journalists and analytics such as sentiment analysis through filtering out parodical utterances.",
            "1 * Equal contribution.",
            "\u2020 Work was done while at the University of Sheffield.",
            "1 Data is available here: https://archive.org/de tails/parody data acl202 The 'Kapou Opa' column by K. Maniatis parodying Greek popular persons was a source of inspiration for this workhttps://www.oneman.gr/originals/to -imerologio-karantinas-tou-dimitri-kouts oumpa/ Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) .",
            "Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person.",
            "2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures.",
            "Highfield (2016) defines parody accounts acting as: a known, real person, for obviously comedic purposes.",
            "There should be no risk of mistaking their tweets for their subject's actual views; these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona.",
            "A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas (Hariman, 2008) .",
            "Table 1 shows examples of very popular (over 50k followers) and active (thousands of tweets sent) political parody accounts on Twitter.",
            "Sample tweets show how the style and topic of parody tweets are similar to those from the real accounts, which may pose issues to automatic classification.",
            "While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods.",
            "In this paper, we aim to bridge this gap and conduct, for the first time, a systematic study of political parody as a figurative device in social media.",
            "To this end, we make the following contributions: 1.",
            "A novel classification task where we seek to automatically classify real and parody tweets.",
            "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries (Section 3); 2.",
            "Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.",
            "These are focused on the robust-ness of classification, with test data from: a) users; b) genders; c) locations; unseen in training (Section 5); 3.",
            "Linguistic analysis of the markers of parody tweets and of the model errors (Section 6).",
            "We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond.",
            "Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody 3 and the poster does not have the intention to mislead.",
            "For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet; 4 and many users were fooled by a Donald Trump parody tweet about 'Dow Joans'.",
            "5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019) , sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018) .",
            "Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) ; (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018) ."
        ],
        "message_pairs": [
            {
                "id": "Analyzing Political Parody in Social Media_38",
                "P_Message": "Hi",
                "DE_Message": "Hi!",
                "history": [],
                "facts": []
            },
            {
                "id": "Analyzing Political Parody in Social Media_39",
                "P_Message": "Is this work about natural language processing or social sciences?",
                "DE_Message": "This is work about natural language processing.",
                "history": [
                    "Hi"
                ],
                "facts": [
                    "In this paper, we present the first computational study of parody."
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_40",
                "P_Message": "How is the study conducted?",
                "DE_Message": "First, we are collecting data from Twitter. Then, we perform on analysis of the data. Finally, we train models on our data to predict whether a tweet is parody or not.",
                "history": [
                    "Hi Hi!",
                    "Is this work about natural language processing or social sciences?"
                ],
                "facts": [
                    "We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.",
                    "We run a battery of supervised machine learning models for automatically detecting parody tweets",
                    "Finally, we identify the markers of parody through a linguistic analysis."
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_41",
                "P_Message": "How many annotators were involved in corpus creation?",
                "DE_Message": "The corpus was created by automatically retrieving tweets from parody accounts. Parody accounts were identified by the authors of this work. In total, 4 researchers took a look at the data.",
                "history": [
                    "Hi Hi!",
                    "Is this work about natural language processing or social sciences?",
                    "This is work about natural language processing.",
                    "Facts: \n1.",
                    "In this paper, we present the first computational study of parody.",
                    "How is the study conducted?"
                ],
                "facts": [
                    "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries"
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_42",
                "P_Message": "Interesting work. What features are more indicative of parody?",
                "DE_Message": "In our work we examine political parody. In political parody, exaggerated views and caricatures are the main features. Posts are also made to mimic real facts, but with a humorous twist.",
                "history": [
                    "Hi Hi!",
                    "Is this work about natural language processing or social sciences?",
                    "This is work about natural language processing.",
                    "Facts: \n1.",
                    "In this paper, we present the first computational study of parody.",
                    "How is the study conducted?",
                    "First, we are collecting data from Twitter.",
                    "Then, we perform on analysis of the data.",
                    "Finally, we train models on our data to predict whether a tweet is parody or not.",
                    "Facts: \n1.",
                    "We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.",
                    "We run a battery of supervised machine learning models for automatically detecting parody tweets\n2.",
                    "Finally, we identify the markers of parody through a linguistic analysis.",
                    "How many annotators were involved in corpus creation?"
                ],
                "facts": [
                    "Parody tweets can often be misinterpreted as facts",
                    "these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona.",
                    "A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas"
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_43",
                "P_Message": "There may be some links to other NLP tasks, like irony detection. What do you think?",
                "DE_Message": "I agree with this statement. Irony can be used to make a parody post, so detecting irony could facilitate better parody detection as well.",
                "history": [
                    "Hi Hi!",
                    "Is this work about natural language processing or social sciences?",
                    "This is work about natural language processing.",
                    "Facts: \n1.",
                    "In this paper, we present the first computational study of parody.",
                    "How is the study conducted?",
                    "First, we are collecting data from Twitter.",
                    "Then, we perform on analysis of the data.",
                    "Finally, we train models on our data to predict whether a tweet is parody or not.",
                    "Facts: \n1.",
                    "We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.",
                    "We run a battery of supervised machine learning models for automatically detecting parody tweets\n2.",
                    "Finally, we identify the markers of parody through a linguistic analysis.",
                    "How many annotators were involved in corpus creation?",
                    "The corpus was created by automatically retrieving tweets from parody accounts.",
                    "Parody accounts were identified by the authors of this work.",
                    "In total, 4 researchers took a look at the data.",
                    "Facts: \n1.",
                    "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries\n Interesting work.",
                    "What features are more indicative of parody?"
                ],
                "facts": [
                    "While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods.",
                    "We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond."
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_44",
                "P_Message": "How about fact checking?",
                "DE_Message": "While fact checking could definitely help, I believe we would not get enough use out of fact checking. Verifying facts can be very challenging and gains would be minimal. Furthermore, while parody can indeed be misinterpreted as facts, it mostly relies on the style of writing (ie. mimicking the real politician) and not so much the content.",
                "history": [
                    "Hi Hi!",
                    "Is this work about natural language processing or social sciences?",
                    "This is work about natural language processing.",
                    "Facts: \n1.",
                    "In this paper, we present the first computational study of parody.",
                    "How is the study conducted?",
                    "First, we are collecting data from Twitter.",
                    "Then, we perform on analysis of the data.",
                    "Finally, we train models on our data to predict whether a tweet is parody or not.",
                    "Facts: \n1.",
                    "We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.",
                    "We run a battery of supervised machine learning models for automatically detecting parody tweets\n2.",
                    "Finally, we identify the markers of parody through a linguistic analysis.",
                    "How many annotators were involved in corpus creation?",
                    "The corpus was created by automatically retrieving tweets from parody accounts.",
                    "Parody accounts were identified by the authors of this work.",
                    "In total, 4 researchers took a look at the data.",
                    "Facts: \n1.",
                    "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries\n Interesting work.",
                    "What features are more indicative of parody?",
                    "In our work we examine political parody.",
                    "In political parody, exaggerated views and caricatures are the main features.",
                    "Posts are also made to mimic real facts, but with a humorous twist.",
                    "Facts: \n1.",
                    "Parody tweets can often be misinterpreted as facts\n2. these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona.",
                    "A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas\n There may be some links to other NLP tasks, like irony detection.",
                    "What do you think?"
                ],
                "facts": [
                    "Parody tweets can often be misinterpreted as facts"
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_45",
                "P_Message": "Great work. Thanks for the chat",
                "DE_Message": "Thank you, have a nice day!",
                "history": [
                    "Hi Hi!",
                    "Is this work about natural language processing or social sciences?",
                    "This is work about natural language processing.",
                    "Facts: \n1.",
                    "In this paper, we present the first computational study of parody.",
                    "How is the study conducted?",
                    "First, we are collecting data from Twitter.",
                    "Then, we perform on analysis of the data.",
                    "Finally, we train models on our data to predict whether a tweet is parody or not.",
                    "Facts: \n1.",
                    "We introduce a new publicly available data set of tweets from real politicians and their corresponding parody accounts.",
                    "We run a battery of supervised machine learning models for automatically detecting parody tweets\n2.",
                    "Finally, we identify the markers of parody through a linguistic analysis.",
                    "How many annotators were involved in corpus creation?",
                    "The corpus was created by automatically retrieving tweets from parody accounts.",
                    "Parody accounts were identified by the authors of this work.",
                    "In total, 4 researchers took a look at the data.",
                    "Facts: \n1.",
                    "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries\n Interesting work.",
                    "What features are more indicative of parody?",
                    "In our work we examine political parody.",
                    "In political parody, exaggerated views and caricatures are the main features.",
                    "Posts are also made to mimic real facts, but with a humorous twist.",
                    "Facts: \n1.",
                    "Parody tweets can often be misinterpreted as facts\n2. these accounts play with stereotypes of these figures or juxtapose their public image with a very different, behind-closed-doors persona.",
                    "A very popular type of parody is political parody which plays an important role in public speech by offering irreverent interpretations of political personas\n There may be some links to other NLP tasks, like irony detection.",
                    "What do you think?",
                    "I agree with this statement.",
                    "Irony can be used to make a parody post, so detecting irony could facilitate better parody detection as well.",
                    "Facts: \n1.",
                    "While closely related figurative devices such as irony and sarcasm have been extensively studied in computational linguistics (Wallace, 2015; Joshi et al., 2017) , parody yet to be explored using computational methods.",
                    "2.",
                    "We argue that understanding the expression and use of parody in natural language and automatically identifying it are important to applications in computational social science and beyond.",
                    "How about fact checking?"
                ],
                "facts": []
            },
            {
                "id": "Analyzing Political Parody in Social Media_46",
                "P_Message": "Hi, how is political parody defined the paper?",
                "DE_Message": "Parody is defined as a means to mimic behavior of a target for comedic purposes.",
                "history": [],
                "facts": [
                    "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.",
                    "Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) .",
                    "Traditional forms of parody include editorial cartoons"
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_47",
                "P_Message": "Can it be detected automatically in social media posts?",
                "DE_Message": "It can! In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.",
                "history": [
                    "Hi, how is political parody defined the paper?"
                ],
                "facts": [
                    "Our results show that political parody tweets can be predicted with an accuracy up to 90%",
                    "Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1."
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_48",
                "P_Message": "Can the model distinguish political parody from other forms of parody? The definition seemed to allow for non-political parody, too.",
                "DE_Message": "Here we only collected political parody data, so the models only learn to identify data in that domain. We have not investigated whether the same model could detect general parody. I believe that while it is possible to detect non-political parody, more general data would be needed for that task.",
                "history": [
                    "Hi, how is political parody defined the paper?",
                    "Parody is defined as a means to mimic behavior of a target for comedic purposes.",
                    "Facts: \n1.",
                    "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.",
                    "2.",
                    "Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) .",
                    "Traditional forms of parody include editorial cartoons\n Can it be detected automatically in social media posts?"
                ],
                "facts": [
                    "Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Rober"
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_49",
                "P_Message": "I see. So the model distinguishes political parody from other posts without any parody, yes? Is it possible that the model only detects the political topic without learning about parody at all?",
                "DE_Message": "The data is all in the political domain. Data comes from politician accounts as well as political parody accounts. So, the model operates entirely within the political domain, and learns to separate between parody and non-parody data.",
                "history": [
                    "Hi, how is political parody defined the paper?",
                    "Parody is defined as a means to mimic behavior of a target for comedic purposes.",
                    "Facts: \n1.",
                    "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.",
                    "2.",
                    "Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) .",
                    "Traditional forms of parody include editorial cartoons\n Can it be detected automatically in social media posts?",
                    "It can!",
                    "In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.",
                    "Facts: \n1.",
                    "Our results show that political parody tweets can be predicted with an accuracy up to 90%\n2.",
                    "Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.",
                    "Can the model distinguish political parody from other forms of parody?",
                    "The definition seemed to allow for non-political parody, too."
                ],
                "facts": [
                    "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries"
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_50",
                "P_Message": "That is very interesting. The resource sounds very useful. Are there annotations on the tweet-level or do you assume that a parody account only tweets parody and the others don't? Could the model be fooled into an author identification task?",
                "DE_Message": "Annotations are on the account level. That is, a parody account only tweets parody, while a real account only tweets normal speech. I do not believe this task will be reduced to author identification though. In our experiments, we also predict parody from previously unseen accounts (both parody and real) with the model performing well on those. This goes to show that the model is able to identify parody to an adequate level.)",
                "history": [
                    "Hi, how is political parody defined the paper?",
                    "Parody is defined as a means to mimic behavior of a target for comedic purposes.",
                    "Facts: \n1.",
                    "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.",
                    "2.",
                    "Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) .",
                    "Traditional forms of parody include editorial cartoons\n Can it be detected automatically in social media posts?",
                    "It can!",
                    "In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.",
                    "Facts: \n1.",
                    "Our results show that political parody tweets can be predicted with an accuracy up to 90%\n2.",
                    "Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.",
                    "Can the model distinguish political parody from other forms of parody?",
                    "The definition seemed to allow for non-political parody, too.",
                    "Here we only collected political parody data, so the models only learn to identify data in that domain.",
                    "We have not investigated whether the same model could detect general parody.",
                    "I believe that while it is possible to detect non-political parody, more general data would be needed for that task.",
                    "Facts: \n1.",
                    "Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Rober\n I see.",
                    "So the model distinguishes political parody from other posts without any parody, yes?",
                    "Is it possible that the model only detects the political topic without learning about parody at all?"
                ],
                "facts": [
                    "test data from: a) users; b) genders; c) locations; unseen in training",
                    "Traditional forms of parody include editorial cartoons, sketches or articles pretending to have been authored by the parodied person.",
                    "2 A new form of parody recently emerged in social media, and Twitter in particular, through accounts that impersonate public figures."
                ]
            },
            {
                "id": "Analyzing Political Parody in Social Media_51",
                "P_Message": "That is very convincing. Thank you for the explanations. Could you tell me a bit about the analyzing part?",
                "DE_Message": "We performed a linguistic analysis, showing that a major difference between politician and parody accounts is the use of first-person pronouns in parody accounts. We also analyzed errors on the model's side, with humorous tweets from real politician's sometimes being confused for parody.",
                "history": [
                    "Hi, how is political parody defined the paper?",
                    "Parody is defined as a means to mimic behavior of a target for comedic purposes.",
                    "Facts: \n1.",
                    "Parody is a figurative device used to imitate an entity for comedic or critical purposes and represents a widespread phenomenon in social media through many popular parody accounts.",
                    "2.",
                    "Parody is a figurative device which is used to imitate and ridicule a particular target (Rose, 1993) and has been studied in linguistics as a figurative trope distinct to irony and satire (Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997) .",
                    "Traditional forms of parody include editorial cartoons\n Can it be detected automatically in social media posts?",
                    "It can!",
                    "In this work we show that we can reach an F1 score of up to 90% using neural-based deep learning methods.",
                    "Facts: \n1.",
                    "Our results show that political parody tweets can be predicted with an accuracy up to 90%\n2.",
                    "Experiments with feature-and neural-based machine learning models for parody detection, which achieve high predictive accuracy of up to 89.7% F1.",
                    "Can the model distinguish political parody from other forms of parody?",
                    "The definition seemed to allow for non-political parody, too.",
                    "Here we only collected political parody data, so the models only learn to identify data in that domain.",
                    "We have not investigated whether the same model could detect general parody.",
                    "I believe that while it is possible to detect non-political parody, more general data would be needed for that task.",
                    "Facts: \n1.",
                    "Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016) ; (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Rober\n I see.",
                    "So the model distinguishes political parody from other posts without any parody, yes?",
                    "Is it possible that the model only detects the political topic without learning about parody at all?",
                    "The data is all in the political domain.",
                    "Data comes from politician accounts as well as political parody accounts.",
                    "So, the model operates entirely within the political domain, and learns to separate between parody and non-parody data.",
                    "Facts: \n1.",
                    "For this task, we create a new large-scale publicly available data set containing a total of 131,666 English tweets from 184 parody accounts and corresponding real accounts of politicians from the US, UK and other countries\n That is very interesting.",
                    "The resource sounds very useful.",
                    "Are there annotations on the tweet-level or do you assume that a parody account only tweets parody and the others don't?",
                    "Could the model be fooled into an author identification task?"
                ],
                "facts": [
                    "Linguistic analysis of the markers of parody tweets and of the model errors"
                ]
            }
        ],
        "argument_mask_0.7": [
            1,
            1,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1
        ],
        "argument_mask_0.75": [
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            1
        ]
    },
    "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem": {
        "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem",
        "title": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem",
        "content": [
            "Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy.",
            "In several domains, however, data is scarce or expensive to retrieve, while a substantial amount of expert knowledge is available.",
            "It seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process.",
            "One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.",
            "Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.",
            "Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge at training time can have a considerable effect and that a small amount of empirical knowledge is sufficient to obtain practically useful results.",
            "Given enough data, Deep Neural Networks (DNNs) are capable of learning complex input-output relations with high accuracy.",
            "In many domains, however, there exists also a substantial degree of expert knowledge: it seems reasonable that if we can inject this additional information in the DNN, we could ease the learning process.",
            "Indeed, methods for hybridizing learning and reasoning (or for taking into account constraints at training time) can accelerate convergence or improve the accuracy, especially when supervised data is scarce.",
            "In this paper we aim at characterizing this trade-off between implicit knowledge (derived from data) and explicit knowledge (supplied by experts), via a set of controlled experiments.",
            "On this purpose, we use a setting that is both rigorous enough from a scientific standpoint and practically relevant: that of constrained problems.",
            "Constrained problems involve assigning values to a set of variables, subject to a number of constraints, and possibly with the goal of minimizing a cost metric.",
            "Depending on the lack or presence of a cost function, they are formally known as Constraint Satisfaction Problems (CSPs) or Constraint Optimization problems (COPs).",
            "Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences) that are not known to the expert, despite being satisfied in historical solutions.",
            "Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility and they struggle with the (very) limited number of past solutions available for practical use cases.",
            "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
            "Despite its simplicity, the PLS is NP-hard, unless we start from an empty square; the problem has practical applications (e.g.",
            "in optical fiber routing), and serves as the basis for more complex problems (e.g.",
            "timetabling).",
            "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).",
            "This combination enables controlled experiments that are difficult to perform on more traditional datasets.",
            "We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time.",
            "We then adjust the amount of initial data (empirical knowledge) and of injected constraints (domain knowledge), and assess the ability of the approach to yield feasible solutions.",
            "Our results are very encouraging, showing that (at least in our setup) embedding domain knowledge in a data-driven approach can have a considerable effect, and that a small amount of empirical knowledge is sufficient to obtain practically useful results.",
            "As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
            "While such techniques are originally designed for problems with discrete decision, they should be adaptable to numeric decisions as well.",
            "Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems.",
            "The paper is organized as follows: Section 2 briefly surveys the related literature and motivates the choice of our baseline techniques; Section 3 discusses the details of the problem and methods we use; Section 4 presents the results of our analysis, while Section 5 provides concluding remarks."
        ],
        "message_pairs": [
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_0",
                "P_Message": "Hi, it is an interesting paper title. What kind of knowledge does domain knowledge refer to here.",
                "DE_Message": "Hi! We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.",
                "history": [],
                "facts": [
                    "Constrained problem are classically modeled by domain experts in a fully declarative fashion"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_1",
                "P_Message": "Interesting, how are you injecting the domain knowledge?",
                "DE_Message": "We have developed a technique inspired by Constraint Programming.",
                "history": [
                    "Hi, it is an interesting paper title.",
                    "What kind of knowledge does domain knowledge refer to here."
                ],
                "facts": [
                    "we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_2",
                "P_Message": "Have been previous Machine Learning approaches been drastically failing on this kind of task?",
                "DE_Message": "Actually, they struggle to provide feasible assignements but they have some advantages. Do you want to know which ones?",
                "history": [
                    "Hi, it is an interesting paper title.",
                    "What kind of knowledge does domain knowledge refer to here.",
                    "Hi!",
                    "We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.",
                    "Facts: \n1.",
                    "Constrained problem are classically modeled by domain experts in a fully declarative fashion\n Interesting, how are you injecting the domain knowledge?"
                ],
                "facts": [
                    "Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_3",
                "P_Message": "Yes, what are their advantages.",
                "DE_Message": "They can take into account some constraints that are not easy to be modelled by the domain expert but that are satisfied in past solutions.",
                "history": [
                    "Hi, it is an interesting paper title.",
                    "What kind of knowledge does domain knowledge refer to here.",
                    "Hi!",
                    "We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.",
                    "Facts: \n1.",
                    "Constrained problem are classically modeled by domain experts in a fully declarative fashion\n Interesting, how are you injecting the domain knowledge?",
                    "We have developed a technique inspired by Constraint Programming.",
                    "Facts: \n1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "Have been previous Machine Learning approaches been drastically failing on this kind of task?"
                ],
                "facts": [
                    "Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences)"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_4",
                "P_Message": "But for some cases you still require domain knowledge? How did you test that?",
                "DE_Message": "Unfortunately, I have not enough information to answer your question.",
                "history": [
                    "Hi, it is an interesting paper title.",
                    "What kind of knowledge does domain knowledge refer to here.",
                    "Hi!",
                    "We refer to the knowledge supplied by the expert when designing a Constraint Satisfaction Problem.",
                    "Facts: \n1.",
                    "Constrained problem are classically modeled by domain experts in a fully declarative fashion\n Interesting, how are you injecting the domain knowledge?",
                    "We have developed a technique inspired by Constraint Programming.",
                    "Facts: \n1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "Have been previous Machine Learning approaches been drastically failing on this kind of task?",
                    "Actually, they struggle to provide feasible assignements but they have some advantages.",
                    "Do you want to know which ones?",
                    "Facts: \n1.",
                    "Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility\n Yes, what are their advantages."
                ],
                "facts": []
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_5",
                "P_Message": "Hi! What is the constrained problem mentioned in the title?",
                "DE_Message": "Hi! We consider the Partial Latin Square completion problem. Do you want to know how does it work?",
                "history": [],
                "facts": [
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_6",
                "P_Message": "Yes, please. What is Partial Latin Square completion problem?",
                "DE_Message": "It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.",
                "history": [
                    "Hi!",
                    "What is the constrained problem mentioned in the title?"
                ],
                "facts": [
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_7",
                "P_Message": "Thank you for the explanation. Then, what is the domain knowledge in this case?",
                "DE_Message": "It is the declarative formulation of the constrained problem.",
                "history": [
                    "Hi!",
                    "What is the constrained problem mentioned in the title?",
                    "Hi!",
                    "We consider the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem\n Yes, please.",
                    "What is Partial Latin Square completion problem?"
                ],
                "facts": [
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_8",
                "P_Message": "I see. Then, how do they inject the domain knowledge in the neural network?",
                "DE_Message": "Our method is inspired by Constraint Programming.",
                "history": [
                    "Hi!",
                    "What is the constrained problem mentioned in the title?",
                    "Hi!",
                    "We consider the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem\n Yes, please.",
                    "What is Partial Latin Square completion problem?",
                    "It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
                    "Thank you for the explanation.",
                    "Then, what is the domain knowledge in this case?"
                ],
                "facts": [
                    "we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_9",
                "P_Message": "so, is it done by adding the constraints in the loss function?",
                "DE_Message": "I am sorry but I have not enough information to answer your question. Do you think that the Partial Latin Square is a suitable benchmark?",
                "history": [
                    "Hi!",
                    "What is the constrained problem mentioned in the title?",
                    "Hi!",
                    "We consider the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem\n Yes, please.",
                    "What is Partial Latin Square completion problem?",
                    "It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
                    "Thank you for the explanation.",
                    "Then, what is the domain knowledge in this case?",
                    "It is the declarative formulation of the constrained problem.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)\n I see.",
                    "Then, how do they inject the domain knowledge in the neural network?"
                ],
                "facts": []
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_10",
                "P_Message": "it seems fine since it is possible to obtain domain knowledge. But I'm not sure whether it is possible to adapt this approach to other domains such as NLP or computer vision. What do you think?",
                "DE_Message": "I think that the main application are Constraints Satisfaction Problems.",
                "history": [
                    "Hi!",
                    "What is the constrained problem mentioned in the title?",
                    "Hi!",
                    "We consider the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem\n Yes, please.",
                    "What is Partial Latin Square completion problem?",
                    "It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
                    "Thank you for the explanation.",
                    "Then, what is the domain knowledge in this case?",
                    "It is the declarative formulation of the constrained problem.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)\n I see.",
                    "Then, how do they inject the domain knowledge in the neural network?",
                    "Our method is inspired by Constraint Programming.",
                    "Facts: \n1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "so, is it done by adding the constraints in the loss function?"
                ],
                "facts": [
                    "Hence, despite our focus remains on a scientific investigation, we also regard this paper as a relevant step towards practical applicability for some data-driven solution methods for constrained problems."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_11",
                "P_Message": "I see. Thank you for your explanations. Good bye",
                "DE_Message": "Bye!",
                "history": [
                    "Hi!",
                    "What is the constrained problem mentioned in the title?",
                    "Hi!",
                    "We consider the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem\n Yes, please.",
                    "What is Partial Latin Square completion problem?",
                    "It requires to complete a partially filled NxN square with all the values in the range from 1 to N, preventing the same value to appear more than once in the same row or column.",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
                    "Thank you for the explanation.",
                    "Then, what is the domain knowledge in this case?",
                    "It is the declarative formulation of the constrained problem.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation)\n I see.",
                    "Then, how do they inject the domain knowledge in the neural network?",
                    "Our method is inspired by Constraint Programming.",
                    "Facts: \n1. we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "so, is it done by adding the constraints in the loss function?",
                    "I am sorry but I have not enough information to answer your question.",
                    "Do you think that the Partial Latin Square is a suitable benchmark?",
                    "it seems fine since it is possible to obtain domain knowledge.",
                    "But I'm not sure whether it is possible to adapt this approach to other domains such as NLP or computer vision.",
                    "What do you think?"
                ],
                "facts": []
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_12",
                "P_Message": "Hello! What is the type of knowledge that is injected in the neural networks?",
                "DE_Message": "Hi! We inject domain knowledge from Constrained Satisfaction Problems.",
                "history": [],
                "facts": [
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions)."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_13",
                "P_Message": "Interesting! What is the problem used to perform the experiment?",
                "DE_Message": "We employ the Partial Latin Square completion problem. Do you want to know how does it work?",
                "history": [
                    "Hello!",
                    "What is the type of knowledge that is injected in the neural networks?"
                ],
                "facts": [
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem,"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_14",
                "P_Message": "Yes, thank you",
                "DE_Message": "You have a partially filled NxN square and you have to complete it with all the values in the range from 1 to N preventing the same value to appear more than once in the same row or column.",
                "history": [
                    "Hello!",
                    "What is the type of knowledge that is injected in the neural networks?",
                    "Hi!",
                    "We inject domain knowledge from Constrained Satisfaction Problems.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).",
                    "Interesting!",
                    "What is the problem used to perform the experiment?"
                ],
                "facts": [
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_15",
                "P_Message": "I understand. And how is knowledge injected in these networks?",
                "DE_Message": "I'm sorry but I have not enough information to answer your question. Do you want to know why we choose the to inject knowledge in a data-driven approach?",
                "history": [
                    "Hello!",
                    "What is the type of knowledge that is injected in the neural networks?",
                    "Hi!",
                    "We inject domain knowledge from Constrained Satisfaction Problems.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).",
                    "Interesting!",
                    "What is the problem used to perform the experiment?",
                    "We employ the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem,\n Yes, thank you"
                ],
                "facts": []
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_16",
                "P_Message": "yes, thank you!",
                "DE_Message": "Data-driven methods can take into account preferences that are hard to design in Constrained Satisfaction Problem. At the same time data-driven method struggle to preserve feasibility.",
                "history": [
                    "Hello!",
                    "What is the type of knowledge that is injected in the neural networks?",
                    "Hi!",
                    "We inject domain knowledge from Constrained Satisfaction Problems.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study grants access to reliable domain knowledge (the declarative formulation), and facilitates the generation of empirical data (problem solutions).",
                    "Interesting!",
                    "What is the problem used to perform the experiment?",
                    "We employ the Partial Latin Square completion problem.",
                    "Do you want to know how does it work?",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem,\n Yes, thank you You have a partially filled NxN square and you have to complete it with all the values in the range from 1 to N preventing the same value to appear more than once in the same row or column.",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
                    "I understand.",
                    "And how is knowledge injected in these networks?"
                ],
                "facts": [
                    "Constrained problem are classically modeled by domain experts in a fully declarative fashion: however, such models can be hard to design, may rely on simplistic and unquantifiable approximations, and may fail to take into account constraints (or preferences)",
                    "Data-driven methods for constrained problems offer a potential solution for some of these issues, but they may have trouble maintaining feasibility"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_52",
                "P_Message": "Hi, what is this paper about?",
                "DE_Message": "Hi! This paper assess the impact of knowledge injection in a data-driven approach.",
                "history": [],
                "facts": [
                    "Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_53",
                "P_Message": "This sounds very interesting. How exactly do you inject knowledge in the network?",
                "DE_Message": "Unfourtunately I have not enough information to answer your question.",
                "history": [
                    "Hi, what is this paper about?"
                ],
                "facts": []
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_54",
                "P_Message": "Ok, thanks anyhow. Which kind of problems has the method you propose been applied to?",
                "DE_Message": "We applied to a particular class of Constrained Satisfaction problem. Do you want to know which one?",
                "history": [
                    "Hi, what is this paper about?",
                    "Hi!",
                    "This paper assess the impact of knowledge injection in a data-driven approach.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.",
                    "This sounds very interesting.",
                    "How exactly do you inject knowledge in the network?"
                ],
                "facts": [
                    "One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.",
                    " As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_55",
                "P_Message": "Yes, please. Which kind of CSP problems?",
                "DE_Message": "We employ the Partial Latin Square completion problem which require to complete a partially filled NxN square with all the values in the range from 1 to N such that no value appear twice in any row and column.",
                "history": [
                    "Hi, what is this paper about?",
                    "Hi!",
                    "This paper assess the impact of knowledge injection in a data-driven approach.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.",
                    "This sounds very interesting.",
                    "How exactly do you inject knowledge in the network?",
                    "Unfourtunately I have not enough information to answer your question.",
                    "Ok, thanks anyhow.",
                    "Which kind of problems has the method you propose been applied to?"
                ],
                "facts": [
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column."
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_56",
                "P_Message": "Thank you. I believe that plugging knowledge into neural architectures is a crucial issue for AI in the coming years. Do you need to encode the knowledge in some language? Does knowledge injection affect training time?",
                "DE_Message": "Our method takes inspiration from Constraint Programming  and it can be applied also at test time.",
                "history": [
                    "Hi, what is this paper about?",
                    "Hi!",
                    "This paper assess the impact of knowledge injection in a data-driven approach.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.",
                    "This sounds very interesting.",
                    "How exactly do you inject knowledge in the network?",
                    "Unfourtunately I have not enough information to answer your question.",
                    "Ok, thanks anyhow.",
                    "Which kind of problems has the method you propose been applied to?",
                    "We applied to a particular class of Constrained Satisfaction problem.",
                    "Do you want to know which one?",
                    "Facts: \n1.",
                    "One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.",
                    "2.",
                    "As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "Yes, please.",
                    "Which kind of CSP problems?"
                ],
                "facts": [
                    "As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "We train a problem-agnostic, data-driven, solution approach on a pool of solutions, and we inject domain knowledge (constraints) both at training time and at solution generation time"
                ]
            },
            {
                "id": "Injecting Domain Knowledge in Neural Networks: a Controlled Experiment on a Constrained Problem_57",
                "P_Message": "Thank you, this is all I wanted to know!",
                "DE_Message": "You are welcome!",
                "history": [
                    "Hi, what is this paper about?",
                    "Hi!",
                    "This paper assess the impact of knowledge injection in a data-driven approach.",
                    "Facts: \n1.",
                    "Using a classical constrained problem as a case study, we perform controlled experiments to probe the impact of progressively adding domain and empirical knowledge in the DNN.",
                    "This sounds very interesting.",
                    "How exactly do you inject knowledge in the network?",
                    "Unfourtunately I have not enough information to answer your question.",
                    "Ok, thanks anyhow.",
                    "Which kind of problems has the method you propose been applied to?",
                    "We applied to a particular class of Constrained Satisfaction problem.",
                    "Do you want to know which one?",
                    "Facts: \n1.",
                    "One such case is that of Constraint Problems, for which declarative approaches exists and pure ML solutions have obtained mixed success.",
                    "2.",
                    "As a byproduct of our analysis, we develop general techniques for taking into account constraints in data-driven methods for decision problems, based on easily accessible methods from the Constraint Programming and Machine Learning domains.",
                    "Yes, please.",
                    "Which kind of CSP problems?",
                    "We employ the Partial Latin Square completion problem which require to complete a partially filled NxN square with all the values in the range from 1 to N such that no value appear twice in any row and column.",
                    "Facts: \n1.",
                    "We use as a benchmark the Partial Latin Square (PLS) completion problem, which requires to complete a partially filled n \u00d7 n square with values in {1..n}, such that no value appears twice on any row or column.",
                    "Thank you.",
                    "I believe that plugging knowledge into neural architectures is a crucial issue for AI in the coming years.",
                    "Do you need to encode the knowledge in some language?",
                    "Does knowledge injection affect training time?"
                ],
                "facts": []
            }
        ],
        "argument_mask_0.7": [
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1
        ],
        "argument_mask_0.75": [
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1
        ]
    },
    "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation": {
        "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation",
        "title": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation",
        "content": [
            "Question Generation (QG) is a Natural Language Processing (NLP) task that aids advances in Question Answering (QA) and conversational assistants.",
            "Existing models focus on generating a question based on a text and possibly the answer to the generated question.",
            "They need to determine the type of interrogative word to be generated while having to pay attention to the grammar and vocabulary of the question.",
            "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.",
            "The first module predicts the interrogative word that is provided to the second module to create the question.",
            "Owing to an increased recall of deciding the interrogative words to be used for the generated questions, the proposed model achieves new state-of-the-art results on the task of QG Question Generation (QG) is the task of creating questions about a text in natural language.",
            "This is an important task for Question Answering (QA) since it can help create QA datasets.",
            "It is also useful for conversational systems like Amazon Alexa.",
            "Due to the surge of interests in these systems, QG is also drawing the attention of the research community.",
            "One of the reasons for the fast advances in QA capabilities is the creation of large datasets like SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017) .",
            "Since the creation of such datasets is either costly if done manually or prone to error if done automatically, reliable and mean- * Equal contribution.",
            "ingful QG can play a key role in the advances of QA (Lewis et al., 2019) .",
            "QG is a difficult task due to the need for understanding of the text to ask about and generating a question that is grammatically correct and semantically adequate according to the given text.",
            "This task is considered to have two parts: what to ask and how to ask.",
            "The first one refers to the identification of relevant portions of the text to ask about.",
            "This requires machine reading comprehension since the system has to understand the text.",
            "The latter refers to the creation of a natural language question that is grammatically correct and semantically precise.",
            "Most of the current approaches utilize sequence-to-sequence models, composed of an encoder model that first transforms a passage into a vector and a decoder model that given this vector, generates a question about the passage (Liu et al., 2019; Sun et al., 2018; Zhao et al., 2018; Pan et al., 2019) .",
            "There are different settings for QG.",
            "Some authors like (Subramanian et al., 2018) assumes that only a passage is given, attempts to find candidate key phrases that represent the core of the questions to be created.",
            "Others follow an answer-aware setting, where the input is a passage and the answer to the question to create (Zhao et al., 2018) .",
            "We assume this setting and consider that the answer is a span of the passage, as in SQuAD.",
            "Follow-ing this approach, the decoder of the sequence-tosequence model has to learn to generate both the interrogative word (i.e., wh-word) and the rest of the question simultaneously.",
            "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
            "We posit that the interrogative word must be predicted by a well-trained classifier.",
            "We consider that selecting the right interrogative word is the key to generate high-quality questions.",
            "For example, a question with a wrong interrogative word for the answer \"the owner\" is: \"what produces a list of requirements for a project?\".",
            "However, with the right interrogative word, who, the question would be: \"who produces a list of requirements for a project?",
            "\", which is clear that is more adequate regarding the answer than the first one.",
            "According to our claim, the independent classification model can improve the recall of interrogative words of a QG model because 1) the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model and 2) the QG model would be able to generate the interrogative word easily by using the copy mechanism, which can copy parts of the input of the encoder.",
            "With these hypotheses, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative-word classifier that predicts the interrogative word and a QG model that generates a question conditioned on the predicted interrogative word.",
            "Figure 1 shows a highlevel overview of our approach.",
            "The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L."
        ],
        "message_pairs": [
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_222",
                "P_Message": "Hi! What is this paper about?",
                "DE_Message": "Hi! The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.",
                "history": [],
                "facts": [
                    "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model."
                ]
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_223",
                "P_Message": "What architecture is used for question generation?",
                "DE_Message": "The exact architecture of the question generation model is not described in the Introduction.",
                "history": [
                    "Hi!",
                    "What is this paper about?"
                ],
                "facts": []
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_224",
                "P_Message": "How about the metrics? How are results measured?",
                "DE_Message": "It is mentioned that their approach achieves a new state-of-the-art on SQuAD using the metrics METEOR and ROUGE-L. What do you think about their hypothesis? Does it make sense for you to generate the interrogative word independently?",
                "history": [
                    "Hi!",
                    "What is this paper about?",
                    "Hi!",
                    "The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.",
                    "Facts: \n1.",
                    "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
                    "2.",
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.",
                    "What architecture is used for question generation?"
                ],
                "facts": [
                    "The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L."
                ]
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_225",
                "P_Message": "I think it makes sense, and it also seems to work. Anything else worth mentioning about this paper?",
                "DE_Message": "I think that's all. They mentioned that their hypothesis makes the generation of the interrogative word easier since it's a classification task.",
                "history": [
                    "Hi!",
                    "What is this paper about?",
                    "Hi!",
                    "The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.",
                    "Facts: \n1.",
                    "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
                    "2.",
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.",
                    "What architecture is used for question generation?",
                    "The exact architecture of the question generation model is not described in the Introduction.",
                    "How about the metrics?",
                    "How are results measured?"
                ],
                "facts": [
                    "the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model"
                ]
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_226",
                "P_Message": "Thank you for the clarification. The intuition makes sense. Bye",
                "DE_Message": "Thank you. Good bye",
                "history": [
                    "Hi!",
                    "What is this paper about?",
                    "Hi!",
                    "The goal of this paper is to improve question generation by dividing the task into two: interrogative word prediction and the generation of the rest of the question.",
                    "Facts: \n1.",
                    "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
                    "2.",
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.",
                    "What architecture is used for question generation?",
                    "The exact architecture of the question generation model is not described in the Introduction.",
                    "How about the metrics?",
                    "How are results measured?",
                    "It is mentioned that their approach achieves a new state-of-the-art on SQuAD using the metrics METEOR and ROUGE-L. What do you think about their hypothesis?",
                    "Does it make sense for you to generate the interrogative word independently?",
                    "Facts: \n1.",
                    "The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L.",
                    "I think it makes sense, and it also seems to work.",
                    "Anything else worth mentioning about this paper?"
                ],
                "facts": []
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_233",
                "P_Message": "hi, is the paper about question generation? ",
                "DE_Message": "Hi! Yes! It is. They propose a pipelined composed of two parts: an interrogative word classifier and a question generation model",
                "history": [],
                "facts": [
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model."
                ]
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_234",
                "P_Message": "Why is interrogative classifier used in this model?",
                "DE_Message": "That is a good question! They argue that classifying the interrogative word is easier that generating it along with the rest of the question. What do you think about this intuition? Does it make sense for you?",
                "history": [
                    "hi, is the paper about question generation?"
                ],
                "facts": [
                    "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
                    "the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model"
                ]
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_235",
                "P_Message": "Yes, makes sense to me. What data set is used to train a classifier? ",
                "DE_Message": "They conduct experiments on SQuAD. Do you think SQuAD is a good benchmark for this?",
                "history": [
                    "hi, is the paper about question generation?",
                    "Hi!",
                    "Yes!",
                    "It is.",
                    "They propose a pipelined composed of two parts: an interrogative word classifier and a question generation model\n Facts: \n1.",
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.",
                    "Why is interrogative classifier used in this model?"
                ],
                "facts": [
                    "The proposed model achieves new state-of-theart results on the task of QG in SQuAD, improving from 46.58 to 47.69 in 21.24 to 22.33 in METEOR, and from 44.53 to 46.94 in ROUGE-L."
                ]
            },
            {
                "id": "Let Me Know What to Ask: Interrogative-Word-Aware Question Generation_236",
                "P_Message": "As long as I know, SQuAD is a question-answering dataset. I'm not sure if this dataset is suitable for question word clarification. What do you think?",
                "DE_Message": "It is not clarification, but prediction. Since SQuAD includes questions, answers, and context, it would be possible to use the answers to create questions for them.",
                "history": [
                    "hi, is the paper about question generation?",
                    "Hi!",
                    "Yes!",
                    "It is.",
                    "They propose a pipelined composed of two parts: an interrogative word classifier and a question generation model\n Facts: \n1.",
                    "In this work, we propose Interrogative-Word-Aware Question Generation (IWAQG), a pipelined system composed of two modules: an interrogative word classifier and a QG model.",
                    "Why is interrogative classifier used in this model?",
                    "That is a good question!",
                    "They argue that classifying the interrogative word is easier that generating it along with the rest of the question.",
                    "What do you think about this intuition?",
                    "Does it make sense for you?",
                    "Facts: \n1.",
                    "The main claim of our work is that separating the two tasks (i.e., interrogative-word classification and question generation) can lead to a better performance.",
                    "2. the interrogative word classification task is easier to solve than generating the interrogative word along with the full question in the QG model\n Yes, makes sense to me.",
                    "What data set is used to train a classifier?"
                ],
                "facts": [
                    "We assume this setting and consider that the answer is a span of the passage, as in SQuAD."
                ]
            }
        ],
        "argument_mask_0.7": [
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1
        ],
        "argument_mask_0.75": [
            1,
            1,
            1,
            0,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            1,
            0,
            0,
            0,
            1,
            1
        ]
    },
    "Syntactically-informed word representations from graph neural network": {
        "id": "Syntactically-informed word representations from graph neural network",
        "title": "Syntactically-informed word representations from graph neural network",
        "content": [
            "Most deep language understanding models depend only on word representations, which are mainly based on language modelling derived from a large amount of raw text.",
            "These models encode distributional knowledge without considering syntactic structural information, although several studies have shown benefits of including such information.",
            "Therefore, we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.",
            "To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.",
            "The model is first pre-trained with only a relatively modest amount of task-independent data that are automatically annotated using existing syntactic tools.",
            "SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.",
            "We finally replace word representations in downstream models with SIWRs for applications.",
            "We evaluate SIWRs on three information extraction tasks, namely nested named entity recognition (NER), binary and n-ary relation extractions (REs).",
            "The results demonstrate that our SIWRs yield performance gains over the base representations in these NLP tasks with 3-9% relative error reduction.",
            "Our SIWRs also perform better than finetuning BERT in binary RE.",
            "We also conduct extensive experiments to analyse the proposed method.",
            "Word representations have been widely used in natural language processing (NLP) tasks.",
            "Most approaches rely on language models (LMs) to obtain static word representations [1] [2] [3] , which conflate all possible meanings of a word in a single real-valued vector.",
            "Recent work investigated contextualised word representations, which assign a different representation to each occurrence of a word based on its local context [4, 5] .",
            "These contextual word representations have demonstrated improvements in downstream tasks over the static ones.",
            "Alternatively, large-scale LMs have been proposed to use in downstream application models with fine-tuning approaches [6] [7] [8] .",
            "These fine-tuning methods have shown promising results with higher performance than contextual word representations in some applications such as text classification and textual entailment [7] .",
            "However, not all tasks can be easily represented by large-scale LMs, therefore required an additional model architecture to be designed on top of them [9, 10] .",
            "On the contrary, contextual representations can be easily adopted as plug-in plugout features.",
            "Also, contextual representations are cheaper to run as they are pre-computed once only for each instance and run in many experiments with smaller models on top.",
            "In other words, the computational costs for fine-tuning methods are much higher than contextual approaches.",
            "All of the LMs mentioned above are mainly trained on a large amount of raw text, and thus do not explicitly encode any linguistic structures.",
            "Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] .",
            "The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig.",
            "1 ; in this paper, we use the term ''syntactic information\" interchangeably with ''POS tags\" and ''dependencies\"), has been well studied and can be obtained efficiently with high accuracy using existing dependency parsing tools [16, 17] .",
            "Many task-oriented neural models do not take into account such syntactic information despite potential performance gains.",
            "To include such information to existing models, it is necessary to change the model architecture.",
            "This leads to the following research question:\n\n Is there a universal way to include syntactic bias into the model without changing the architecture while retaining large-scale information?",
            "In this paper, we will demonstrate that syntax can be pre-encoded with contextual word representations which can be beneficial for subsequent applications.",
            "We introduce Syntactically-Informed Word Representations (SIWRs) that can incorporate syntactic information in neural models without explicitly changing their architecture.",
            "Syntactic information is integrated into existing word representations such as GloVe [18] , ELMo [5] and BERT [19] by learning from automatically annotated data which are task-independent.",
            "We propose the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.",
            "Since in English word order is important, we preserve it by adding these connections into the graph layer.",
            "We then obtain SIWRs from the pre-trained SIWR model using a contextualised word representation extraction scheme.",
            "Finally, we incorporate SIWRs into downstream models by only replacing the word representations with SIWRs.",
            "We show that SIWRs enrich the base word representations with syntactic information and boost the performance in downstream tasks.",
            "Unlike previous work [20] , our findings demonstrate that syntactic information is helpful in the advent of contextual word representations and large pre-trained models.",
            "Fig.",
            "2 shows the architecture of our SIWR model.",
            "We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations.",
            "We then feed them to our SIWR model, which consists of a twostacked GCN layer over dependency trees along with self and sequential information.",
            "The GCN is used to include syntactic information into the base word representations.",
            "The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.",
            "We only pre-train the SIWR model once with a relatively modest amount of task-agnostic data that are automatically annotated by using existing syntactic tools.",
            "Once the SIWR model is obtained, we apply the model to downstream task data and obtain SIWRs by combining the outputs of all layers in the model.",
            "We simply replace word representations in downstream task models with SIWRs to cater for different applications.",
            "We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).",
            "SIWRs show improvements over the base representations achieving the following relative error reductions: 3.79% in F1-score for nested NER, 6.64% in F1-score for RE, and 6.98% of accuracy for n-ary RE, which results in comparable performance to the state-of-the-art on the three tasks.",
            "In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE.",
            "We also employ BERT as the base representations in our SIWRs BERT for comparison.",
            "Surprisingly, our SIWRs BERT based on contextual BERT even perform better than the fine-tuning in binary RE with the F1-score of 72.45% and 66.84% respectively.",
            "Meanwhile, our enhanced representations perform comparably to the fine-tuning BERT with less training parameters in the nested NER with the F1-score of 82.06% and 82.84% respectively.",
            "Our extensive analysis also shows that the syntactic inductive bias can be easily transferred to subsequent NLP tasks and beneficial for performance improvement.",
            "The contributions of our work are as follows:\n\n We propose a method for the construction of syntacticallyinformed representations (SIWRs) based on static and/or contextual representations.",
            "SIWRs allow us to incorporate syntactic information into existing NLP neural models simply by replacing the original word representations, without altering the architecture of these models with a relatively modest amount of syntactically annotated data.",
            "We demonstrate that SIWRs improve the performance over base representations on three downstream NLP tasks: nested NER, binary RE, and n-ary RE.",
            "The improvement can be obtained with only a small number of weight parameters compared with training task-oriented syntactic representations in downstream models.",
            "Extensive analysis of SIWRs over base representations indicates that syntactic dependencies can be beneficial for both static and contextual embeddings, contrary to previous findings [20] .",
            "Last, we show that our enhanced representations can be transferred more easily to different linguistic tasks than fine-tuning large-scale language models in downstream tasks that require inference."
        ],
        "message_pairs": [
            {
                "id": "Syntactically-informed word representations from graph neural network_21",
                "P_Message": "Hi, interesting paper title. How do you add the syntactic information into the word representations?",
                "DE_Message": "we build a syntactic graph neural model that takes pre-trained word representations as feature input. The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.",
                "history": [],
                "facts": [
                    "SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_22",
                "P_Message": "Which kind of graph neural network do you experimented with?",
                "DE_Message": "We use a graph convolutional neural network (GCN). ",
                "history": [
                    "Hi, interesting paper title.",
                    "How do you add the syntactic information into the word representations?"
                ],
                "facts": [
                    "the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_23",
                "P_Message": "Do GCNs come with special properties which helps for the task?",
                "DE_Message": "The word graph is built on syntactic dependency and other types of connections such as word order connections. Do you want to know what objectives the model trained on?",
                "history": [
                    "Hi, interesting paper title.",
                    "How do you add the syntactic information into the word representations?",
                    "we build a syntactic graph neural model that takes pre-trained word representations as feature input.",
                    "The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.",
                    "Facts: \n1.",
                    "SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.",
                    "Which kind of graph neural network do you experimented with?"
                ],
                "facts": [
                    "Since in English word order is important, we preserve it by adding these connections into the graph layer."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_24",
                "P_Message": "Yes, please. Which objectives did the model try to solve?",
                "DE_Message": "The model is trained for part-of-speech tagging and syntactic dependency parsing.",
                "history": [
                    "Hi, interesting paper title.",
                    "How do you add the syntactic information into the word representations?",
                    "we build a syntactic graph neural model that takes pre-trained word representations as feature input.",
                    "The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.",
                    "Facts: \n1.",
                    "SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.",
                    "Which kind of graph neural network do you experimented with?",
                    "We use a graph convolutional neural network (GCN).",
                    "Facts: \n1. the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.",
                    "Do GCNs come with special properties which helps for the task?"
                ],
                "facts": [
                    "The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_25",
                "P_Message": "And which pre-trained word representations did you use?",
                "DE_Message": "Our model can take both static and contextualized word representations including GloVe, ELMo and BERT.",
                "history": [
                    "Hi, interesting paper title.",
                    "How do you add the syntactic information into the word representations?",
                    "we build a syntactic graph neural model that takes pre-trained word representations as feature input.",
                    "The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.",
                    "Facts: \n1.",
                    "SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.",
                    "Which kind of graph neural network do you experimented with?",
                    "We use a graph convolutional neural network (GCN).",
                    "Facts: \n1. the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.",
                    "Do GCNs come with special properties which helps for the task?",
                    "The word graph is built on syntactic dependency and other types of connections such as word order connections.",
                    "Do you want to know what objectives the model trained on?",
                    "Facts: \n1.",
                    "Since in English word order is important, we preserve it by adding these connections into the graph layer.",
                    "Yes, please.",
                    "Which objectives did the model try to solve?"
                ],
                "facts": [
                    "a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.",
                    "We first prepare pre-trained static and contextual word representations, e.g., GloVe, ELMo, and contextual BERT, as the base representations."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_26",
                "P_Message": "How did you evaluate the improvement of the new word representations?",
                "DE_Message": "We compare performance of using the word representations in the downstream tasks compared to using the base representations. The downstream tasks include a nested named entity recognition and two relation extraction tasks.",
                "history": [
                    "Hi, interesting paper title.",
                    "How do you add the syntactic information into the word representations?",
                    "we build a syntactic graph neural model that takes pre-trained word representations as feature input.",
                    "The pre-trained word representations and the GNN are used as contextualized word representations in downstream tasks.",
                    "Facts: \n1.",
                    "SIWRs are then obtained by applying the model to downstream task data and extracting the intermediate word representations.",
                    "Which kind of graph neural network do you experimented with?",
                    "We use a graph convolutional neural network (GCN).",
                    "Facts: \n1. the SIWR model extends a graph convolutional neural network (GCN) and builds on top of these word representations.",
                    "Do GCNs come with special properties which helps for the task?",
                    "The word graph is built on syntactic dependency and other types of connections such as word order connections.",
                    "Do you want to know what objectives the model trained on?",
                    "Facts: \n1.",
                    "Since in English word order is important, we preserve it by adding these connections into the graph layer.",
                    "Yes, please.",
                    "Which objectives did the model try to solve?",
                    "The model is trained for part-of-speech tagging and syntactic dependency parsing.",
                    "Facts: \n1.",
                    "The SIWR model jointly predicts part-of-speech (POS) tags and syntactic dependencies.",
                    "And which pre-trained word representations did you use?"
                ],
                "facts": [
                    "We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).",
                    "In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_27",
                "P_Message": "Hi! Thanks for joining. Can you explain what is the main idea of the paper?",
                "DE_Message": "This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.",
                "history": [],
                "facts": [
                    "we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.",
                    "To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_28",
                "P_Message": "What type of syntactic information do you use? Is it based on dependency or constituency? ",
                "DE_Message": "We use part-of-speech and syntactic dependency. ",
                "history": [
                    "Hi!",
                    "Thanks for joining.",
                    "Can you explain what is the main idea of the paper?"
                ],
                "facts": [
                    "The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig.",
                    "1 ; in this paper, we use the term ''syntactic information\" interchangeably with ''POS tags\" and ''dependencies\"), has been well studied and can be obtained efficiently with high accuracy using existing"
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_29",
                "P_Message": "Sounds great! What tasks do you test?",
                "DE_Message": "We evaluate the extracted word representations on a nested named entity recognition task and two relation extraction tasks.",
                "history": [
                    "Hi!",
                    "Thanks for joining.",
                    "Can you explain what is the main idea of the paper?",
                    "This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.",
                    "Facts: \n1. we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.",
                    "2.",
                    "To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.",
                    "What type of syntactic information do you use?",
                    "Is it based on dependency or constituency?"
                ],
                "facts": [
                    "We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).",
                    "In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_30",
                "P_Message": "Can you tell me why you only select part-of-speech and dependency as the syntactic information you want to use to help pretraining?",
                "DE_Message": "Previous work shows that part-of-speech and dependency are useful for downstream tasks. However, this doesn't limit the use of other information such as coreference in our model.",
                "history": [
                    "Hi!",
                    "Thanks for joining.",
                    "Can you explain what is the main idea of the paper?",
                    "This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.",
                    "Facts: \n1. we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.",
                    "2.",
                    "To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.",
                    "What type of syntactic information do you use?",
                    "Is it based on dependency or constituency?",
                    "We use part-of-speech and syntactic dependency.",
                    "Facts: \n1.",
                    "The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig.",
                    "1 ; in this paper, we use the term ''syntactic information\" interchangeably with ''POS tags\" and ''dependencies\"), has been well studied and can be obtained efficiently with high accuracy using existing \n Sounds great!",
                    "What tasks do you test?"
                ],
                "facts": [
                    "Recent studies have shown that downstream task performance may benefit from linguistic structures such as syntactic information [11, 12] , even when contextual word representations and pre-trained models are also used [13] [14] [15] ."
                ]
            },
            {
                "id": "Syntactically-informed word representations from graph neural network_31",
                "P_Message": "Thanks for explaining. I love to see the results of injecting other types of information. Do you think if it would be a good idea if you try to inject syntactic information beginning from pretraining from scratch?",
                "DE_Message": "We cannot answer given the current content. We can inject syntactic information during pre-training from scratch, however, this will cost more time and computational resources. Intermediate pretraining allows us to directly incorporate additional information into pre-trained models.",
                "history": [
                    "Hi!",
                    "Thanks for joining.",
                    "Can you explain what is the main idea of the paper?",
                    "This paper proposes a method to incorporate syntactic information into pre-trained word representations using a graph neural network.",
                    "Facts: \n1. we propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch.",
                    "2.",
                    "To obtain SIWRs, a graph-based neural model is built on top of either static or contextualised word representations such as GloVe, ELMo and BERT.",
                    "What type of syntactic information do you use?",
                    "Is it based on dependency or constituency?",
                    "We use part-of-speech and syntactic dependency.",
                    "Facts: \n1.",
                    "The syntactic information, i.e., part-of-speech (POS) tags and dependencies (see example in Fig.",
                    "1 ; in this paper, we use the term ''syntactic information\" interchangeably with ''POS tags\" and ''dependencies\"), has been well studied and can be obtained efficiently with high accuracy using existing \n Sounds great!",
                    "What tasks do you test?",
                    "We evaluate the extracted word representations on a nested named entity recognition task and two relation extraction tasks.",
                    "Facts: \n1.",
                    "We compare the enriched SIWRs with their base representations ELMo [5] and biomedical word embeddings (PubMed) [21] on the existing models in three downstream NLP tasks: (a) nested named entity recognition (nested NER), (b) relation extraction (RE) and (c) n-ary relation extraction (n-ary RE).",
                    "2.",
                    "In addition, we implement BERT [19] in both contextual and fine-tuning methods on nested NER and binary RE.",
                    "Can you tell me why you only select part-of-speech and dependency as the syntactic information you want to use to help pretraining?"
                ],
                "facts": [
                    "We propose new syntactically-informed word representations (SIWRs), which allow us to enrich the pre-trained word representations with syntactic information without training language models from scratch."
                ]
            }
        ],
        "argument_mask_0.7": [
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            1,
            0
        ],
        "argument_mask_0.75": [
            1,
            1,
            0,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            1,
            1,
            1,
            1,
            1,
            1,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            0,
            0,
            0,
            1,
            0,
            1,
            1,
            0,
            0,
            1,
            1,
            1,
            0,
            1,
            0,
            0,
            1,
            0
        ]
    }
}